{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Projeto \"Maestro das Palavras\": Ensinando uma IA a Entender Letras de M√∫sica üé∂\n",
    "\n",
    "## A Miss√£o: O Cora√ß√£o da Can√ß√£o\n",
    "\n",
    "Imagine uma intelig√™ncia artificial que n√£o apenas l√™ palavras, mas *sente* o ritmo e a alma da linguagem, especialmente como ela flui nas letras de m√∫sica. Esse √© o nosso projeto! Estamos treinando um verdadeiro **\"Maestro das Palavras\"** digital.\n",
    "\n",
    "A sua grande miss√£o? Olhar para um trecho de uma m√∫sica onde uma palavra foi escondida (`[mask]`) e, usando sua intelig√™ncia, adivinhar qual palavra se encaixa ali, como um m√∫sico que completa uma melodia inacabada.\n",
    "\n",
    "## A Magia por Tr√°s da Cortina: Transformers e Aten√ß√£o üß†‚ú®\n",
    "\n",
    "Como nosso Maestro aprende essa habilidade quase m√°gica? Ele usa uma arquitetura de rede neural revolucion√°ria chamada **Transformer**.\n",
    "\n",
    "Pense no Transformer como um ouvinte *extremamente* atento. Em vez de apenas olhar a palavra anterior ou seguinte, ele possui um mecanismo de **\"Aten√ß√£o\"** que permite \"ouvir\" e ponderar a import√¢ncia de **todas** as palavras na frase (ou at√© em frases pr√≥ximas) ao mesmo tempo.\n",
    "\n",
    "Isso √© crucial para a m√∫sica! Uma palavra no final de um verso pode depender totalmente de algo dito no in√≠cio. O Transformer captura essas conex√µes, longas ou curtas, entendendo o **contexto** geral da letra, assim como entendemos o sentimento de uma estrofe inteira, n√£o apenas de notas isoladas.\n",
    "\n",
    "## O M√©todo do Mestre: BERT e a Arte de Aprender üéì\n",
    "\n",
    "Nosso Maestro segue a filosofia de um \"mestre\" renomado no mundo da IA, conhecido como **BERT** (Bidirectional Encoder Representations from Transformers). O BERT popularizou uma forma genial de treinar esses Transformers para entender a linguagem profundamente.\n",
    "\n",
    "A ideia chave do BERT √© ser **bidirecional**: para entender uma palavra, olhe para o que vem *antes* E o que vem *depois* dela. √â como ler uma partitura completa antes de tocar uma nota espec√≠fica.\n",
    "\n",
    "## O Treinamento: Completando as Lacunas (MLM) üìù\n",
    "\n",
    "Como o Maestro pratica? Atrav√©s de um exerc√≠cio fundamental inspirado no BERT, chamado **Masked Language Modeling (MLM)** ‚Äì Modelagem de Linguagem Mascarada. √â exatamente isso que o nosso c√≥digo faz!\n",
    "\n",
    "1.  Pegamos uma letra de m√∫sica do nosso \"songbook\" (os arquivos `.txt` em `musicas/`).\n",
    "2.  **Mascaramos** aleatoriamente algumas palavras, substituindo-as por um token especial `[mask]`.\n",
    "    * Exemplo: \"Todo [mask] tem seu fim\"\n",
    "3.  **Desafiamos** o Maestro: \"Ei, usando todo o seu conhecimento de contexto (via Transformer), adivinhe qual palavra original estava aqui onde est√° o `[mask]`!\"\n",
    "\n",
    "Ao for√ßar o modelo a prever essas palavras escondidas com base no contexto ao redor (bidirecionalmente), ele aprende representa√ß√µes ricas e profundas sobre como as palavras se relacionam em letras de m√∫sica. Ele n√£o est√° apenas decorando, est√° *compreendendo*.\n",
    "\n",
    "## O Resultado: Um Vislumbre da Compreens√£o üé§\n",
    "\n",
    "Ap√≥s treinar intensamente com milhares desses exemplos mascarados, nosso Maestro das Palavras se torna capaz de:\n",
    "\n",
    "* Receber uma nova frase com `[mask]`.\n",
    "* Analisar o contexto usando sua arquitetura Transformer.\n",
    "* Prever as palavras mais prov√°veis para preencher a lacuna, demonstrando sua compreens√£o da linguagem l√≠rica.\n",
    "\n",
    "Este projeto, portanto, √© a constru√ß√£o e o treinamento de um modelo Transformer, inspirado nos princ√≠pios do BERT e usando a tarefa de MLM, para desenvolver uma compreens√£o contextual profunda especificamente a partir de letras de m√∫sica. √â um passo fascinante na jornada para criar IAs que n√£o apenas processem, mas talvez um dia, at√© *sintam* a arte da linguagem."
   ],
   "id": "23288c2911f7f233"
  },
  {
   "cell_type": "markdown",
   "id": "c4bed58b15c8e73d",
   "metadata": {},
   "source": [
    "# Configura√ß√£o Inicial e Importa√ß√£o de Bibliotecas\n",
    "\n",
    "Este trecho de c√≥digo realiza as configura√ß√µes iniciais para o ambiente do notebook, definindo o backend do Keras e importando as bibliotecas necess√°rias para manipula√ß√£o de dados, opera√ß√µes com matrizes, processamento de texto e constru√ß√£o de modelos. Al√©m disso, cria uma classe de configura√ß√£o (usando `@dataclass`) que cont√©m par√¢metros importantes como tamanho m√°ximo de sequ√™ncia, tamanho do lote, taxa de aprendizado, entre outros, facilitando a parametriza√ß√£o dos modelos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a89620b2e2393cb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:02.375680Z",
     "start_time": "2025-04-09T13:25:02.345647Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "os.environ[ \"KERAS_BACKEND\" ] = \"torch\"\n",
    "\n",
    "import keras_hub\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "from pprint import pprint"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:02.423647Z",
     "start_time": "2025-04-09T13:25:02.399649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Identificar se est√° usando a CPU ou GPU no PyTorch\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\" )\n",
    "print( f\"Using device: {device}\" )"
   ],
   "id": "a5efb95b0d0e627",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "id": "6cce974cf0c26a19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:06.836720Z",
     "start_time": "2025-04-09T13:25:06.823721Z"
    }
   },
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 256\n",
    "    BATCH_SIZE = 16  #32\n",
    "    LR = 0.0001  #0.001\n",
    "    VOCAB_SIZE = 30000\n",
    "    EMBED_DIM = 128\n",
    "    NUM_HEAD = 8  # used in bert model\n",
    "    FF_DIM = 128  # used in bert model\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "\n",
    "config = Config()"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "id": "39422ab42f331838",
   "metadata": {},
   "source": [
    "# Leitura de Dados e Cria√ß√£o de DataFrames\n",
    "\n",
    "Este trecho de c√≥digo define fun√ß√µes que realizam a leitura de arquivos de texto de uma pasta espec√≠fica, extraem o conte√∫do dos arquivos e geram dataframes. A fun√ß√£o `get_text_list_from_files` recebe uma lista de arquivos e retorna seus conte√∫dos como uma lista de strings. Em seguida, a fun√ß√£o `get_data_from_text_files` utiliza essa lista para criar um dataframe onde cada linha corresponde ao conte√∫do de um arquivo (letra de m√∫sica). Por fim, os dataframes de treino e teste/valida√ß√£o s√£o criados e seus tamanhos s√£o exibidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "822b332050609de1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:07.447719Z",
     "start_time": "2025-04-09T13:25:06.884718Z"
    }
   },
   "source": [
    "def get_text_list_from_files( files ) -> list[ str ]:\n",
    "    \"\"\"\n",
    "    L√™ o conte√∫do de m√∫ltiplos arquivos e retorna uma lista de strings,\n",
    "    onde cada string representa o conte√∫do de um arquivo.\n",
    "\n",
    "    Args:\n",
    "        files (list[str]): Uma lista contendo os caminhos dos arquivos a serem lidos.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Uma lista onde cada elemento √© o conte√∫do de um arquivo.\n",
    "    \"\"\"\n",
    "    text_list: list[ str ] = [ ]\n",
    "    for file_path in files:\n",
    "        with open( file_path, \"r\", encoding = \"utf-8\" ) as file:\n",
    "            # L√™ todo o conte√∫do do arquivo de uma vez.\n",
    "            # Se o arquivo contiver v√°rias linhas e voc√™ precisar de cada linha como um item separado na lista,\n",
    "            # voc√™ pode iterar sobre o objeto 'file' (por exemplo, `for line in file: text_list.append(line.strip())`).\n",
    "            text_list.append( file.read() )\n",
    "    return text_list\n",
    "\n",
    "\n",
    "def get_data_from_text_files( folder_name: str ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Busca arquivos de texto em uma pasta espec√≠fica, l√™ seus conte√∫dos e\n",
    "    retorna um DataFrame do pandas com uma coluna contendo o texto de cada arquivo.\n",
    "\n",
    "    Args:\n",
    "        folder_name (str): O nome da pasta onde os arquivos de texto est√£o localizados.\n",
    "                           Espera-se que os arquivos estejam dentro de um subdiret√≥rio chamado 'musicas'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Um DataFrame com uma coluna chamada 'lyric', onde cada linha\n",
    "                      cont√©m o texto de um arquivo. O DataFrame √© embaralhado e o\n",
    "                      √≠ndice √© resetado.\n",
    "    \"\"\"\n",
    "\n",
    "    # Utiliza a biblioteca glob para encontrar todos os arquivos com extens√£o .txt\n",
    "    # dentro do subdiret√≥rio especificado dentro de 'musicas'.\n",
    "    files = glob.glob( f\"musicas/{folder_name}/*.txt\" )\n",
    "\n",
    "    # Chama a fun√ß√£o auxiliar para obter uma lista contendo o texto de cada arquivo encontrado.\n",
    "    texts: list[ str ] = get_text_list_from_files( files )\n",
    "\n",
    "    # Cria um DataFrame do pandas com uma √∫nica coluna chamada 'lyric'.\n",
    "    df = pd.DataFrame( { \"lyric\": texts } )\n",
    "\n",
    "    # Embaralha as linhas do DataFrame para garantir uma ordem aleat√≥ria dos dados.\n",
    "    # 'frac=1' indica que todas as linhas devem ser amostradas.\n",
    "    df = df.sample( frac = 1 )\n",
    "\n",
    "    # Reseta o √≠ndice do DataFrame.\n",
    "    # 'drop=True' impede que o √≠ndice antigo seja adicionado como uma nova coluna.\n",
    "    df = df.reset_index( drop = True )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Cria o DataFrame de treino chamando a fun√ß√£o com o nome da pasta 'train'.\n",
    "train_df = get_data_from_text_files( \"train\" )\n",
    "\n",
    "# Cria o DataFrame de teste/valida√ß√£o chamando a fun√ß√£o com o nome da pasta 'test'.\n",
    "test_df = get_data_from_text_files( \"test\" )\n",
    "\n",
    "# Imprime o tamanho (n√∫mero de linhas) do DataFrame de treino.\n",
    "print( f\"Tamanho do DataFrame de Treino: {len( train_df )}\" )\n",
    "\n",
    "# Imprime o tamanho (n√∫mero de linhas) do DataFrame de teste/valida√ß√£o.\n",
    "print( f\"Tamanho do DataFrame de Teste/Valida√ß√£o: {len( test_df )}\" )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do DataFrame de Treino: 1444\n",
      "Tamanho do DataFrame de Teste/Valida√ß√£o: 361\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "id": "aa3089bbffe5c2b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:07.525718Z",
     "start_time": "2025-04-09T13:25:07.496734Z"
    }
   },
   "source": [
    "train_df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               lyric\n",
       "0  Ela vem pronta pro abate Novinha fica a vontad...\n",
       "1  Meu peda√ßo de pecado De corpo colado Vem dan√ßa...\n",
       "2  L√° vai a chalana Bem longe se vai Navegando no...\n",
       "3  Eu n√£o sei por que estou aqui H√° uma chuva de ...\n",
       "4  J√° esperei meu prometeu No c√©u da terra se per..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ela vem pronta pro abate Novinha fica a vontad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meu peda√ßo de pecado De corpo colado Vem dan√ßa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L√° vai a chalana Bem longe se vai Navegando no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eu n√£o sei por que estou aqui H√° uma chuva de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J√° esperei meu prometeu No c√©u da terra se per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "id": "1413c85f5980aae3",
   "metadata": {},
   "source": [
    "# Pr√©-processamento e Vetoriza√ß√£o de Texto com TensorFlow\n",
    "\n",
    "Este trecho de c√≥digo configura fun√ß√µes para pr√©-processamento e vetoriza√ß√£o de textos utilizando o TensorFlow. Inicialmente, define-se uma fun√ß√£o de normaliza√ß√£o customizada para converter o texto em min√∫sculas, remover tags HTML e caracteres especiais. Em seguida, √© criada uma camada `TextVectorization` do Keras, que √© adaptada ao conjunto de textos de treino para transformar o texto em sequ√™ncias num√©ricas, permitindo a cria√ß√£o e ajuste de um vocabul√°rio customizado. Tamb√©m √© exemplificada a obten√ß√£o do ID de um token especial (`[mask]`) e a fun√ß√£o `encode` para converter novos textos em arrays NumPy contendo suas respectivas sequ√™ncias num√©ricas.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:09.544718Z",
     "start_time": "2025-04-09T13:25:09.523720Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 56,
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def custom_standardization( input_data ):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o para realizar a normaliza√ß√£o de texto. Esta fun√ß√£o converte o texto\n",
    "    para min√∫sculas, remove tags HTML e caracteres especiais.\n",
    "\n",
    "    Args:\n",
    "        input_data (tf.Tensor): Um tensor de string contendo o texto de entrada.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Um tensor de string contendo o texto normalizado.\n",
    "    \"\"\"\n",
    "    # Converter todas as letras do texto de entrada para min√∫sculas.\n",
    "    lowercase = tf.strings.lower( input_data )\n",
    "\n",
    "    # Utiliza uma express√£o regular para remover todas as ocorr√™ncias da tag HTML \"<br />\"\n",
    "    # e as substitui por um espa√ßo em branco.\n",
    "    stripped_html = tf.strings.regex_replace( lowercase, \"<br />\", \" \" )\n",
    "\n",
    "    # Remove caracteres especiais do texto.\n",
    "    # A express√£o regular \"[%s]\" busca qualquer caractere dentro do conjunto especificado.\n",
    "    # `re.escape()` √© usado para garantir que os caracteres especiais na string de pontua√ß√£o\n",
    "    # sejam tratados literalmente na express√£o regular.\n",
    "    return tf.strings.regex_replace(\n",
    "            stripped_html,\n",
    "            \"[%s]\" % re.escape( \"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\" ),\n",
    "            \"\"\n",
    "    )"
   ],
   "id": "e617962188106c4a"
  },
  {
   "cell_type": "code",
   "id": "7d8904fb48419e91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:09.671727Z",
     "start_time": "2025-04-09T13:25:09.658727Z"
    }
   },
   "source": [
    "def get_vectorize_layer( texts: list[ str ], vocab_size: int, max_seq: int, special_tokens: list = [ \"[MASK]\" ]\n",
    "                         ) -> TextVectorization:\n",
    "    \"\"\"\n",
    "    Constr√≥i e retorna uma camada de vetoriza√ß√£o de texto (TextVectorization) do Keras.\n",
    "\n",
    "    Esta camada converte uma lista de strings de texto em sequ√™ncias de inteiros,\n",
    "    onde cada inteiro corresponde a um token no vocabul√°rio.\n",
    "\n",
    "    Args:\n",
    "        texts (list[str]): Uma lista de strings representando os textos de entrada\n",
    "                           usados para adaptar a camada de vetoriza√ß√£o.\n",
    "        vocab_size (int): O tamanho m√°ximo do vocabul√°rio a ser criado. A camada\n",
    "                          manter√° os `vocab_size` tokens mais frequentes.\n",
    "        max_seq (int): O comprimento m√°ximo da sequ√™ncia de sa√≠da. Sequ√™ncias mais\n",
    "                       longas ser√£o truncadas e sequ√™ncias mais curtas ser√£o preenchidas\n",
    "                       para atingir este comprimento.\n",
    "        special_tokens (list[str], optional): Uma lista de tokens especiais que ser√£o\n",
    "                                                adicionados ao vocabul√°rio. Por padr√£o,\n",
    "                                                cont√©m apenas o token '[MASK]'.\n",
    "\n",
    "    Returns:\n",
    "        TextVectorization: Uma camada de vetoriza√ß√£o de texto do Keras configurada e adaptada.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cria uma inst√¢ncia da camada TextVectorization do Keras.\n",
    "    vectorize_layer = TextVectorization(\n",
    "            max_tokens = vocab_size,  # Define o n√∫mero m√°ximo de tokens no vocabul√°rio.\n",
    "            output_mode = \"int\",\n",
    "            # Configura a sa√≠da para ser uma sequ√™ncia de n√∫meros inteiros (√≠ndices do vocabul√°rio).\n",
    "            standardize = custom_standardization,\n",
    "            # Especifica a fun√ß√£o de pr√©-processamento personalizada a ser aplicada ao texto.\n",
    "            output_sequence_length = max_seq,  # Define o comprimento fixo para todas as sequ√™ncias de sa√≠da.\n",
    "    )\n",
    "\n",
    "    # Adapta a camada de vetoriza√ß√£o aos textos de entrada fornecidos.\n",
    "    # Este processo calcula a frequ√™ncia dos tokens nos textos e constr√≥i o vocabul√°rio.\n",
    "    vectorize_layer.adapt( texts )\n",
    "\n",
    "    # Obt√©m o vocabul√°rio aprendido pela camada de vetoriza√ß√£o.\n",
    "    # O vocabul√°rio √© uma lista de strings, onde cada string √© um token.\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "    # Por padr√£o, a camada TextVectorization adiciona dois tokens especiais ao in√≠cio do vocabul√°rio:\n",
    "    # - \"\" (string vazia) com √≠ndice 0, usado para padding (preenchimento).\n",
    "    # - \"[UNK]\" (token desconhecido) com √≠ndice 1, usado para palavras que n√£o est√£o no vocabul√°rio.\n",
    "\n",
    "    # Esta se√ß√£o ajusta o vocabul√°rio para incluir os tokens especiais fornecidos pelo usu√°rio\n",
    "    # e remover os tokens padr√£o iniciais para liberar espa√ßo.\n",
    "    # - `vocab[2:]`: Seleciona todos os tokens a partir do terceiro elemento (√≠ndices 2 em diante),\n",
    "    #                ignorando \"\" e \"[UNK]\".\n",
    "    # - `vocab_size - len(special_tokens)`: Calcula quantos tokens do vocabul√°rio original\n",
    "    #                                      podem ser mantidos, considerando o espa√ßo necess√°rio\n",
    "    #                                      para os tokens especiais.\n",
    "    vocab = vocab[ 2: vocab_size - len( special_tokens ) ] + special_tokens\n",
    "\n",
    "    # Define o vocabul√°rio da camada de vetoriza√ß√£o com o vocabul√°rio modificado.\n",
    "    # Isso garante que os tokens especiais sejam reconhecidos pela camada.\n",
    "    vectorize_layer.set_vocabulary( vocab )\n",
    "\n",
    "    # Retorna a camada de vetoriza√ß√£o configurada.\n",
    "    return vectorize_layer"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "id": "14af57ee8c312389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:12.804787Z",
     "start_time": "2025-04-09T13:25:09.768793Z"
    }
   },
   "source": [
    "# Extrai os valores da coluna 'lyric' do DataFrame de treino e os converte para uma lista.\n",
    "train_texts = train_df.lyric.values.tolist()\n",
    "\n",
    "# Cria a camada de vetoriza√ß√£o de texto, utilizando apenas os textos de treinamento para adaptar o vocabul√°rio.\n",
    "vectorize_layer = get_vectorize_layer(\n",
    "        train_texts,\n",
    "        config.VOCAB_SIZE,\n",
    "        config.MAX_LEN,\n",
    "        special_tokens = [ \"[mask]\" ],\n",
    ")\n",
    "\n",
    "# Processamento de um novo dado: \"[mask]\"\n",
    "# - Aplica a fun√ß√£o de pr√©-processamento\n",
    "# - Divide em token, pega seu id e cria uma sequ√™ncia de comprimento config.MAX_LEN\n",
    "# - Converte o resultado para um array NumPy\n",
    "# - Pega o id do \"[mask]\"\n",
    "mask_token_id = vectorize_layer( [ \"[mask]\" ] ).cpu().numpy()[ 0 ][ 0 ]"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "id": "cf38423e529f2726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:12.850786Z",
     "start_time": "2025-04-09T13:25:12.835790Z"
    }
   },
   "source": [
    "def encode( texts: list[ str ] ) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Converte uma lista de strings de texto em um array NumPy de sequ√™ncias num√©ricas.\n",
    "\n",
    "    Args:\n",
    "        texts (list[str]): Uma lista de strings representando os textos de entrada a serem codificados.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Um tensor do TensorFlow contendo as sequ√™ncias num√©ricas correspondentes aos textos de entrada.\n",
    "    \"\"\"\n",
    "    # Aplica a camada de vetoriza√ß√£o aos textos de entrada para obter as sequ√™ncias num√©ricas.\n",
    "    encoded_texts = vectorize_layer( texts )\n",
    "    # Retorna as sequ√™ncias codificadas como um array NumPy.\n",
    "    return encoded_texts.numpy()"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "id": "7f5c334136918c96",
   "metadata": {},
   "source": [
    "# Gera√ß√£o de Dados Mascarados para Treinamento\n",
    "\n",
    "Esta fun√ß√£o, `get_masked_input_and_labels`, √© respons√°vel por preparar os dados para treinamento de modelos de linguagem que utilizam mascaramento, como os modelos de predi√ß√£o de tokens (por exemplo, BERT). Ela cria uma vers√£o mascarada das sequ√™ncias num√©ricas e gera os r√≥tulos correspondentes, al√©m de definir pesos para o c√°lculo da perda, de forma que apenas os tokens realmente mascarados contribuam para o treinamento.\n",
    "\n",
    "**Resumo do Processo:**\n",
    "\n",
    "- **Cria√ß√£o da M√°scara Inicial:**\n",
    "  Um array de booleanos (`inp_mask`) √© criado, onde cada posi√ß√£o √© selecionada com probabilidade de 15% para ser mascarada. Tokens especiais (com IDs menores ou iguais a 2) s√£o exclu√≠dos do mascaramento.\n",
    "\n",
    "- **Defini√ß√£o dos R√≥tulos:**\n",
    "  Um array `labels` √© inicializado com valor -1, indicando posi√ß√µes que n√£o ser√£o consideradas na perda. Em seguida, os IDs dos tokens selecionados pela m√°scara s√£o copiados para `labels`, marcando os tokens a serem previstos.\n",
    "\n",
    "- **Gera√ß√£o da Entrada Mascarada:**\n",
    "  Uma c√≥pia dos textos codificados √© feita e, para 90% dos tokens inicialmente selecionados, os valores s√£o substitu√≠dos pelo token de m√°scara (`mask_token_id`).\n",
    "\n",
    "- **Substitui√ß√£o por Tokens Aleat√≥rios:**\n",
    "  Entre os tokens mascarados, 1/9 ser√£o substitu√≠dos por tokens aleat√≥rios (dentro de um intervalo espec√≠fico), adicionando variabilidade √† tarefa de predi√ß√£o.\n",
    "\n",
    "- **C√°lculo dos Pesos:**\n",
    "  Um array de pesos (`sample_weights`) √© criado para indicar que somente os tokens mascarados (diferentes de -1 nos labels) devem ser considerados no c√°lculo da perda.\n",
    "\n",
    "- **Sa√≠da:**\n",
    "  A fun√ß√£o retorna a entrada modificada com tokens mascarados (`encoded_texts_masked`), os r√≥tulos (`y_labels` que cont√™m os tokens originais) e os pesos amostrais para treinamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "977366761074a52b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:12.896787Z",
     "start_time": "2025-04-09T13:25:12.881786Z"
    }
   },
   "source": [
    "def get_masked_input_and_labels( encoded_texts ):\n",
    "    \"\"\"\n",
    "    Cria as entradas mascaradas e os r√≥tulos para a tarefa de Masked Language Modeling (MLM).\n",
    "\n",
    "    Esta fun√ß√£o recebe um array NumPy de textos codificados e realiza as seguintes etapas:\n",
    "    1. Seleciona aleatoriamente uma pequena porcentagem dos tokens para mascarar.\n",
    "    2. Garante que tokens especiais n√£o sejam mascarados.\n",
    "    3. Cria um array de r√≥tulos contendo os IDs dos tokens originais que foram mascarados.\n",
    "    4. Cria uma c√≥pia dos textos codificados de entrada e aplica o mascaramento:\n",
    "       - 90% dos tokens selecionados para mascaramento s√£o substitu√≠dos pelo token de m√°scara (`mask_token_id`).\n",
    "       - 10% dos tokens selecionados para mascaramento s√£o substitu√≠dos por um token aleat√≥rio do vocabul√°rio.\n",
    "    5. Cria um array de pesos de amostra para garantir que a perda seja calculada apenas para os tokens mascarados.\n",
    "\n",
    "    Args:\n",
    "        encoded_texts (np.ndarray): Um array NumPy de inteiros representando os textos codificados.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Uma tupla contendo tr√™s arrays NumPy:\n",
    "            - encoded_texts_masked (np.ndarray): A vers√£o da entrada com alguns tokens substitu√≠dos por [MASK] ou por tokens aleat√≥rios.\n",
    "            - y_labels (np.ndarray): Os tokens originais da entrada, que servem como os r√≥tulos para o treinamento (apenas nas posi√ß√µes mascaradas).\n",
    "            - sample_weights (np.ndarray): Um array de pesos que indica quais posi√ß√µes em `y_labels` devem ser consideradas no c√°lculo da perda.\n",
    "    \"\"\"\n",
    "    # Cria um array booleano com a mesma forma de encoded_texts. Cada elemento ser√° True com uma probabilidade de 0.15.\n",
    "    # Isso indica as posi√ß√µes onde os tokens podem ser mascarados.\n",
    "    inp_mask = np.random.rand( *encoded_texts.shape ) < 0.15\n",
    "\n",
    "    # Garante que os tokens com IDs menores ou iguais a 2 (geralmente tokens especiais como padding ou desconhecido)\n",
    "    # n√£o sejam considerados para mascaramento. Define as posi√ß√µes correspondentes em inp_mask como False.\n",
    "    inp_mask[ encoded_texts <= 2 ] = False\n",
    "\n",
    "    # Inicializa um array com a mesma forma de encoded_texts preenchido com o valor -1.\n",
    "    # Este array armazenar√° os r√≥tulos para a tarefa de previs√£o. O valor -1 indica que o token naquela posi√ß√£o n√£o √© um alvo para previs√£o.\n",
    "    labels = -1 * np.ones( encoded_texts.shape, dtype = int )\n",
    "\n",
    "    # Para as posi√ß√µes onde inp_mask √© True (ou seja, os tokens selecionados para mascaramento),\n",
    "    # os valores dos IDs dos tokens originais (de encoded_texts) s√£o atribu√≠dos ao array de labels.\n",
    "    # Assim, labels conter√° os IDs dos tokens que foram mascarados, e o modelo aprender√° a prever esses IDs.\n",
    "    labels[ inp_mask ] = encoded_texts[ inp_mask ]\n",
    "\n",
    "    # Cria uma c√≥pia do array encoded_texts original. Esta c√≥pia ser√° modificada para conter as entradas mascaradas.\n",
    "    encoded_texts_masked = np.copy( encoded_texts )\n",
    "\n",
    "    # Cria uma nova m√°scara booleana, inp_mask_2mask.\n",
    "    # Ela ser√° True apenas nas posi√ß√µes onde inp_mask tamb√©m √© True (o token foi selecionado para mascaramento)\n",
    "    # E um novo n√∫mero aleat√≥rio gerado para essa posi√ß√£o √© menor que 0.9.\n",
    "    # Isso significa que apenas 90% dos 15% dos tokens selecionados inicialmente para mascaramento ser√£o, de fato, mascarados.\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand( *encoded_texts.shape ) < 0.90)\n",
    "\n",
    "    # Nas posi√ß√µes onde inp_mask_2mask √© True, o token correspondente em encoded_texts_masked √© substitu√≠do pelo ID do token de m√°scara.\n",
    "    encoded_texts_masked[ inp_mask_2mask ] = mask_token_id\n",
    "\n",
    "    # Cria uma nova m√°scara booleana, inp_mask_2random.\n",
    "    # Ela ser√° True apenas nas posi√ß√µes onde inp_mask_2mask tamb√©m √© True (o token foi selecionado para ser mascarado)\n",
    "    # E um novo n√∫mero aleat√≥rio gerado para essa posi√ß√£o √© menor que 1/9.\n",
    "    # Isso significa que aproximadamente 1/9 dos 90% dos tokens que seriam mascarados ser√£o, em vez disso, substitu√≠dos por um token aleat√≥rio.\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand( *encoded_texts.shape ) < 1 / 9)\n",
    "\n",
    "    # Nas posi√ß√µes onde inp_mask_2random √© True, o token em encoded_texts_masked √© substitu√≠do por um token aleat√≥rio.\n",
    "    # np.random.randint(3, mask_token_id, inp_mask_2random.sum()) gera um array de n√∫meros inteiros aleat√≥rios\n",
    "    # entre 3 (inclusive) e mask_token_id (exclusive). Come√ßa em 3 porque as primeiras posi√ß√µes (0, 1, 2) geralmente\n",
    "    # s√£o reservadas para tokens especiais como padding e [UNK].\n",
    "    encoded_texts_masked[ inp_mask_2random ] = np.random.randint(\n",
    "            3, mask_token_id, inp_mask_2random.sum()\n",
    "    )\n",
    "\n",
    "    # Cria um array com a mesma forma de labels preenchido com o valor 1.\n",
    "    # Este array ser√° usado como pesos de amostra durante o treinamento.\n",
    "    sample_weights = np.ones( labels.shape )\n",
    "\n",
    "    # Nas posi√ß√µes onde o valor em labels √© -1 (ou seja, os tokens n√£o foram mascarados),\n",
    "    # o valor correspondente em sample_weights √© definido como 0.\n",
    "    # Isso garante que a perda durante o treinamento seja calculada apenas para os tokens que foram realmente mascarados.\n",
    "    sample_weights[ labels == -1 ] = 0\n",
    "\n",
    "    # Cria uma c√≥pia do array encoded_texts original. Este array representa os r√≥tulos originais para cada token.\n",
    "    y_labels = np.copy( encoded_texts )\n",
    "\n",
    "    # Retorna uma tupla contendo:\n",
    "    # - A entrada com alguns tokens substitu√≠dos por [MASK] ou tokens aleat√≥rios.\n",
    "    # - Os r√≥tulos originais (para calcular a perda nos tokens mascarados).\n",
    "    # - Os pesos de amostra para focar o treinamento nos tokens mascarados.\n",
    "    return encoded_texts_masked, y_labels, sample_weights"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "id": "fc0c9d5289de1a13",
   "metadata": {},
   "source": [
    "# Prepara√ß√£o dos Dados para Treinamento de Modelo de Linguagem Mascarada\n",
    "\n",
    "Esta se√ß√£o codifica as letras em sequ√™ncias num√©ricas, aplica o mascaramento necess√°rio para a tarefa de predi√ß√£o de tokens e cria os datasets do TensorFlow para treinamento e valida√ß√£o. Com o mascaramento aplicado, os r√≥tulos s√£o definidos a partir dos tokens originais e os pesos s√£o configurados de forma que apenas os tokens mascarados contribuam para a perda durante o treinamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "55a74b6efbf9b0d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:13.006787Z",
     "start_time": "2025-04-09T13:25:12.929788Z"
    }
   },
   "source": [
    "# Codifica√ß√£o das letras\n",
    "# Utiliza a fun√ß√£o 'encode' para converter as letras (lyrics) do DataFrame de treino em sequ√™ncias num√©ricas.\n",
    "x_train = encode( train_df.lyric.values )\n",
    "# Utiliza a fun√ß√£o 'encode' para converter as letras (lyrics) do DataFrame de teste em sequ√™ncias num√©ricas.\n",
    "x_test = encode( test_df.lyric.values )"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "id": "d546c097a2d0259e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:13.070786Z",
     "start_time": "2025-04-09T13:25:13.040789Z"
    }
   },
   "source": [
    "# Mascaramento das letras para a tarefa de Masked Language Modeling (MLM)\n",
    "# Aplica a fun√ß√£o 'get_masked_input_and_labels' aos dados de treino codificados para criar as entradas mascaradas,\n",
    "# os r√≥tulos (tokens originais) e os pesos de amostra para o treinamento.\n",
    "x_train_masked, y_train_labels, train_sample_weights = get_masked_input_and_labels(\n",
    "        x_train\n",
    ")\n",
    "# Aplica a fun√ß√£o 'get_masked_input_and_labels' aos dados de teste codificados para criar as entradas mascaradas,\n",
    "# os r√≥tulos (tokens originais) e os pesos de amostra para a valida√ß√£o.\n",
    "x_val_masked, y_val_labels, val_sample_weights = get_masked_input_and_labels(\n",
    "        x_test\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:13.132787Z",
     "start_time": "2025-04-09T13:25:13.104789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cria√ß√£o dos Datasets TensorFlow para otimizar o pipeline de dados\n",
    "\n",
    "# Dataset de Treino para Masked Language Modeling (MLM)\n",
    "# Cria um dataset TensorFlow a partir das fatias dos arrays NumPy: entradas mascaradas, r√≥tulos e pesos de amostra do treino.\n",
    "mlm_train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        (x_train_masked, y_train_labels, train_sample_weights)\n",
    ")\n",
    "# Configura√ß√µes do dataset de treino:\n",
    "# - shuffle(1000): Embaralha os elementos do dataset. Um buffer de 1000 elementos √© usado para o embaralhamento.\n",
    "# - batch(config.BATCH_SIZE): Agrupa os elementos do dataset em lotes do tamanho definido em 'config.BATCH_SIZE'.\n",
    "# - prefetch(tf.data.AUTOTUNE): Permite que o dataset prepare os pr√≥ximos lotes em segundo plano enquanto o lote atual est√° sendo processado, melhorando a performance.\n",
    "mlm_train_ds = mlm_train_ds.shuffle( 1000 ).batch( config.BATCH_SIZE ).prefetch( tf.data.AUTOTUNE )\n",
    "\n",
    "# Dataset de Teste (Valida√ß√£o) para Masked Language Modeling (MLM)\n",
    "# Cria um dataset TensorFlow a partir das fatias dos arrays NumPy: entradas mascaradas, r√≥tulos e pesos de amostra do teste/valida√ß√£o.\n",
    "mlm_val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        (x_val_masked, y_val_labels, val_sample_weights)\n",
    ")\n",
    "# Configura√ß√µes do dataset de teste/valida√ß√£o:\n",
    "# - batch(config.BATCH_SIZE): Agrupa os elementos do dataset em lotes do tamanho definido em 'config.BATCH_SIZE'.\n",
    "# - prefetch(tf.data.AUTOTUNE): Permite que o dataset prepare os pr√≥ximos lotes em segundo plano enquanto o lote atual est√° sendo processado, melhorando a performance.\n",
    "# Observa√ß√£o: O dataset de teste/valida√ß√£o geralmente n√£o √© embaralhado, pois a ordem dos dados n√£o deve influenciar a avalia√ß√£o.\n",
    "mlm_val_ds = mlm_val_ds.batch( config.BATCH_SIZE ).prefetch( tf.data.AUTOTUNE )"
   ],
   "id": "5a6318f63c1b3e6",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# M√≥dulo BERT e Self-Attention\n",
    "\n",
    "Este bloco define a fun√ß√£o `bert_module`, que implementa uma camada Transformer com self-attention multi-cabe√ßa e componentes feed-forward. A fun√ß√£o inclui opera√ß√µes de dropout e normaliza√ß√£o de camada para melhorar a regulariza√ß√£o e estabilidade do treinamento, preparando a entrada para representa√ß√µes contextuais enriquecidas.\n"
   ],
   "id": "67be1561f10b4df5"
  },
  {
   "cell_type": "code",
   "id": "5ac4d706",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:13.179793Z",
     "start_time": "2025-04-09T13:25:13.166789Z"
    }
   },
   "source": [
    "def bert_module( query, key, value, i ):\n",
    "    \"\"\"\n",
    "    Implementa um bloco (m√≥dulo) do encoder Transformer, similar aos utilizados em modelos como o BERT.\n",
    "\n",
    "    Este m√≥dulo realiza opera√ß√µes de multi-cabe√ßas de autoaten√ß√£o, dropout, normaliza√ß√£o de camada e uma rede feed-forward.\n",
    "\n",
    "    Args:\n",
    "        query (tf.Tensor): Tensor de entrada representando a consulta (query) para a camada de aten√ß√£o.\n",
    "        key (tf.Tensor): Tensor de entrada representando a chave (key) para a camada de aten√ß√£o.\n",
    "        value (tf.Tensor): Tensor de entrada representando o valor (value) para a camada de aten√ß√£o.\n",
    "        i (int): Um √≠ndice para identificar este m√≥dulo espec√≠fico do encoder (√∫til quando se tem m√∫ltiplas camadas).\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: O tensor de sa√≠da deste m√≥dulo do encoder, com a mesma dimens√£o da entrada,\n",
    "                   mas com representa√ß√µes contextuais aprimoradas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Camada de Multi-Cabe√ßas de Autoaten√ß√£o\n",
    "    # A autoaten√ß√£o permite que o modelo aprenda as rela√ß√µes entre as diferentes palavras em uma sequ√™ncia,\n",
    "    # ponderando a import√¢ncia de cada palavra em rela√ß√£o √†s outras.\n",
    "    # Utilizar m√∫ltiplas \"cabe√ßas\" de aten√ß√£o permite que o modelo capture diferentes tipos de rela√ß√µes\n",
    "    # e foque em diferentes partes da sequ√™ncia simultaneamente.\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "            num_heads = config.NUM_HEAD,  # Define o n√∫mero de cabe√ßas de aten√ß√£o.\n",
    "            key_dim = config.EMBED_DIM // config.NUM_HEAD,  # Define a dimens√£o de cada cabe√ßa de aten√ß√£o.\n",
    "            name = f\"encoder_{i}_multiheadattention\",  # Nomeia a camada para f√°cil identifica√ß√£o.\n",
    "    )( query, key, value )\n",
    "\n",
    "    # Dropout para Regulariza√ß√£o\n",
    "    # O dropout √© uma t√©cnica de regulariza√ß√£o que ajuda a prevenir o overfitting (sobreajuste)\n",
    "    # ao desativar aleatoriamente uma propor√ß√£o de neur√¥nios durante o treinamento.\n",
    "    attention_output = layers.Dropout( 0.1, name = f\"encoder_{i}_att_dropout\" )(\n",
    "            attention_output\n",
    "    )\n",
    "\n",
    "    # Normaliza√ß√£o de Camada (Layer Normalization) da Sa√≠da da Aten√ß√£o\n",
    "    # A normaliza√ß√£o de camada √© aplicada para estabilizar o processo de aprendizado e acelerar a converg√™ncia.\n",
    "    # Ela normaliza as ativa√ß√µes dentro de cada amostra em um lote.\n",
    "    # Aqui, a sa√≠da da aten√ß√£o √© adicionada √† entrada original (query) antes da normaliza√ß√£o,\n",
    "    # uma t√©cnica conhecida como \"residual connection\" (conex√£o residual), que ajuda no fluxo de informa√ß√µes.\n",
    "    attention_output = layers.LayerNormalization(\n",
    "            epsilon = 1e-6, name = f\"encoder_{i}_att_layernormalization\"\n",
    "    )( query + attention_output )\n",
    "\n",
    "    # Rede Feed-Forward (FFN)\n",
    "    # A rede feed-forward √© composta por duas camadas densas com uma fun√ß√£o de ativa√ß√£o ReLU entre elas.\n",
    "    # Ela serve para processar a sa√≠da da camada de aten√ß√£o de forma n√£o linear e capturar padr√µes mais complexos.\n",
    "    ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense( config.FF_DIM, activation = \"relu\" ),\n",
    "                # A primeira camada densa expande a dimens√£o para config.FF_DIM e aplica a ativa√ß√£o ReLU.\n",
    "                layers.Dense( config.EMBED_DIM ),\n",
    "                # A segunda camada densa projeta a sa√≠da de volta para a dimens√£o original (config.EMBED_DIM).\n",
    "            ],\n",
    "            name = f\"encoder_{i}_ffn\",  # Nomeia a rede feed-forward.\n",
    "    )\n",
    "\n",
    "    # Aplica√ß√£o da Rede Feed-Forward\n",
    "    ffn_output = ffn( attention_output )\n",
    "\n",
    "    # Dropout ap√≥s a Rede Feed-Forward para Regulariza√ß√£o\n",
    "    ffn_output = layers.Dropout( 0.1, name = f\"encoder_{i}_ffn_dropout\" )(\n",
    "            ffn_output\n",
    "    )\n",
    "\n",
    "    # Normaliza√ß√£o de Camada da Sa√≠da da Rede Feed-Forward\n",
    "    # Similar √† normaliza√ß√£o aplicada ap√≥s a aten√ß√£o, aqui a sa√≠da da rede feed-forward √© adicionada\n",
    "    # √† sa√≠da da camada de aten√ß√£o (antes da FFN) atrav√©s de uma conex√£o residual, e ent√£o normalizada.\n",
    "    sequence_output = layers.LayerNormalization(\n",
    "            epsilon = 1e-6, name = f\"encoder_{i}_ffn_layernormalization\"\n",
    "    )( attention_output + ffn_output )\n",
    "\n",
    "    # O resultado final (sequence_output) possui a mesma dimensionalidade da entrada (query),\n",
    "    # mas as representa√ß√µes dos tokens foram enriquecidas com informa√ß√µes contextuais aprendidas\n",
    "    # atrav√©s das camadas de aten√ß√£o e feed-forward.\n",
    "    return sequence_output"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fun√ß√£o de Perda e Modelo de Linguagem Mascarada\n",
    "\n",
    "Nesta se√ß√£o, s√£o definidos a fun√ß√£o de perda (entropia cruzada esparsa) e o acompanhamento da perda usando m√©tricas. Em seguida, √© implementada a classe `MaskedLanguageModel`, que estende o `keras.Model` para customizar o c√°lculo da perda e m√©tricas, permitindo um controle mais refinado durante o treinamento do modelo de linguagem mascarada.\n"
   ],
   "id": "4a45ba05c147ee84"
  },
  {
   "cell_type": "code",
   "id": "9af62811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:13.224785Z",
     "start_time": "2025-04-09T13:25:13.211789Z"
    }
   },
   "source": [
    "# Fun√ß√£o de perda de entropia cruzada esparsa\n",
    "# Esta fun√ß√£o de perda √© apropriada para tarefas de classifica√ß√£o onde os r√≥tulos (y) s√£o fornecidos como √≠ndices inteiros (sparse labels).\n",
    "# O par√¢metro 'reduction = None' especifica que a perda n√£o deve ser agregada (m√©dia ou soma) ao longo do lote.\n",
    "# Isso far√° com que a fun√ß√£o de perda retorne um valor de perda para cada exemplo individual no lote.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy( reduction = None )\n",
    "\n",
    "# Objeto para rastrear a m√©dia dos valores de perda ao longo do treinamento (ou avalia√ß√£o)\n",
    "# 'keras.metrics.Mean' √© uma classe que calcula a m√©dia de valores ao longo do tempo.\n",
    "# Aqui, ser√° usada para manter o controle da perda m√©dia durante cada √©poca de treinamento.\n",
    "loss_tracker = keras.metrics.Mean( name = \"loss\" )\n",
    "\n",
    "\n",
    "# Classe customizada para o modelo de Masked Language Modeling (MLM)\n",
    "# Esta classe herda de 'keras.Model' e permite definir o comportamento de perda e m√©tricas de maneira personalizada,\n",
    "# oferecendo maior controle sobre o processo de treinamento.\n",
    "class MaskedLanguageModel( keras.Model ):\n",
    "\n",
    "    # M√©todo para calcular a perda durante o treinamento ou avalia√ß√£o\n",
    "    # 'x' representa as entradas, 'y' os r√≥tulos verdadeiros, 'y_pred' as previs√µes do modelo\n",
    "    # e 'sample_weight' os pesos das amostras (usados aqui para focar nos tokens mascarados).\n",
    "    def compute_loss( self, x = None, y = None, y_pred = None, sample_weight = None ):\n",
    "        # Como a fun√ß√£o de perda 'loss_fn' foi inicializada com 'reduction = None', ela retorna um tensor\n",
    "        # contendo um valor de perda para cada exemplo no lote.\n",
    "        # Queremos que a perda total do lote seja o valor a ser otimizado, ent√£o somamos todas as perdas individuais.\n",
    "        loss = loss_fn( y, y_pred, sample_weight )\n",
    "\n",
    "        # Atualiza o estado do rastreador de perda ('loss_tracker') com os valores de perda do lote atual.\n",
    "        # O par√¢metro 'sample_weight' garante que a m√©dia da perda seja ponderada corretamente pelos tokens que foram mascarados.\n",
    "        loss_tracker.update_state( loss, sample_weight = sample_weight )\n",
    "\n",
    "        # Retorna a soma total das perdas do lote. Este valor ser√° usado pelo otimizador para ajustar os pesos do modelo.\n",
    "        return keras.ops.sum( loss )\n",
    "\n",
    "    # M√©todo para calcular as m√©tricas durante o treinamento ou avalia√ß√£o\n",
    "    # Recebe as entradas, os r√≥tulos verdadeiros, as previs√µes e os pesos das amostras.\n",
    "    def compute_metrics( self, x, y, y_pred, sample_weight ):\n",
    "        # Retorna um dicion√°rio contendo as m√©tricas a serem monitoradas.\n",
    "        # Neste caso, estamos retornando apenas a perda m√©dia calculada pelo 'loss_tracker'.\n",
    "        return { \"loss\": loss_tracker.result() }\n",
    "\n",
    "    # Propriedade para listar os objetos de m√©trica do modelo\n",
    "    # Este m√©todo √© importante para que o Keras possa gerenciar o estado das m√©tricas automaticamente.\n",
    "    # Ao listar os objetos 'Metric' aqui, o m√©todo 'reset_states()' ser√° chamado automaticamente no in√≠cio de cada √©poca\n",
    "    # de treinamento ou no in√≠cio da fun√ß√£o 'evaluate()'.\n",
    "    # Se esta propriedade n√£o for implementada, seria necess√°rio chamar 'reset_states()' manualmente.\n",
    "    @property\n",
    "    def metrics( self ):\n",
    "        return [ loss_tracker ]"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Constru√ß√£o do Modelo BERT para Masked Language Modeling\n",
    "\n",
    "Esta parte do notebook monta o modelo BERT adaptado para a tarefa de Masked Language Modeling (MLM). Inclui a defini√ß√£o das camadas de entrada, incorpora√ß√£o de tokens e posicional, aplica√ß√£o dos blocos de Transformer (atrav√©s do `bert_module`) e, por fim, a camada de classifica√ß√£o que gera as predi√ß√µes dos tokens mascarados. O modelo √© compilado utilizando o otimizador Adam.\n"
   ],
   "id": "b75bf4d58716f829"
  },
  {
   "cell_type": "code",
   "id": "83087148",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:13.302786Z",
     "start_time": "2025-04-09T13:25:13.257790Z"
    }
   },
   "source": [
    "def create_masked_language_bert_model():\n",
    "    \"\"\"\n",
    "    Cria um modelo BERT para a tarefa de Masked Language Modeling (MLM).\n",
    "\n",
    "    Este modelo consiste em uma camada de embedding para as palavras, uma camada de embedding para as posi√ß√µes,\n",
    "    m√∫ltiplos blocos Transformer (definidos na fun√ß√£o `bert_module`) e uma camada de classifica√ß√£o para prever os tokens mascarados.\n",
    "\n",
    "    Returns:\n",
    "        MaskedLanguageModel: Uma inst√¢ncia do modelo BERT configurada para MLM.\n",
    "    \"\"\"\n",
    "    # Define a camada de entrada do modelo.\n",
    "    # Espera-se que a entrada seja uma sequ√™ncia de tokens (IDs inteiros) com comprimento m√°ximo definido em 'config.MAX_LEN'.\n",
    "    inputs = layers.Input( (config.MAX_LEN,), dtype = \"int64\" )\n",
    "\n",
    "    # Cria a camada de embedding de palavras (word embeddings).\n",
    "    # Esta camada transforma cada ID de token em um vetor denso de tamanho 'config.EMBED_DIM',\n",
    "    # representando a sem√¢ntica da palavra.\n",
    "    word_embeddings = layers.Embedding(\n",
    "            config.VOCAB_SIZE, config.EMBED_DIM, name = \"word_embedding\"\n",
    "    )( inputs )\n",
    "\n",
    "    # Cria a camada de embedding de posi√ß√£o (position embeddings) usando a biblioteca TensorFlow Hub.\n",
    "    # Em modelos Transformer, a ordem das palavras √© importante. Essa camada adiciona informa√ß√µes sobre a posi√ß√£o de cada token na sequ√™ncia.\n",
    "    # 'sequence_length' define o comprimento m√°ximo da sequ√™ncia para o qual os embeddings de posi√ß√£o ser√£o gerados.\n",
    "    position_embeddings = keras_hub.layers.PositionEmbedding(\n",
    "            sequence_length = config.MAX_LEN\n",
    "    )( word_embeddings )\n",
    "\n",
    "    # Combina as embeddings de palavras e as embeddings de posi√ß√£o.\n",
    "    # A soma dessas duas embeddings fornece ao modelo informa√ß√µes sobre o significado da palavra e sua posi√ß√£o na frase.\n",
    "    embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "    # Inicializa a sa√≠da do encoder com a combina√ß√£o das embeddings.\n",
    "    encoder_output = embeddings\n",
    "\n",
    "    # Aplica m√∫ltiplos blocos Transformer (definidos na fun√ß√£o 'bert_module').\n",
    "    # O n√∫mero de camadas (blocos) √© definido em 'config.NUM_LAYERS'.\n",
    "    # Cada bloco Transformer realiza autoaten√ß√£o multi-cabe√ßas e uma rede feed-forward.\n",
    "    for i in range( config.NUM_LAYERS ):\n",
    "        encoder_output = bert_module( encoder_output, encoder_output, encoder_output, i )\n",
    "\n",
    "    # Cria a camada de classifica√ß√£o para a tarefa de Masked Language Modeling (MLM).\n",
    "    # Esta camada densa projeta a sa√≠da do encoder de volta para o tamanho do vocabul√°rio ('config.VOCAB_SIZE').\n",
    "    # A fun√ß√£o de ativa√ß√£o 'softmax' transforma as sa√≠das brutas da rede em um vetor de probabilidades,\n",
    "    # onde cada probabilidade representa a chance de um determinado token do vocabul√°rio ser o token mascarado.\n",
    "    mlm_output = layers.Dense( config.VOCAB_SIZE, name = \"mlm_cls\", activation = \"softmax\" )(\n",
    "            encoder_output\n",
    "    )\n",
    "\n",
    "    # Cria uma inst√¢ncia do modelo MaskedLanguageModel que definimos anteriormente.\n",
    "    # As entradas do modelo s√£o as sequ√™ncias de tokens ('inputs') e a sa√≠da √© a previs√£o dos tokens mascarados ('mlm_output').\n",
    "    mlm_model = MaskedLanguageModel( inputs, mlm_output, name = \"masked_bert_model\" )\n",
    "\n",
    "    # Define o otimizador a ser usado para o treinamento.\n",
    "    # 'Adam' √© um algoritmo de otimiza√ß√£o popular. A taxa de aprendizado ('learning_rate') √© definida em 'config.LR'.\n",
    "    optimizer = keras.optimizers.Adam( learning_rate = config.LR )\n",
    "\n",
    "    # Compila o modelo, configurando o otimizador. A fun√ß√£o de perda √© definida dentro da classe MaskedLanguageModel.\n",
    "    mlm_model.compile( optimizer = optimizer )\n",
    "\n",
    "    # Retorna a inst√¢ncia do modelo BERT para MLM.\n",
    "    return mlm_model\n",
    "\n",
    "\n",
    "# Cria um mapeamento do ID do token para o token em si.\n",
    "# 'vectorize_layer.get_vocabulary()' retorna uma lista de todos os tokens no vocabul√°rio.\n",
    "# 'enumerate' cria pares de √≠ndice e token, e 'dict()' transforma esses pares em um dicion√°rio.\n",
    "id2token = dict( enumerate( vectorize_layer.get_vocabulary() ) )\n",
    "\n",
    "# Cria um mapeamento do token para o seu ID.\n",
    "# Este dicion√°rio √© criado invertendo o dicion√°rio 'id2token'. Para cada valor (token) em 'id2token',\n",
    "# sua chave (ID) se torna o novo valor, e o token se torna a nova chave.\n",
    "token2id = { y: x for x, y in id2token.items() }"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Mapeamento de Vocabul√°rio e Callback de Gera√ß√£o de Texto\n",
    "\n",
    "Neste trecho, √© criado um mapeamento bidirecional entre IDs e tokens, facilitando a interpreta√ß√£o das sequ√™ncias processadas. Tamb√©m √© definida a classe `MaskedTextGenerator`, um callback do Keras que, ao final de cada √©poca, imprime exemplos de predi√ß√µes substituindo os tokens mascarados ‚Äì auxiliando na visualiza√ß√£o do desempenho do modelo durante o treinamento.\n"
   ],
   "id": "f2e1371be47078c2"
  },
  {
   "cell_type": "code",
   "id": "874798b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:13.412788Z",
     "start_time": "2025-04-09T13:25:13.338789Z"
    }
   },
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import torch  # Importa torch para verificar o tipo do tensor\n",
    "\n",
    "\n",
    "# Por herdar de keras.callbacks.Callback, esta classe pode ser usada durante o treinamento\n",
    "# para executar a√ß√µes personalizadas ao final de cada √©poca (ou em outros momentos do treinamento).\n",
    "class MaskedTextGenerator( keras.callbacks.Callback ):\n",
    "    def __init__( self, sample_tokens, top_k = 5 ):\n",
    "        \"\"\"\n",
    "        Inicializa o gerador de texto mascarado.\n",
    "\n",
    "        Args:\n",
    "            sample_tokens (np.ndarray ou torch.Tensor): Um exemplo de sequ√™ncia de tokens de entrada contendo o token de m√°scara.\n",
    "            top_k (int, optional): O n√∫mero de candidatos principais a serem considerados para cada posi√ß√£o mascarada. Defaults to 5.\n",
    "        \"\"\"\n",
    "        self.sample_tokens = sample_tokens  # Exemplo de entrada que cont√©m o token de m√°scara para previs√£o.\n",
    "        self.k = top_k  # N√∫mero de candidatos principais (tokens com maior probabilidade) a serem considerados para substituir a m√°scara.\n",
    "\n",
    "    # Converte uma sequ√™ncia de IDs de tokens para uma string leg√≠vel.\n",
    "    def decode( self, tokens ):\n",
    "        \"\"\"\n",
    "        Decodifica uma sequ√™ncia de IDs de tokens para uma string.\n",
    "\n",
    "        Args:\n",
    "            tokens (np.ndarray): Um array NumPy de IDs de tokens.\n",
    "\n",
    "        Returns:\n",
    "            str: A string correspondente √† sequ√™ncia de tokens, com tokens desconhecidos representados por \"[UNK]\" e ignorando o token de padding (ID 0).\n",
    "        \"\"\"\n",
    "        return \" \".join( [ id2token.get( int( t ), \"[UNK]\" ) for t in tokens if t != 0 ] )\n",
    "\n",
    "    # Converte um ID de token para seu token correspondente.\n",
    "    def convert_ids_to_tokens( self, id ):\n",
    "        \"\"\"\n",
    "        Converte um ID de token para o seu token correspondente usando o mapeamento id2token.\n",
    "\n",
    "        Args:\n",
    "            id (int): O ID do token.\n",
    "\n",
    "        Returns:\n",
    "            str: O token correspondente ao ID.\n",
    "        \"\"\"\n",
    "        return id2token[ id ]\n",
    "\n",
    "    # Este m√©todo √© executado ao final de cada √©poca de treinamento.\n",
    "    def on_epoch_end( self, epoch, logs = None ):\n",
    "        \"\"\"\n",
    "        Executa a√ß√µes ao final de cada √©poca de treinamento. Neste caso, gera e imprime as principais previs√µes para o token mascarado no exemplo de entrada.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): O √≠ndice da √©poca que acaba de terminar.\n",
    "            logs (dict, optional): Um dicion√°rio contendo as m√©tricas de perda e outras informa√ß√µes registradas durante a √©poca. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Obt√©m as previs√µes do modelo para o exemplo de entrada fornecido.\n",
    "        # 'self.model' se refere ao modelo que est√° sendo treinado e que este callback est√° associado.\n",
    "        prediction = self.model.predict( self.sample_tokens )\n",
    "\n",
    "        # Procura o √≠ndice onde o token de m√°scara est√° presente na sequ√™ncia de tokens de entrada.\n",
    "        # 'np.where' retorna um array de √≠ndices onde a condi√ß√£o √© verdadeira.\n",
    "        # Como 'self.sample_tokens' pode ser um lote (mesmo que de tamanho 1), pegamos o √≠ndice dentro da sequ√™ncia.\n",
    "        masked_index = np.where( self.sample_tokens == mask_token_id )\n",
    "        # 'masked_index' ser√° uma tupla de arrays (para cada dimens√£o). Pegamos os √≠ndices da segunda dimens√£o (a sequ√™ncia).\n",
    "        masked_index = masked_index[ 1 ]\n",
    "\n",
    "        # 'prediction' ter√° a mesma forma que a entrada, exceto que a √∫ltima dimens√£o conter√° as probabilidades\n",
    "        # para cada token do vocabul√°rio. Pegamos as probabilidades de predi√ß√£o para o token mascarado na sequ√™ncia.\n",
    "        mask_prediction = prediction[ 0 ][ masked_index ]\n",
    "\n",
    "        # Para o token mascarado, obtemos os √≠ndices dos 'top_k' tokens com as maiores probabilidades.\n",
    "        # 'argsort()' retorna os √≠ndices que ordenariam um array. Usamos '[-self.k:]' para pegar os √∫ltimos 'k' √≠ndices (maiores probabilidades)\n",
    "        # e '[::-1]' para inverter a ordem, obtendo os √≠ndices em ordem decrescente de probabilidade.\n",
    "        top_indices = mask_prediction[ 0 ].argsort()[ -self.k: ][ ::-1 ]\n",
    "        # Obtemos os valores das probabilidades correspondentes aos 'top_indices'.\n",
    "        values = mask_prediction[ 0 ][ top_indices ]\n",
    "\n",
    "        # Itera sobre os 'top_k' candidatos previstos.\n",
    "        for i in range( len( top_indices ) ):\n",
    "            # Obt√©m o √≠ndice do token previsto.\n",
    "            p = top_indices[ i ]\n",
    "            # Obt√©m a probabilidade correspondente.\n",
    "            v = values[ i ]\n",
    "\n",
    "            # Cria uma c√≥pia da sequ√™ncia de tokens de entrada para n√£o modificar a original.\n",
    "            # Verifica se self.sample_tokens √© um tensor do PyTorch e o move para a CPU se necess√°rio.\n",
    "            if isinstance( self.sample_tokens, torch.Tensor ):\n",
    "                tokens = np.copy( self.sample_tokens[ 0 ].cpu().numpy() )\n",
    "            else:\n",
    "                tokens = np.copy( self.sample_tokens[ 0 ] )\n",
    "\n",
    "            # Substitui o token de m√°scara na c√≥pia da sequ√™ncia pelo token previsto atual.\n",
    "            tokens[ masked_index[ 0 ] ] = p\n",
    "\n",
    "            # Cria um dicion√°rio contendo o texto original, a previs√£o com o token substitu√≠do,\n",
    "            # a probabilidade da previs√£o e o token previsto.\n",
    "            result = {\n",
    "                \"input_text\": self.decode( self.sample_tokens[ 0 ].cpu().numpy() ) if isinstance( self.sample_tokens,\n",
    "                                                                                                  torch.Tensor\n",
    "                                                                                                  ) else self.decode(\n",
    "                        self.sample_tokens[ 0 ]\n",
    "                ),\n",
    "                \"prediction\": self.decode( tokens ),\n",
    "                \"probability\": v,\n",
    "                \"predicted mask token\": self.convert_ids_to_tokens( p ),\n",
    "            }\n",
    "            # Imprime o resultado formatado para facilitar a leitura.\n",
    "            pprint( result )"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"masked_bert_model\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"masked_bert_model\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001B[1m \u001B[0m\u001B[1mLayer (type)       \u001B[0m\u001B[1m \u001B[0m‚îÉ\u001B[1m \u001B[0m\u001B[1mOutput Shape     \u001B[0m\u001B[1m \u001B[0m‚îÉ\u001B[1m \u001B[0m\u001B[1m   Param #\u001B[0m\u001B[1m \u001B[0m‚îÉ\u001B[1m \u001B[0m\u001B[1mConnected to     \u001B[0m\u001B[1m \u001B[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ input_layer_4       ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)       ‚îÇ          \u001B[38;5;34m0\u001B[0m ‚îÇ -                 ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mInputLayer\u001B[0m)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ word_embedding      ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  ‚îÇ  \u001B[38;5;34m3,840,000\u001B[0m ‚îÇ input_layer_4[\u001B[38;5;34m0\u001B[0m]‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mEmbedding\u001B[0m)         ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ position_embedding‚Ä¶ ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  ‚îÇ     \u001B[38;5;34m32,768\u001B[0m ‚îÇ word_embedding[\u001B[38;5;34m0\u001B[0m‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mPositionEmbedding\u001B[0m) ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_6 (\u001B[38;5;33mAdd\u001B[0m)         ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  ‚îÇ          \u001B[38;5;34m0\u001B[0m ‚îÇ word_embedding[\u001B[38;5;34m0\u001B[0m‚Ä¶ ‚îÇ\n",
       "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ position_embeddi‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ encoder_0_multihea‚Ä¶ ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  ‚îÇ     \u001B[38;5;34m66,048\u001B[0m ‚îÇ add_6[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],      ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mMultiHeadAttentio‚Ä¶\u001B[0m ‚îÇ                   ‚îÇ            ‚îÇ add_6[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],      ‚îÇ\n",
       "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ add_6[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]       ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ encoder_0_att_drop‚Ä¶ ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  ‚îÇ          \u001B[38;5;34m0\u001B[0m ‚îÇ encoder_0_multih‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mDropout\u001B[0m)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_7 (\u001B[38;5;33mAdd\u001B[0m)         ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  ‚îÇ          \u001B[38;5;34m0\u001B[0m ‚îÇ add_6[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],      ‚îÇ\n",
       "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ encoder_0_att_dr‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ encoder_0_att_laye‚Ä¶ ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  ‚îÇ        \u001B[38;5;34m256\u001B[0m ‚îÇ add_7[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]       ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mLayerNormalizatio‚Ä¶\u001B[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ encoder_0_ffn       ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  ‚îÇ     \u001B[38;5;34m33,024\u001B[0m ‚îÇ encoder_0_att_la‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mSequential\u001B[0m)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ encoder_0_ffn_drop‚Ä¶ ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  ‚îÇ          \u001B[38;5;34m0\u001B[0m ‚îÇ encoder_0_ffn[\u001B[38;5;34m0\u001B[0m]‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mDropout\u001B[0m)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_8 (\u001B[38;5;33mAdd\u001B[0m)         ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  ‚îÇ          \u001B[38;5;34m0\u001B[0m ‚îÇ encoder_0_att_la‚Ä¶ ‚îÇ\n",
       "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ encoder_0_ffn_dr‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ encoder_0_ffn_laye‚Ä¶ ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  ‚îÇ        \u001B[38;5;34m256\u001B[0m ‚îÇ add_8[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]       ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mLayerNormalizatio‚Ä¶\u001B[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ mlm_cls (\u001B[38;5;33mDense\u001B[0m)     ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m,       ‚îÇ  \u001B[38;5;34m3,870,000\u001B[0m ‚îÇ encoder_0_ffn_la‚Ä¶ ‚îÇ\n",
       "‚îÇ                     ‚îÇ \u001B[38;5;34m30000\u001B[0m)            ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)        </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape      </span>‚îÉ<span style=\"font-weight: bold\">    Param # </span>‚îÉ<span style=\"font-weight: bold\"> Connected to      </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ input_layer_4       ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                 ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ word_embedding      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  ‚îÇ  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,000</span> ‚îÇ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ position_embedding‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span> ‚îÇ word_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionEmbedding</span>) ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ word_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>‚Ä¶ ‚îÇ\n",
       "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ position_embeddi‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ encoder_0_multihea‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> ‚îÇ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      ‚îÇ\n",
       "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ encoder_0_att_drop‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ encoder_0_multih‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      ‚îÇ\n",
       "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ encoder_0_att_dr‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ encoder_0_att_laye‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> ‚îÇ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ encoder_0_ffn       ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> ‚îÇ encoder_0_att_la‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ encoder_0_ffn_drop‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ encoder_0_ffn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ encoder_0_att_la‚Ä¶ ‚îÇ\n",
       "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ encoder_0_ffn_dr‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ encoder_0_ffn_laye‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> ‚îÇ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ mlm_cls (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,       ‚îÇ  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,870,000</span> ‚îÇ encoder_0_ffn_la‚Ä¶ ‚îÇ\n",
       "‚îÇ                     ‚îÇ <span style=\"color: #00af00; text-decoration-color: #00af00\">30000</span>)            ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m7,842,352\u001B[0m (29.92 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,842,352</span> (29.92 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m7,842,352\u001B[0m (29.92 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,842,352</span> (29.92 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Instancia√ß√£o e Resumo do Modelo\n",
    "\n",
    "Aqui s√£o processados exemplos de entrada para teste do callback, a cria√ß√£o do modelo BERT para MLM com a fun√ß√£o definida, e a visualiza√ß√£o de seu resumo. Esse passo garante que a arquitetura do modelo esteja conforme o esperado antes de iniciar o treinamento.\n"
   ],
   "id": "3abe55c17cca363"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Converte o exemplo de texto contendo a m√°scara para uma sequ√™ncia de IDs de tokens\n",
    "# utilizando a camada de vetoriza√ß√£o ('vectorize_layer') que foi adaptada aos dados de treino.\n",
    "sample_tokens = vectorize_layer( [ \"Eu tenho [mask] \" ] )\n",
    "\n",
    "# Cria uma inst√¢ncia do callback MaskedTextGenerator, que ser√° usado para gerar exemplos de predi√ß√£o\n",
    "# ao final de cada √©poca de treinamento.\n",
    "# Verifica se 'sample_tokens' √© um tensor do PyTorch. Se for, move-o para a CPU e o converte para um array NumPy\n",
    "# antes de pass√°-lo para o callback, pois o callback espera um array NumPy ou similar.\n",
    "if isinstance( sample_tokens, torch.Tensor ):\n",
    "    generator_callback = MaskedTextGenerator( sample_tokens.cpu().numpy()\n",
    "                                              )  # Movendo para a CPU e convertendo para numpy\n",
    "else:\n",
    "    generator_callback = MaskedTextGenerator( sample_tokens )\n",
    "\n",
    "# Cria uma inst√¢ncia do modelo BERT para a tarefa de Masked Language Modeling (MLM)\n",
    "# chamando a fun√ß√£o 'create_masked_language_bert_model' que define a arquitetura do modelo.\n",
    "bert_masked_model = create_masked_language_bert_model()\n",
    "\n",
    "# Exibe um resumo da arquitetura do modelo BERT criado.\n",
    "# O m√©todo 'summary()' mostra as camadas do modelo, suas formas de sa√≠da e o n√∫mero de par√¢metros trein√°veis e n√£o trein√°veis.\n",
    "# bert_masked_model.summary()"
   ],
   "id": "a457bb9295b21d59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Treinamento do Modelo\n",
    "\n",
    "Esta se√ß√£o configura os par√¢metros de treinamento, incluindo o c√°lculo dos passos por √©poca com base no tamanho dos dataframes de treino e valida√ß√£o. Em seguida, o modelo √© treinado utilizando os datasets previamente criados, e o callback de gera√ß√£o de texto fornece feedback visual ao final de cada √©poca.\n"
   ],
   "id": "ec5c43f3b841fb16"
  },
  {
   "cell_type": "code",
   "id": "f99975dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:43.051075Z",
     "start_time": "2025-04-09T13:25:13.463790Z"
    }
   },
   "source": [
    "# Calcula o n√∫mero de passos (batches) por √©poca para os conjuntos de treino e valida√ß√£o.\n",
    "# O n√∫mero de passos √© determinado dividindo o n√∫mero total de amostras pelo tamanho do lote (BATCH_SIZE).\n",
    "# Usamos len(train_df) para obter o n√∫mero de amostras no DataFrame de treino.\n",
    "train_steps_per_epoch: int = len( train_df ) // config.BATCH_SIZE\n",
    "# Usamos len(test_df) para obter o n√∫mero de amostras no DataFrame de teste/valida√ß√£o.\n",
    "val_steps_per_epoch: int = len( test_df ) // config.BATCH_SIZE\n",
    "\n",
    "# Imprime o n√∫mero calculado de passos por √©poca para treino e valida√ß√£o.\n",
    "print( f\"Passos por √©poca (Treino): {train_steps_per_epoch}\" )\n",
    "print( f\"Passos por √©poca (Valida√ß√£o): {val_steps_per_epoch}\" )\n",
    "\n",
    "# Inicia o treinamento do modelo BERT para Masked Language Modeling (MLM).\n",
    "# O m√©todo 'fit' √© usado para treinar o modelo com os dados de treinamento.\n",
    "history = bert_masked_model.fit(\n",
    "        # Dataset de treinamento para MLM, contendo as entradas mascaradas, r√≥tulos e pesos.\n",
    "        mlm_train_ds,\n",
    "        validation_data = mlm_val_ds,\n",
    "        # Dataset de valida√ß√£o para MLM, usado para avaliar o desempenho do modelo durante o treinamento.\n",
    "        epochs = 5,\n",
    "        # Define o n√∫mero total de √©pocas (passagens completas pelos dados de treinamento) para treinar o modelo.\n",
    "        steps_per_epoch = train_steps_per_epoch,\n",
    "        # Especifica quantos passos de otimiza√ß√£o devem ser realizados em cada √©poca para o conjunto de treinamento.\n",
    "        validation_steps = val_steps_per_epoch,\n",
    "        # Especifica quantos passos de avalia√ß√£o devem ser realizados em cada √©poca para o conjunto de valida√ß√£o.\n",
    "        callbacks = [ generator_callback ]\n",
    "        # Passa uma lista de callbacks para serem executados em diferentes est√°gios do treinamento.\n",
    "        # Aqui, 'generator_callback' ser√° chamado ao final de cada √©poca para gerar exemplos de predi√ß√£o.\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passos por √©poca (Treino): 90\n",
      "Passos por √©poca (Valida√ß√£o): 22\n",
      "Epoch 1/5\n",
      "\u001B[1m 3/90\u001B[0m \u001B[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[1m10:56\u001B[0m 8s/step - loss: 10.3125"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[68], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPassos por √©poca (Valida√ß√£o): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_steps_per_epoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m )\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Treina o modelo usando os datasets separados e validation_data\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mbert_masked_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmlm_train_ds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmlm_val_ds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Passa o dataset de valida√ß√£o\u001B[39;49;00m\n\u001B[0;32m     13\u001B[0m \u001B[43m        \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Ou o n√∫mero de √©pocas desejado\u001B[39;49;00m\n\u001B[0;32m     14\u001B[0m \u001B[43m        \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtrain_steps_per_epoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Passos para treino\u001B[39;49;00m\n\u001B[0;32m     15\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalidation_steps\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mval_steps_per_epoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Passos para valida√ß√£o\u001B[39;49;00m\n\u001B[0;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[43mgenerator_callback\u001B[49m\u001B[43m \u001B[49m\u001B[43m]\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Mant√©m o callback para ver exemplos\u001B[39;49;00m\n\u001B[0;32m     17\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m bert_masked_model\u001B[38;5;241m.\u001B[39msave( \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbert_mlm.keras\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m( \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mTreinamento conclu√≠do e modelo salvo!\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n",
      "File \u001B[1;32mC:\\Codes\\transformer_mlm\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mC:\\Codes\\transformer_mlm\\.venv\\lib\\site-packages\\keras\\src\\backend\\torch\\trainer.py:257\u001B[0m, in \u001B[0;36mTorchTrainer.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001B[0m\n\u001B[0;32m    253\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, data \u001B[38;5;129;01min\u001B[39;00m epoch_iterator:\n\u001B[0;32m    254\u001B[0m     \u001B[38;5;66;03m# Callbacks\u001B[39;00m\n\u001B[0;32m    255\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m--> 257\u001B[0m     logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Callbacks\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_end(step, logs)\n",
      "File \u001B[1;32mC:\\Codes\\transformer_mlm\\.venv\\lib\\site-packages\\keras\\src\\backend\\torch\\trainer.py:117\u001B[0m, in \u001B[0;36mTorchTrainer.make_train_function.<locals>.one_step_on_data\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001B[39;00m\n\u001B[0;32m    116\u001B[0m data \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m--> 117\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Codes\\transformer_mlm\\.venv\\lib\\site-packages\\keras\\src\\backend\\torch\\trainer.py:55\u001B[0m, in \u001B[0;36mTorchTrainer.train_step\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     52\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compute_loss(\n\u001B[0;32m     53\u001B[0m     x\u001B[38;5;241m=\u001B[39mx, y\u001B[38;5;241m=\u001B[39my, y_pred\u001B[38;5;241m=\u001B[39my_pred, sample_weight\u001B[38;5;241m=\u001B[39msample_weight, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     54\u001B[0m )\n\u001B[1;32m---> 55\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_loss_tracker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_state\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflatten\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m     57\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     59\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mscale_loss(loss)\n",
      "File \u001B[1;32mC:\\Codes\\transformer_mlm\\.venv\\lib\\site-packages\\keras\\src\\metrics\\reduction_metrics.py:139\u001B[0m, in \u001B[0;36mMean.update_state\u001B[1;34m(self, values, sample_weight)\u001B[0m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mupdate_state\u001B[39m(\u001B[38;5;28mself\u001B[39m, values, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 139\u001B[0m     values, sample_weight \u001B[38;5;241m=\u001B[39m \u001B[43mreduce_to_samplewise_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    140\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduce_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\n\u001B[0;32m    141\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    142\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal\u001B[38;5;241m.\u001B[39massign_add(ops\u001B[38;5;241m.\u001B[39msum(values))\n\u001B[0;32m    143\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\Codes\\transformer_mlm\\.venv\\lib\\site-packages\\keras\\src\\metrics\\reduction_metrics.py:15\u001B[0m, in \u001B[0;36mreduce_to_samplewise_values\u001B[1;34m(values, sample_weight, reduce_fn, dtype)\u001B[0m\n\u001B[0;32m     13\u001B[0m values \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39mcast(values, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 15\u001B[0m     sample_weight \u001B[38;5;241m=\u001B[39m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_to_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     18\u001B[0m         sample_weight \u001B[38;5;241m=\u001B[39m losses\u001B[38;5;241m.\u001B[39mloss\u001B[38;5;241m.\u001B[39mapply_mask(\n\u001B[0;32m     19\u001B[0m             sample_weight, mask, dtype\u001B[38;5;241m=\u001B[39mdtype, reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msum\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     20\u001B[0m         )\n",
      "File \u001B[1;32mC:\\Codes\\transformer_mlm\\.venv\\lib\\site-packages\\keras\\src\\ops\\core.py:958\u001B[0m, in \u001B[0;36mconvert_to_tensor\u001B[1;34m(x, dtype, sparse, ragged)\u001B[0m\n\u001B[0;32m    956\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m any_symbolic_tensors((x,)):\n\u001B[0;32m    957\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ConvertToTensor(dtype\u001B[38;5;241m=\u001B[39mdtype, sparse\u001B[38;5;241m=\u001B[39msparse)(x)\n\u001B[1;32m--> 958\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbackend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_to_tensor\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    959\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mragged\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mragged\u001B[49m\n\u001B[0;32m    960\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Codes\\transformer_mlm\\.venv\\lib\\site-packages\\keras\\src\\backend\\torch\\core.py:236\u001B[0m, in \u001B[0;36mconvert_to_tensor\u001B[1;34m(x, dtype, sparse, ragged)\u001B[0m\n\u001B[0;32m    232\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m result_type(\n\u001B[0;32m    233\u001B[0m         \u001B[38;5;241m*\u001B[39m[\u001B[38;5;28mgetattr\u001B[39m(item, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mtype\u001B[39m(item)) \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m tree\u001B[38;5;241m.\u001B[39mflatten(x)]\n\u001B[0;32m    234\u001B[0m     )\n\u001B[0;32m    235\u001B[0m dtype \u001B[38;5;241m=\u001B[39m to_torch_dtype(dtype)\n\u001B[1;32m--> 236\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mget_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Salvamento do Modelo e Visualiza√ß√£o dos Resultados\n",
    "\n",
    "Ap√≥s o treinamento, o modelo √© salvo para uso futuro. Nesta etapa, tamb√©m √© gerado um gr√°fico que exibe a evolu√ß√£o da perda (tanto para treino quanto para valida√ß√£o) ao longo das √©pocas, permitindo a avalia√ß√£o visual do desempenho do treinamento.\n"
   ],
   "id": "5757a2a79c773c9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Salva o modelo BERT treinado para a tarefa de Masked Language Modeling (MLM) em um arquivo.\n",
    "# O formato \".keras\" √© o formato padr√£o para salvar modelos no Keras 3.\n",
    "bert_masked_model.save( \"bert_mlm.keras\" )\n",
    "\n",
    "# Imprime uma mensagem indicando que o treinamento foi conclu√≠do e o modelo foi salvo com sucesso.\n",
    "print( \"\\nTreinamento conclu√≠do e modelo salvo!\" )\n"
   ],
   "id": "557818e8d06a52bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:43.062067500Z",
     "start_time": "2025-04-09T11:28:27.949241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importa a biblioteca matplotlib.pyplot para criar gr√°ficos.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plota o hist√≥rico da perda (loss) durante o treinamento.\n",
    "# 'history' √© o objeto retornado pelo m√©todo 'fit' e cont√©m informa√ß√µes sobre o treinamento, incluindo a perda em cada √©poca.\n",
    "# 'history.history['loss']' acessa a lista de valores da perda de treinamento em cada √©poca.\n",
    "plt.plot( history.history[ 'loss' ], label = 'Training Loss' )\n",
    "\n",
    "# Plota o hist√≥rico da perda de valida√ß√£o durante o treinamento.\n",
    "# No Keras, a perda calculada no conjunto de valida√ß√£o durante o treinamento √© geralmente armazenada sob a chave 'val_loss' no dicion√°rio 'history.history'.\n",
    "plt.plot( history.history[ 'val_loss' ], label = 'Validation Loss' )\n",
    "plt.title( 'Training and Validation Loss' )  # Define o t√≠tulo do gr√°fico.\n",
    "plt.xlabel( 'Epoch' )  # Define o r√≥tulo do eixo x como 'Epoch' (√©poca).\n",
    "plt.ylabel( 'Loss' )  # Define o r√≥tulo do eixo y como 'Loss' (perda).\n",
    "plt.legend()  # Exibe a legenda para identificar as linhas do gr√°fico (perda de treinamento e perda de valida√ß√£o).\n",
    "plt.show()  # Exibe o gr√°fico gerado."
   ],
   "id": "80d9bb8dbdfce5b2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:43.064066700Z",
     "start_time": "2025-04-09T11:28:28.100841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retoma o treinamento\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Carrega o modelo salvo\n",
    "if 0:\n",
    "    bert_mlm_model = load_model( \"bert_mlm.keras\" )\n",
    "    bert_mlm_model.fit( mlm_ds.repeat(), epochs = 5, steps_per_epoch = steps, callbacks = [ generator_callback ] )"
   ],
   "id": "54e2be6bf43e279a",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Predi√ß√£o de Frases\n",
    "\n",
    "Utiliza√ß√£o do modelo treinado para realizar a predi√ß√£o de novas frases."
   ],
   "id": "a5c8af1d595f4aa2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:43.065068200Z",
     "start_time": "2025-04-09T11:28:28.137445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Fun√ß√£o para prever o token mascarado e mostrar top K predi√ß√µes\n",
    "def predict_masked_token( text_with_mask: str, model, vectorize_layer, id2token, top_k = 5 ):\n",
    "    \"\"\"\n",
    "    Prev√™ o token mascarado em uma frase e retorna as top K previs√µes com probabilidades.\n",
    "\n",
    "    Args:\n",
    "        text_with_mask (str): A frase de entrada contendo exatamente um token '[mask]'.\n",
    "        model (keras.Model): O modelo treinado (MLM).\n",
    "        vectorize_layer (TextVectorization): A camada de vetoriza√ß√£o J√Å ADAPTADA.\n",
    "        id2token (dict): Dicion√°rio mapeando ID para token.\n",
    "        top_k (int): Quantidade de melhores previs√µes a serem mostradas.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame com as top K previs√µes, tokens e probabilidades.\n",
    "                         Retorna None se '[mask]' n√£o for encontrado ou mais de um for encontrado.\n",
    "    \"\"\"\n",
    "    # 1. Vetorizar a frase de entrada\n",
    "    #    O reshape [1, -1] adiciona a dimens√£o do batch (lote de tamanho 1)\n",
    "    sample_tokens = vectorize_layer( [ text_with_mask ] )  # N√£o precisa de .numpy() aqui para predict\n",
    "\n",
    "    # Verifica se a vetoriza√ß√£o retornou algo e se tem o formato esperado\n",
    "    if sample_tokens is None or sample_tokens.shape[ 0 ] == 0:\n",
    "        print( \"Erro: A vetoriza√ß√£o n√£o produziu uma sa√≠da v√°lida.\" )\n",
    "        return None\n",
    "\n",
    "    # 2. Encontrar o √≠ndice do token [mask]\n",
    "    #    `.numpy()` converte o tensor para numpy array para usar np.where\n",
    "    #    Pega o id do token de m√°scara global que definimos antes\n",
    "    masked_index_tuple = np.where( sample_tokens.cpu().numpy()[ 0 ] == mask_token_id )\n",
    "\n",
    "    # Verifica se encontrou o token de m√°scara e se √© apenas um\n",
    "    if len( masked_index_tuple[ 0 ] ) != 1:\n",
    "        print(\n",
    "                f\"Erro: Esperado exatamente um token '[mask]' na frase vetorizada, mas encontrado(s) {len( masked_index_tuple[ 0 ] )}.\"\n",
    "        )\n",
    "        print( f\"Frase vetorizada: {sample_tokens.numpy()[ 0 ]}\" )\n",
    "        print( f\"ID esperado para [mask]: {mask_token_id}\" )\n",
    "        # Tenta decodificar para ajudar a depurar\n",
    "        try:\n",
    "            decoded_tokens = [ id2token.get( int( t ), \"[UNK]\" ) for t in sample_tokens.numpy()[ 0 ] if t != 0 ]\n",
    "            print( f\"Tokens decodificados: {' '.join( decoded_tokens )}\" )\n",
    "        except Exception as e:\n",
    "            print( f\"N√£o foi poss√≠vel decodificar os tokens: {e}\" )\n",
    "        return None\n",
    "\n",
    "    masked_index = masked_index_tuple[ 0 ][ 0 ]  # Pega o √≠ndice escalar\n",
    "\n",
    "    # 3. Fazer a predi√ß√£o com o modelo\n",
    "    prediction = model.predict( sample_tokens )\n",
    "\n",
    "    # 4. Extrair as probabilidades para a posi√ß√£o mascarada\n",
    "    #    prediction[0] -> batch de tamanho 1\n",
    "    #    prediction[0][masked_index] -> probabilidades para o token na posi√ß√£o mascarada\n",
    "    mask_prediction_probabilities = prediction[ 0 ][ masked_index ]\n",
    "\n",
    "    # 5. Encontrar os top K √≠ndices e suas probabilidades\n",
    "    #    argsort() retorna os √≠ndices que ordenariam o array (do menor para o maior)\n",
    "    #    [-top_k:] pega os K maiores √≠ndices\n",
    "    #    [::-1] inverte para ter do maior para o menor\n",
    "    top_indices = mask_prediction_probabilities.argsort()[ -top_k: ][ ::-1 ]\n",
    "    top_probabilities = mask_prediction_probabilities[ top_indices ]\n",
    "\n",
    "    # 6. Converter os IDs dos tokens previstos para palavras\n",
    "    top_tokens = [ id2token.get( idx, \"[UNK]\" ) for idx in top_indices ]\n",
    "\n",
    "    # 7. Criar e retornar um DataFrame com os resultados\n",
    "    results_df = pd.DataFrame( {\n",
    "        \"Token Previsto\": top_tokens,\n",
    "        \"Probabilidade\": top_probabilities\n",
    "    }\n",
    "    )\n",
    "\n",
    "    print( f\"\\nPredi√ß√µes para: '{text_with_mask}'\" )\n",
    "    return results_df"
   ],
   "id": "b08a6f73cd999b58",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:25:43.071067300Z",
     "start_time": "2025-04-09T11:30:53.245377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "frase_teste_1 = \"Pra sempre com [mask]\"\n",
    "resultados_df_1 = predict_masked_token( frase_teste_1, bert_masked_model, vectorize_layer, id2token, top_k = 10 )\n",
    "\n",
    "if resultados_df_1 is not None:\n",
    "    print( resultados_df_1.to_string( index = False ) )  # .to_string para melhor formata√ß√£o no print"
   ],
   "id": "c63d4fb3a3660983",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step\n",
      "\n",
      "Predi√ß√µes para: 'Pra sempre com [mask]'\n",
      "Token Previsto  Probabilidade\n",
      "           n√£o       0.010693\n",
      "            de       0.010530\n",
      "            eu       0.010237\n",
      "             o       0.009339\n",
      "             e       0.008349\n",
      "          voc√™       0.007479\n",
      "            me       0.007308\n",
      "             a       0.007162\n",
      "           pra       0.007002\n",
      "           meu       0.006103\n"
     ]
    }
   ],
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
