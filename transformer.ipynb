{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformer\n",
    "\n",
    "- Encoder: O Encoder recebe a sequência de entrada (por exemplo, uma frase em português) e a processa para criar uma representação vetorial de alta qualidade dessa sequência. Essa representação captura o significado e o contexto de cada palavra em relação às outras palavras da frase.\n",
    "- Decoder: O Decoder recebe a representação gerada pelo Encoder e a utiliza para gerar uma sequência de saída (por exemplo, a tradução da frase para o inglês).\n"
   ],
   "id": "e98cd1fa27373e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Masked Language Model\n",
    "\n",
    "Um Masked Language Model (MLM) é um tipo de modelo de linguagem amplamente utilizado em processamento de linguagem natural.\n",
    "\n",
    "Durante o treinamento, uma parte dos tokens (palavras ou subpalavras) no texto de entrada é substituída por um token especial de máscara, como \"[MASK]\". O objetivo do modelo é prever corretamente quais eram os tokens originais que foram mascarados.\n",
    "\n",
    "Essa estratégia obriga o modelo a aprender contextos ricos e relações entre as palavras, o que é fundamental para o desempenho em diversas tarefas, como análise de sentimentos, tradução, e resposta a perguntas. Modelos famosos que utilizam essa técnica incluem o BERT, que demonstrou ganhos significativos em várias benchmarks de NLP .\n"
   ],
   "id": "beae2a5e32306d38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## IMBD\n",
    "\n",
    "Para este projeto, será utilizado a base do IMBD."
   ],
   "id": "a8259a8ef1b3febb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuração",
   "id": "c4bed58b15c8e73d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:06:00.565242Z",
     "start_time": "2025-04-04T13:05:50.930351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ[ \"KERAS_BACKEND\" ] = \"torch\"  # or jax, or tensorflow\n",
    "\n",
    "import keras_hub\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "from pprint import pprint"
   ],
   "id": "a89620b2e2393cb4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/viniciuscandeia/development/transformer_mlm/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:06:00.588087Z",
     "start_time": "2025-04-04T13:06:00.584468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 256\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 0.001\n",
    "    VOCAB_SIZE = 30000\n",
    "    EMBED_DIM = 128\n",
    "    NUM_HEAD = 8  # used in bert model\n",
    "    FF_DIM = 128  # used in bert model\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "\n",
    "config = Config()"
   ],
   "id": "6cce974cf0c26a19",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Carregando os dados\n",
    "\n",
    "Primeiro, vamos carregar os dados que estão na pasta \"aclImbd\".\n",
    "\n",
    "Duas funções serão utilizadas para isso:\n",
    "\n",
    "- Uma irá criar uma lista contendo o conteúdo dos arquivos.\n",
    "- A outra ficará responsável por criar um dataframe."
   ],
   "id": "39422ab42f331838"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:06:09.379672Z",
     "start_time": "2025-04-04T13:06:00.601300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_text_list_from_files( files ) -> list[ str ]:\n",
    "    \"\"\"\n",
    "       Esta função irá retornar uma lista contendo todas as frases dos arquivos.\n",
    "    \"\"\"\n",
    "    text_list: list[ str ] = [ ]\n",
    "    for name in files:\n",
    "        with open( name ) as f:\n",
    "            for line in f:\n",
    "                text_list.append( line )\n",
    "    return text_list\n",
    "\n",
    "\n",
    "def get_data_from_text_files( folder_name ):\n",
    "    # Arquivos de texto com avaliações positivas e negativas\n",
    "    pos_files = glob.glob( f\"aclImdb/{folder_name}/pos/*.txt\" )\n",
    "    neg_files = glob.glob( f\"aclImdb/{folder_name}/neg/*.txt\" )\n",
    "\n",
    "    # Listas com as avaliações\n",
    "    pos_texts: list[ str ] = get_text_list_from_files( pos_files )\n",
    "    neg_texts: list[ str ] = get_text_list_from_files( neg_files )\n",
    "\n",
    "    # Criação de um dataframe, com colunas chamadas \"review\" e \"sentiment\"\n",
    "    df = pd.DataFrame(\n",
    "            {\n",
    "                # Concatenação das listas\n",
    "                \"review\": pos_texts + neg_texts,\n",
    "                # Criação de uma nova lista\n",
    "                \"sentiment\": [ 0 ] * len( pos_texts ) + [ 1 ] * len( neg_texts ),\n",
    "            }\n",
    "    )\n",
    "\n",
    "    # Sample -> pega uma amostra aleatória\n",
    "    # len(df) -> do tamanho do df original\n",
    "    # reset_index -> ao usar sample, o índice original das linhas é mantido\n",
    "    df = df.sample( len( df ) ).reset_index( drop = True )\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = get_data_from_text_files( \"train\" )\n",
    "test_df = get_data_from_text_files( \"test\" )\n",
    "\n",
    "# Concatenação dos dataframes para realizar pré-processamento em toda a base\n",
    "all_data = pd.concat( [ train_df, test_df ], ignore_index = True )"
   ],
   "id": "822b332050609de1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:06:09.520760Z",
     "start_time": "2025-04-04T13:06:09.511871Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.head()",
   "id": "aa3089bbffe5c2b6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              review  sentiment\n",
       "0  This is the first of \"The Complete Dramatic Wo...          1\n",
       "1  WOW. One of the greatest movies I have EVER - ...          0\n",
       "2  Me and a friend rented this movie because it s...          1\n",
       "3  This movie is the first time movie experience ...          0\n",
       "4  \"Ice Age\" is one of the cartoon movies ever pr...          0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is the first of \"The Complete Dramatic Wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WOW. One of the greatest movies I have EVER - ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Me and a friend rented this movie because it s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This movie is the first time movie experience ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Ice Age\" is one of the cartoon movies ever pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:06:09.689230Z",
     "start_time": "2025-04-04T13:06:09.684191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def custom_standardization( input_data ):\n",
    "    \"\"\"\n",
    "        Normalização de texto.\n",
    "    \"\"\"\n",
    "    # Converter todas as letras para minúsculas\n",
    "    lowercase = tf.strings.lower( input_data )\n",
    "\n",
    "    # Expressão Regular para remover a tag HTML\n",
    "    stripped_html = tf.strings.regex_replace( lowercase, \"<br />\", \" \" )\n",
    "\n",
    "    # Remover caracteres especiais\n",
    "    return tf.strings.regex_replace(\n",
    "            stripped_html,\n",
    "            \"[%s]\" % re.escape( \"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\" ),\n",
    "            \"\"\n",
    "    )"
   ],
   "id": "e617962188106c4a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vetorização de Texto\n",
    "\n",
    "Para um transformer, a vetorização de um texto é o processo fundamental de transformar o texto bruto em uma representação numérica que o modelo possa entender e processar. Em essência, é como traduzir a linguagem humana para a linguagem matemática que o transformer consegue trabalhar.\n",
    "\n",
    "Imagine que o transformer é um computador que só entende números. Para que ele consiga ler e compreender um texto, precisamos converter cada palavra (ou parte da palavra) em um conjunto de números. Esse conjunto de números é o que chamamos de vetor.\n",
    "\n",
    "Aqui está um detalhamento do processo de vetorização para um transformer:\n",
    "\n",
    "1. Tokenização: O primeiro passo é dividir o texto em unidades menores, chamadas tokens. Um token pode ser uma palavra inteira, parte de uma palavra (subpalavra), ou até mesmo um caractere. Por exemplo, a frase \"O gato comeu o rato\" poderia ser tokenizada como: [\"O\", \"gato\", \"comeu\", \"o\", \"rato\"].\n",
    "\n",
    "2. Criação do Vocabulário: Em seguida, é criado um vocabulário, que é uma lista de todos os tokens únicos presentes no conjunto de dados de treinamento do modelo. Cada token nesse vocabulário recebe um índice único.\n",
    "\n",
    "3. Indexação: Cada token no texto de entrada é então mapeado para o seu índice correspondente no vocabulário. Usando o exemplo anterior e supondo um vocabulário, os tokens poderiam ser convertidos em índices como: [10, 25, 50, 10, 75].\n",
    "\n",
    "4. Embedding: A etapa crucial para transformers é a criação de embeddings. Em vez de simplesmente usar os índices brutos, cada índice é transformado em um vetor denso de números reais. Esse vetor captura o significado semântico e as relações entre as palavras.\n",
    "\n",
    "    - Word Embeddings: Cada palavra (ou token) é associada a um vetor de baixa dimensionalidade (por exemplo, 512 ou 768 dimensões). Palavras com significados semelhantes tendem a ter vetores próximos no espaço vetorial. Por exemplo, os vetores para \"gato\" e \"felino\" provavelmente estarão mais próximos do que os vetores para \"gato\" e \"carro\".\n",
    "\n",
    "    - Positional Embeddings: Transformers também precisam entender a ordem das palavras em uma frase. Para isso, são adicionados embeddings posicionais aos word embeddings. Esses vetores codificam a posição de cada token na sequência, permitindo que o modelo diferencie entre \"o gato comeu o rato\" e \"o rato comeu o gato\".\n",
    "\n",
    "5. Input para o Transformer: Os vetores resultantes (a soma dos word embeddings e positional embeddings para cada token) são então alimentados como entrada para as diferentes camadas do transformer (como as camadas de atenção)."
   ],
   "id": "1413c85f5980aae3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:06:09.912059Z",
     "start_time": "2025-04-04T13:06:09.906565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_vectorize_layer( texts: list[ str ], vocab_size: int, max_seq: int, special_tokens: list = [ \"[MASK]\" ] ):\n",
    "    \"\"\"Build Text vectorization layer\n",
    "\n",
    "    Args:\n",
    "      texts (list): List of string i.e input texts\n",
    "      vocab_size (int): vocab size\n",
    "      max_seq (int): Maximum sequence length.\n",
    "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
    "\n",
    "    Returns:\n",
    "        layers.Layer: Return TextVectorization Keras Layer\n",
    "    \"\"\"\n",
    "\n",
    "    # Criação da camada de TextVectorization\n",
    "    vectorize_layer = TextVectorization(\n",
    "            max_tokens = vocab_size,  # Define o tamanho máximo do vocabulário\n",
    "            output_mode = \"int\",  # Define que a saída deve ser uma sequência de números inteiros\n",
    "            standardize = custom_standardization,  # Aplicar função de pré-processamento\n",
    "            output_sequence_length = max_seq,  # Garantir que toda sequência de saída tenha comprimento max_seq\n",
    "    )\n",
    "\n",
    "    # todo Mostrar um exemplo\n",
    "\n",
    "    # Adaptação aos textos de entrada\n",
    "    # A camada \"sabe\" como mapear palavras para números inteiros, com base nas entradas\n",
    "    vectorize_layer.adapt( texts )\n",
    "\n",
    "    # Obtém o vocabulário aprendido\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "    # Por padrão, o TextVectorization coloca \"\" (para padding, índice 0) e \"[UNK]\" (para palavras desconhecidas,\n",
    "    # índice 1) no início do vocabulário.\n",
    "\n",
    "    # Pegando uma porção do vocabulário:\n",
    "    # - Ignorando \"\" e \"[UNK]\"\n",
    "    # - Pega quase tudo, deixando espaço suficiente para special_tokens\n",
    "    vocab = vocab[ 2: vocab_size - len( special_tokens ) ] + special_tokens\n",
    "\n",
    "    # Atualiza o vocabulário da camada\n",
    "    vectorize_layer.set_vocabulary( vocab )\n",
    "\n",
    "    # Retorna a camada\n",
    "    return vectorize_layer"
   ],
   "id": "7d8904fb48419e91",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:06:13.781603Z",
     "start_time": "2025-04-04T13:06:09.961501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pegando a camada de vetorização\n",
    "vectorize_layer = get_vectorize_layer(\n",
    "        all_data.review.values.tolist(),\n",
    "        config.VOCAB_SIZE,\n",
    "        config.MAX_LEN,\n",
    "        special_tokens = [ \"[mask]\" ],\n",
    ")\n",
    "\n",
    "# Processamento de um novo dado: \"[mask]\"\n",
    "# - Aplica a função de pré-processamento\n",
    "# - Divide em token, pega seu id e cria uma sequência de comprimento config.MAX_LEN\n",
    "# - Converte o resultado para um array NumPy\n",
    "# - Pega o id do \"[mask]\"\n",
    "mask_token_id = vectorize_layer( [ \"[mask]\" ] ).cpu().numpy()[ 0 ][ 0 ]"
   ],
   "id": "14af57ee8c312389",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:07:30.079010Z",
     "start_time": "2025-04-04T13:07:30.068819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode( texts ):\n",
    "    \"\"\" Retorna um array NumPy das sequências numéricas dos textos de entrada.\"\"\"\n",
    "    # Criação das sequências numéricas para os textos de entrada\n",
    "    encoded_texts = vectorize_layer( texts )\n",
    "    # Retorna as sequências como um array NumPy\n",
    "    return encoded_texts.cpu().numpy()"
   ],
   "id": "cf38423e529f2726",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Contexto para o código: Mascaramento\n",
    "\n",
    "Em essência, o mascaramento envolve ocultar aleatoriamente algumas das palavras (ou tokens) em uma sequência de texto de entrada. O objetivo é fazer com que o modelo aprenda a prever as palavras que foram mascaradas, com base no contexto das palavras vizinhas não mascaradas.\n",
    "\n",
    "Imagine a frase: \"O gato está dormindo no tapete.\"\n",
    "\n",
    "No processo de mascaramento, poderíamos aleatoriamente escolher algumas palavras para ocultar, substituindo-as por um token especial, geralmente chamado [MASK]. Por exemplo, a frase poderia se tornar:\n",
    "\n",
    "\"O [MASK] está dormindo no [MASK].\"\n",
    "\n",
    "O modelo de linguagem, durante o treinamento, receberia essa versão mascarada da frase como entrada e teria como objetivo prever as palavras originais que foram substituídas por [MASK]. Neste caso, o modelo deveria aprender a prever \"gato\" para o primeiro [MASK] e \"tapete\" para o segundo."
   ],
   "id": "7f5c334136918c96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:06:59.098427Z",
     "start_time": "2025-04-04T13:06:59.088752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_masked_input_and_labels( encoded_texts ):\n",
    "    # Cria um array NumPy com o mesmo tamanho de encoded_texts preenchido com números aleatórios entre 0 e 1\n",
    "    # Vai comparar cada um dos números com 0.15, se for maior ou igual, será True, do contrário, será False\n",
    "    # inp_mask será um array de booleanos de mesmo tamanho que encoded_texts\n",
    "    inp_mask = np.random.rand( *encoded_texts.shape ) < 0.15\n",
    "\n",
    "    # Não deixa realizar o mascaramento em tokens especiais\n",
    "    inp_mask[ encoded_texts <= 2 ] = False\n",
    "\n",
    "    # Cria um array com o mesmo tamanho de encoded_texts preenchido com o valor -1\n",
    "    # O valor -1 é usado para indicar que esses tokens não são alvos para a previsão durante o treinamento\n",
    "    labels = -1 * np.ones( encoded_texts.shape, dtype = int )\n",
    "\n",
    "    # Para as posições no array qye são True em inp_mask, os valores dos IDs dos tokens são atribuídos a labels\n",
    "    # Assim, labels terá os IDs dos tokens que foram mascarados, e o modelo terá que prever esses IDs\n",
    "    labels[ inp_mask ] = encoded_texts[ inp_mask ]\n",
    "\n",
    "    # Cria uma cópia de encoded_texts\n",
    "    encoded_texts_masked = np.copy( encoded_texts )\n",
    "\n",
    "    # Cria uma nova máscara booleana\n",
    "    # Ela é True apenas nas posições onde inp_mask também é True E um novo número aleatório gerado para\n",
    "    # essa posição é menor que 0.9\n",
    "    # Apenas 90% dos 15% dos tokens selecionados para mascaramente, serão, de fato, mascarados\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand( *encoded_texts.shape ) < 0.90)\n",
    "\n",
    "    # Atualizando posições onde inp_mask_2mask são True para a máscara\n",
    "    encoded_texts_masked[ inp_mask_2mask ] = mask_token_id\n",
    "\n",
    "    # Cria uma nova máscara booleana\n",
    "    # Ela é True apenas nas posições onde inp_mask_2mask também é True E um novo número aleatório gerado para\n",
    "    # essa posição é menor que 1/9\n",
    "    # 1/9 dos 90% serão tokens aleatórios\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand( *encoded_texts.shape ) < 1 / 9)\n",
    "\n",
    "    # Nas posições onde inp_mask_2random é True, o token em encoded_texts_masked será um token aleatório\n",
    "    # Gera um array de números aleatórios partindo de 3 e indo até antes de mask_token_id\n",
    "    # Começou em 3 porque as primeiras posições foram excluídas\n",
    "    encoded_texts_masked[ inp_mask_2random ] = np.random.randint(\n",
    "            3, mask_token_id, inp_mask_2random.sum()\n",
    "    )\n",
    "\n",
    "    # Cria um array de mesmo tamanho que labels preenchido com 1\n",
    "    sample_weights = np.ones( labels.shape )\n",
    "\n",
    "    # Nas posições dos tokens que têm -1, o valor em sample_weights será 0\n",
    "    # Isso significa que a perda durante o treinamento será calculada apenas para os tokens\n",
    "    # que foram realmente mascarados\n",
    "    sample_weights[ labels == -1 ] = 0\n",
    "\n",
    "    # Cria uma cópia de encoded_texts\n",
    "    y_labels = np.copy( encoded_texts )\n",
    "\n",
    "    # A versão da entrada com alguns tokens substituídos por [MASK] ou por tokens aleatórios\n",
    "    # Os tokens originais da entrada, que servem como os rótulos para o treinamento\n",
    "    # Um array de pesos que indica quais posições em y_labels devem ser consideradas no cálculo da perda\n",
    "    return encoded_texts_masked, y_labels, sample_weights"
   ],
   "id": "977366761074a52b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:07:39.798057Z",
     "start_time": "2025-04-04T13:07:34.072827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We have 25000 examples for training\n",
    "x_train = encode( train_df.review.values )  # encode reviews with vectorizer\n",
    "y_train = train_df.sentiment.values\n",
    "train_classifier_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices( (x_train, y_train) )\n",
    "    .shuffle( 1000 )\n",
    "    .batch( config.BATCH_SIZE )\n",
    ")\n",
    "\n",
    "# We have 25000 examples for testing\n",
    "x_test = encode( test_df.review.values )\n",
    "y_test = test_df.sentiment.values\n",
    "test_classifier_ds = tf.data.Dataset.from_tensor_slices( (x_test, y_test) ).batch(\n",
    "        config.BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Dataset for end to end model input (will be used at the end)\n",
    "test_raw_classifier_ds = test_df\n",
    "\n",
    "# Prepare data for masked language model\n",
    "x_all_review = encode( all_data.review.values )\n",
    "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
    "        x_all_review\n",
    ")\n",
    "\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        (x_masked_train, y_masked_labels, sample_weights)\n",
    ")\n",
    "mlm_ds = mlm_ds.shuffle( 1000 ).batch( config.BATCH_SIZE )"
   ],
   "id": "55a74b6efbf9b0d2",
   "outputs": [],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
