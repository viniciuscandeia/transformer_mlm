{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Projeto\n",
    "\n",
    "Este projeto desenvolverá um modelo transformer BERT e o treinará utilizando a tarefa de Masked Language Model (MLM), aplicado ao contexto das músicas brasileiras de forró.\n",
    "\n",
    "Para isso, foi utilizado o dataset de:"
   ],
   "id": "e86385d09c2aa532"
  },
  {
   "cell_type": "markdown",
   "id": "e98cd1fa27373e9",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "- Encoder: O Encoder recebe a sequência de entrada (por exemplo, uma frase em português) e a processa para criar uma representação vetorial de alta qualidade dessa sequência. Essa representação captura o significado e o contexto de cada palavra em relação às outras palavras da frase.\n",
    "- Decoder: O Decoder recebe a representação gerada pelo Encoder e a utiliza para gerar uma sequência de saída (por exemplo, a tradução da frase para o inglês).\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Modelo BERT\n",
    "\n",
    "Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "1. Bloco transformer com componentes/operacoes Multi-Head Self-Attention, Feed Foward Network e dropout.\n",
    "\n",
    "2. Customização do modelo, definindo como calcular as metricas que serao utilizadas, neste caso calculando a perda com Sparse Categorical Crossentropy e atualizando com a média da perda.\n",
    "\n",
    "3. Construção do modelo BERT para o mlm.\n",
    "\n",
    "4. Callback para a geração de texto para tokens mascarados e apresentação de resultados."
   ],
   "id": "890b7c9e1b5814a3"
  },
  {
   "cell_type": "markdown",
   "id": "beae2a5e32306d38",
   "metadata": {},
   "source": [
    "# Masked Language Model\n",
    "\n",
    "Um Masked Language Model (MLM) é um tipo de modelo de linguagem amplamente utilizado em processamento de linguagem natural.\n",
    "\n",
    "Durante o treinamento, uma parte dos tokens (palavras ou subpalavras) no texto de entrada é substituída por um token especial de máscara, como \"[MASK]\". O objetivo do modelo é prever corretamente quais eram os tokens originais que foram mascarados.\n",
    "\n",
    "Essa estratégia obriga o modelo a aprender contextos ricos e relações entre as palavras, o que é fundamental para o desempenho em diversas tarefas, como análise de sentimentos, tradução, e resposta a perguntas. Modelos famosos que utilizam essa técnica incluem o BERT, que demonstrou ganhos significativos em várias benchmarks de NLP .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bed58b15c8e73d",
   "metadata": {},
   "source": [
    "## Configuração"
   ]
  },
  {
   "cell_type": "code",
   "id": "a89620b2e2393cb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:05:53.464640Z",
     "start_time": "2025-04-08T20:05:53.458643Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "os.environ[ \"KERAS_BACKEND\" ] = \"torch\"  # or jax, or tensorflow\n",
    "\n",
    "import keras_hub\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "from pprint import pprint"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:05:53.542639Z",
     "start_time": "2025-04-08T20:05:53.528649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\" )\n",
    "print( f\"Using device: {device}\" )"
   ],
   "id": "a5efb95b0d0e627",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "6cce974cf0c26a19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:05:53.619824Z",
     "start_time": "2025-04-08T20:05:53.610819Z"
    }
   },
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 256\n",
    "    BATCH_SIZE = 16  #32\n",
    "    LR = 0.0001  #0.001\n",
    "    VOCAB_SIZE = 30000\n",
    "    EMBED_DIM = 128\n",
    "    NUM_HEAD = 8  # used in bert model\n",
    "    FF_DIM = 128  # used in bert model\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "\n",
    "config = Config()"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "39422ab42f331838",
   "metadata": {},
   "source": [
    "# Carregando os dados\n",
    "\n",
    "Primeiro, vamos carregar os dados que estão na pasta \"aclImbd\".\n",
    "\n",
    "Duas funções serão utilizadas para isso:\n",
    "\n",
    "- Uma irá criar uma lista contendo o conteúdo dos arquivos.\n",
    "- A outra ficará responsável por criar um dataframe."
   ]
  },
  {
   "cell_type": "code",
   "id": "822b332050609de1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:05:59.467977Z",
     "start_time": "2025-04-08T20:05:53.652770Z"
    }
   },
   "source": [
    "def get_text_list_from_files( files ) -> list[ str ]:\n",
    "    \"\"\"\n",
    "       Esta função irá retornar uma lista contendo todas as frases dos arquivos.\n",
    "    \"\"\"\n",
    "    text_list: list[ str ] = [ ]\n",
    "    for name in files:\n",
    "        with open( name, \"r\", encoding = \"utf-8\" ) as f:\n",
    "            # Lê o conteúdo completo do arquivo; se houver várias linhas e desejar cada linha\n",
    "            # como exemplo separado, use um loop for sobre f.\n",
    "            text_list.append( f.read() )\n",
    "    return text_list\n",
    "\n",
    "\n",
    "def get_data_from_text_files( folder_name ):\n",
    "    # Arquivos de texto que serão utilizados\n",
    "    files = glob.glob( f\"musicas/{folder_name}/*.txt\" )\n",
    "\n",
    "    # Listas com os textos\n",
    "    texts: list[ str ] = get_text_list_from_files( files )\n",
    "\n",
    "    # Criação de um dataframe, com coluna chamada \"lyric\"\n",
    "    df = pd.DataFrame( { \"lyric\": texts } )\n",
    "\n",
    "    # Sample -> pega uma amostra aleatória\n",
    "    # len(df) -> do tamanho do df original\n",
    "    # reset_index -> ao usar sample, o índice original das linhas é mantido\n",
    "    df = df.sample( len( df ) ).reset_index( drop = True )\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = get_data_from_text_files( \"train\" )\n",
    "test_df = get_data_from_text_files( \"test\" )\n",
    "\n",
    "# Novidade\n",
    "print( f\"Tamanho do DataFrame de Treino: {len( train_df )}\" )\n",
    "print( f\"Tamanho do DataFrame de Teste/Validação: {len( test_df )}\" )\n",
    "\n",
    "# Concatenação dos dataframes para realizar pré-processamento em toda a base\n",
    "all_data = pd.concat( [ train_df, test_df ], ignore_index = True )\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do DataFrame de Treino: 1444\n",
      "Tamanho do DataFrame de Teste/Validação: 361\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "aa3089bbffe5c2b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:05:59.515951Z",
     "start_time": "2025-04-08T20:05:59.501984Z"
    }
   },
   "source": [
    "train_df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               lyric\n",
       "0  Quero te encontrar tô louca pra te ver Sentir ...\n",
       "1  Eu sei você sofreu demais E sabe que eu não fu...\n",
       "2  É alta madrugada não consigo suportar E essa d...\n",
       "3  Me ligou de madrugada Disse que tá com saudade...\n",
       "4  Diz um velho ditado Quem Espera sempre alcança..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quero te encontrar tô louca pra te ver Sentir ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eu sei você sofreu demais E sabe que eu não fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>É alta madrugada não consigo suportar E essa d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me ligou de madrugada Disse que tá com saudade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diz um velho ditado Quem Espera sempre alcança...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "e617962188106c4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:05:59.625954Z",
     "start_time": "2025-04-08T20:05:59.610949Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def custom_standardization( input_data ):\n",
    "    \"\"\"Normalização de texto. \"\"\"\n",
    "    # Converter todas as letras para minúsculas\n",
    "    lowercase = tf.strings.lower( input_data )\n",
    "\n",
    "    # Expressão Regular para remover a tag HTML\n",
    "    stripped_html = tf.strings.regex_replace( lowercase, \"<br />\", \" \" )\n",
    "\n",
    "    # Remover caracteres especiais\n",
    "    return tf.strings.regex_replace(\n",
    "            stripped_html,\n",
    "            \"[%s]\" % re.escape( \"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\" ),\n",
    "            \"\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "id": "1413c85f5980aae3",
   "metadata": {},
   "source": [
    "## Vetorização de Texto\n",
    "\n",
    "Para um transformer, a vetorização de um texto é o processo fundamental de transformar o texto bruto em uma representação numérica que o modelo possa entender e processar. Em essência, é como traduzir a linguagem humana para a linguagem matemática que o transformer consegue trabalhar.\n",
    "\n",
    "Imagine que o transformer é um computador que só entende números. Para que ele consiga ler e compreender um texto, precisamos converter cada palavra (ou parte da palavra) em um conjunto de números. Esse conjunto de números é o que chamamos de vetor.\n",
    "\n",
    "Aqui está um detalhamento do processo de vetorização para um transformer:\n",
    "\n",
    "1. Tokenização: O primeiro passo é dividir o texto em unidades menores, chamadas tokens. Um token pode ser uma palavra inteira, parte de uma palavra (subpalavra), ou até mesmo um caractere. Por exemplo, a frase \"O gato comeu o rato\" poderia ser tokenizada como: [\"O\", \"gato\", \"comeu\", \"o\", \"rato\"].\n",
    "\n",
    "2. Criação do Vocabulário: Em seguida, é criado um vocabulário, que é uma lista de todos os tokens únicos presentes no conjunto de dados de treinamento do modelo. Cada token nesse vocabulário recebe um índice único.\n",
    "\n",
    "3. Indexação: Cada token no texto de entrada é então mapeado para o seu índice correspondente no vocabulário. Usando o exemplo anterior e supondo um vocabulário, os tokens poderiam ser convertidos em índices como: [10, 25, 50, 10, 75].\n",
    "\n",
    "4. Embedding: A etapa crucial para transformers é a criação de embeddings. Em vez de simplesmente usar os índices brutos, cada índice é transformado em um vetor denso de números reais. Esse vetor captura o significado semântico e as relações entre as palavras.\n",
    "\n",
    "    - Word Embeddings: Cada palavra (ou token) é associada a um vetor de baixa dimensionalidade (por exemplo, 512 ou 768 dimensões). Palavras com significados semelhantes tendem a ter vetores próximos no espaço vetorial. Por exemplo, os vetores para \"gato\" e \"felino\" provavelmente estarão mais próximos do que os vetores para \"gato\" e \"carro\".\n",
    "\n",
    "    - Positional Embeddings: Transformers também precisam entender a ordem das palavras em uma frase. Para isso, são adicionados embeddings posicionais aos word embeddings. Esses vetores codificam a posição de cada token na sequência, permitindo que o modelo diferencie entre \"o gato comeu o rato\" e \"o rato comeu o gato\".\n",
    "\n",
    "5. Input para o Transformer: Os vetores resultantes (a soma dos word embeddings e positional embeddings para cada token) são então alimentados como entrada para as diferentes camadas do transformer (como as camadas de atenção)."
   ]
  },
  {
   "cell_type": "code",
   "id": "7d8904fb48419e91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:05:59.689949Z",
     "start_time": "2025-04-08T20:05:59.676949Z"
    }
   },
   "source": [
    "def get_vectorize_layer( texts: list[ str ], vocab_size: int, max_seq: int, special_tokens: list = [ \"[MASK]\" ] ):\n",
    "    \"\"\"Build Text vectorization layer\n",
    "\n",
    "    Args:\n",
    "      texts (list): List of string i.e input texts\n",
    "      vocab_size (int): vocab size\n",
    "      max_seq (int): Maximum sequence length.\n",
    "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
    "\n",
    "    Returns:\n",
    "        layers.Layer: Return TextVectorization Keras Layer\n",
    "    \"\"\"\n",
    "\n",
    "    # Criação da camada de TextVectorization\n",
    "    vectorize_layer = TextVectorization(\n",
    "            max_tokens = vocab_size,  # Define o tamanho máximo do vocabulário\n",
    "            output_mode = \"int\",  # Define que a saída deve ser uma sequência de números inteiros\n",
    "            standardize = custom_standardization,  # Aplicar função de pré-processamento\n",
    "            output_sequence_length = max_seq,  # Garantir que toda sequência de saída tenha comprimento max_seq\n",
    "    )\n",
    "\n",
    "    # todo Mostrar um exemplo\n",
    "\n",
    "    # Adaptação aos textos de entrada\n",
    "    # A camada \"sabe\" como mapear palavras para números inteiros, com base nas entradas\n",
    "    vectorize_layer.adapt( texts )\n",
    "\n",
    "    # Obtém o vocabulário aprendido\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "    # Por padrão, o TextVectorization coloca \"\" (para padding, índice 0) e \"[UNK]\" (para palavras desconhecidas,\n",
    "    # índice 1) no início do vocabulário.\n",
    "\n",
    "    # Pegando uma porção do vocabulário:\n",
    "    # - Ignorando \"\" e \"[UNK]\"\n",
    "    # - Pega quase tudo, deixando espaço suficiente para special_tokens\n",
    "    vocab = vocab[ 2: vocab_size - len( special_tokens ) ] + special_tokens\n",
    "\n",
    "    # Atualiza o vocabulário da camada\n",
    "    vectorize_layer.set_vocabulary( vocab )\n",
    "\n",
    "    # Retorna a camada\n",
    "    return vectorize_layer"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "14af57ee8c312389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:06:00.096949Z",
     "start_time": "2025-04-08T20:05:59.943950Z"
    }
   },
   "source": [
    "# Novidade\n",
    "# Pega os textos APENAS do treino para adaptar o vetorizador\n",
    "train_texts = train_df.lyric.values.tolist()\n",
    "\n",
    "# Pegando a camada de vetorização\n",
    "vectorize_layer = get_vectorize_layer(\n",
    "        train_texts,\n",
    "        config.VOCAB_SIZE,\n",
    "        config.MAX_LEN,\n",
    "        special_tokens = [ \"[mask]\" ],\n",
    ")\n",
    "\n",
    "# Processamento de um novo dado: \"[mask]\"\n",
    "# - Aplica a função de pré-processamento\n",
    "# - Divide em token, pega seu id e cria uma sequência de comprimento config.MAX_LEN\n",
    "# - Converte o resultado para um array NumPy\n",
    "# - Pega o id do \"[mask]\"\n",
    "mask_token_id = vectorize_layer( [ \"[mask]\" ] ).cpu().numpy()[ 0 ][ 0 ]"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "cf38423e529f2726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:06:00.128949Z",
     "start_time": "2025-04-08T20:06:00.111952Z"
    }
   },
   "source": [
    "def encode( texts ):\n",
    "    \"\"\" Retorna um array NumPy das sequências numéricas dos textos de entrada.\"\"\"\n",
    "    # Criação das sequências numéricas para os textos de entrada\n",
    "    encoded_texts = vectorize_layer( texts )\n",
    "    # Retorna as sequências como um array NumPy\n",
    "    return encoded_texts.cpu().numpy()"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "id": "7f5c334136918c96",
   "metadata": {},
   "source": [
    "### Mascaramento de Texto\n",
    "\n",
    "Em essência, o mascaramento envolve ocultar aleatoriamente algumas das palavras (ou tokens) em uma sequência de texto de entrada. O objetivo é fazer com que o modelo aprenda a prever as palavras que foram mascaradas, com base no contexto das palavras vizinhas não mascaradas.\n",
    "\n",
    "Imagine a frase: \"O gato está dormindo no tapete.\"\n",
    "\n",
    "No processo de mascaramento, poderíamos aleatoriamente escolher algumas palavras para ocultar, substituindo-as por um token especial, geralmente chamado [MASK]. Por exemplo, a frase poderia se tornar:\n",
    "\n",
    "\"O [MASK] está dormindo no [MASK].\"\n",
    "\n",
    "O modelo de linguagem, durante o treinamento, receberia essa versão mascarada da frase como entrada e teria como objetivo prever as palavras originais que foram substituídas por [MASK]. Neste caso, o modelo deveria aprender a prever \"gato\" para o primeiro [MASK] e \"tapete\" para o segundo."
   ]
  },
  {
   "cell_type": "code",
   "id": "977366761074a52b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:06:00.205959Z",
     "start_time": "2025-04-08T20:06:00.190949Z"
    }
   },
   "source": [
    "def get_masked_input_and_labels( encoded_texts ):\n",
    "    # Cria um array NumPy com o mesmo tamanho de encoded_texts preenchido com números aleatórios entre 0 e 1\n",
    "    # Vai comparar cada um dos números com 0.15, se for maior ou igual, será True, do contrário, será False\n",
    "    # inp_mask será um array de booleanos de mesmo tamanho que encoded_texts\n",
    "    inp_mask = np.random.rand( *encoded_texts.shape ) < 0.15\n",
    "\n",
    "    # Não deixa realizar o mascaramento em tokens especiais\n",
    "    inp_mask[ encoded_texts <= 2 ] = False\n",
    "\n",
    "    # Cria um array com o mesmo tamanho de encoded_texts preenchido com o valor -1\n",
    "    # O valor -1 é usado para indicar que esses tokens não são alvos para a previsão durante o treinamento\n",
    "    labels = -1 * np.ones( encoded_texts.shape, dtype = int )\n",
    "\n",
    "    # Para as posições no array qye são True em inp_mask, os valores dos IDs dos tokens são atribuídos a labels\n",
    "    # Assim, labels terá os IDs dos tokens que foram mascarados, e o modelo terá que prever esses IDs\n",
    "    labels[ inp_mask ] = encoded_texts[ inp_mask ]\n",
    "\n",
    "    # Cria uma cópia de encoded_texts\n",
    "    encoded_texts_masked = np.copy( encoded_texts )\n",
    "\n",
    "    # Cria uma nova máscara booleana\n",
    "    # Ela é True apenas nas posições onde inp_mask também é True E um novo número aleatório gerado para\n",
    "    # essa posição é menor que 0.9\n",
    "    # Apenas 90% dos 15% dos tokens selecionados para mascaramente, serão, de fato, mascarados\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand( *encoded_texts.shape ) < 0.90)\n",
    "\n",
    "    # Atualizando posições onde inp_mask_2mask são True para a máscara\n",
    "    encoded_texts_masked[ inp_mask_2mask ] = mask_token_id\n",
    "\n",
    "    # Cria uma nova máscara booleana\n",
    "    # Ela é True apenas nas posições onde inp_mask_2mask também é True E um novo número aleatório gerado para\n",
    "    # essa posição é menor que 1/9\n",
    "    # 1/9 dos 90% serão tokens aleatórios\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand( *encoded_texts.shape ) < 1 / 9)\n",
    "\n",
    "    # Nas posições onde inp_mask_2random é True, o token em encoded_texts_masked será um token aleatório\n",
    "    # Gera um array de números aleatórios partindo de 3 e indo até antes de mask_token_id\n",
    "    # Começou em 3 porque as primeiras posições foram excluídas\n",
    "    encoded_texts_masked[ inp_mask_2random ] = np.random.randint(\n",
    "            3, mask_token_id, inp_mask_2random.sum()\n",
    "    )\n",
    "\n",
    "    # Cria um array de mesmo tamanho que labels preenchido com 1\n",
    "    sample_weights = np.ones( labels.shape )\n",
    "\n",
    "    # Nas posições dos tokens que têm -1, o valor em sample_weights será 0\n",
    "    # Isso significa que a perda durante o treinamento será calculada apenas para os tokens\n",
    "    # que foram realmente mascarados\n",
    "    sample_weights[ labels == -1 ] = 0\n",
    "\n",
    "    # Cria uma cópia de encoded_texts\n",
    "    y_labels = np.copy( encoded_texts )\n",
    "\n",
    "    # A versão da entrada com alguns tokens substituídos por [MASK] ou por tokens aleatórios\n",
    "    # Os tokens originais da entrada, que servem como os rótulos para o treinamento\n",
    "    # Um array de pesos que indica quais posições em y_labels devem ser consideradas no cálculo da perda\n",
    "    return encoded_texts_masked, y_labels, sample_weights"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "id": "fc0c9d5289de1a13",
   "metadata": {},
   "source": [
    "## Codificação e Datasets\n",
    "\n",
    "1. Classificação de Sentimentos: Cria datasets de treinamento e teste (train_classifier_ds e test_classifier_ds) onde as revisões são codificadas e emparelhadas com seus rótulos de sentimento.\n",
    "\n",
    "2. Modelo de Linguagem Mascarada: Cria um dataset (mlm_ds) para treinar um modelo a prever palavras mascaradas em um conjunto de todas as revisões."
   ]
  },
  {
   "cell_type": "code",
   "id": "55a74b6efbf9b0d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:06:00.285958Z",
     "start_time": "2025-04-08T20:06:00.209974Z"
    }
   },
   "source": [
    "# Codificação das letras\n",
    "x_train = encode( train_df.lyric.values )\n",
    "x_test = encode( test_df.lyric.values )"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "id": "d546c097a2d0259e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:06:00.348930Z",
     "start_time": "2025-04-08T20:06:00.320946Z"
    }
   },
   "source": [
    "# Mascaramento das letras\n",
    "x_train_masked, y_train_labels, train_sample_weights = get_masked_input_and_labels(\n",
    "        x_train\n",
    ")\n",
    "x_val_masked, y_val_labels, val_sample_weights = get_masked_input_and_labels(\n",
    "        x_test\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:06:00.427092Z",
     "start_time": "2025-04-08T20:06:00.387105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Criação dos Datasets TensorFlow\n",
    "\n",
    "# Dataset de Treino para MLM\n",
    "mlm_train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        (x_train_masked, y_train_labels, train_sample_weights)\n",
    ")\n",
    "# Embaralha e agrupa em lotes\n",
    "mlm_train_ds = mlm_train_ds.shuffle( 1000 ).batch( config.BATCH_SIZE ).prefetch( tf.data.AUTOTUNE )\n",
    "\n",
    "# Dataset de Teste para MLM\n",
    "mlm_val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        (x_val_masked, y_val_labels, val_sample_weights)\n",
    ")\n",
    "# APENAS agrupa em lotes (não precisa embaralhar teste)\n",
    "mlm_val_ds = mlm_val_ds.batch( config.BATCH_SIZE ).prefetch( tf.data.AUTOTUNE )\n"
   ],
   "id": "5a6318f63c1b3e6",
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "id": "776d07c167f0b164",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:06:00.473277Z",
     "start_time": "2025-04-08T20:06:00.460281Z"
    }
   },
   "source": [
    "# Aplica a codificação em toda a base\n",
    "# x_all_review = encode( all_data..values )\n",
    "\n",
    "# Passa a codificação da base para realizar o mascaramento dos dados\n",
    "\n",
    "# x_masked_train: As revisões com alguns tokens mascarados (substituídos por um token especial ou por tokens aleatórios).\n",
    "#   Esta será a entrada para o modelo de linguagem mascarada.\n",
    "\n",
    "# y_masked_labels: Os rótulos para a tarefa de mascaramento.\n",
    "\n",
    "# sample_weights: Pesos que indicam quais tokens devem ser considerados no cálculo da perda\n",
    "# x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
    "#         x_all_review\n",
    "# )\n",
    "\n",
    "# Cria um dataset onde, cada elemento será uma tupla:\n",
    "# - As reviews mascaradas\n",
    "# - Rótulos originais\n",
    "# - Pesos de amostra\n",
    "# mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
    "#         (x_train_masked, y_masked_labels, sample_weights)\n",
    "# )\n",
    "#\n",
    "# # Embaralha o dataset e agrupa em lotes\n",
    "# mlm_ds = mlm_ds.shuffle( 1000 ).batch( config.BATCH_SIZE )"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Treinamento do modelo BERT",
   "id": "67be1561f10b4df5"
  },
  {
   "cell_type": "code",
   "id": "5ac4d706",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:06:00.521277Z",
     "start_time": "2025-04-08T20:06:00.507277Z"
    }
   },
   "source": [
    "def bert_module( query, key, value, i ):\n",
    "\n",
    "    #Fazer uma \"self-attention\" com multiplas cabeças\n",
    "    #self attention e uma maneira do modelo enteder as relacoes de uma sequencia de palavras\n",
    "    #por ter mais de 1 cabeça permite notar essas relacoes em diferentes partes do texto\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "            num_heads = config.NUM_HEAD,\n",
    "            key_dim = config.EMBED_DIM // config.NUM_HEAD,\n",
    "            name = \"encoder_{}_multiheadattention\".format( i ),\n",
    "    )( query, key, value )\n",
    "\n",
    "    #Dropout para ajudar na regularizacao (evitar overfitting)\n",
    "    attention_output = layers.Dropout( 0.1, name = \"encoder_{}_att_dropout\".format( i ) )(\n",
    "            attention_output\n",
    "    )\n",
    "\n",
    "    #Layer normalization da atencao que foi calculada e da entrada original\n",
    "    #serve para garantir o processo de informacao de maneira consistente, ajudando na performance e eficiencia\n",
    "    attention_output = layers.LayerNormalization(\n",
    "            epsilon = 1e-6, name = \"encoder_{}_att_layernormalization\".format( i )\n",
    "    )( query + attention_output )\n",
    "\n",
    "    # Feed-forward layer, serve para que informacoes em estagios tadios do processamento sejam\n",
    "    #enviados para estagios mais iniciais\n",
    "    ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense( config.FF_DIM, activation = \"relu\" ),  #expande  dimensao para FF_DIMe ativacao ReLU\n",
    "                layers.Dense( config.EMBED_DIM ),  #volta para a dimensao original\n",
    "            ],\n",
    "            name = \"encoder_{}_ffn\".format( i ),\n",
    "    )\n",
    "\n",
    "    #aplicacao de dropout novamente\n",
    "    ffn_output = ffn( attention_output )\n",
    "    ffn_output = layers.Dropout( 0.1, name = \"encoder_{}_ffn_dropout\".format( i ) )(\n",
    "            ffn_output\n",
    "    )\n",
    "\n",
    "    #layer normalization novamente com a saida da camada de atencao\n",
    "    sequence_output = layers.LayerNormalization(\n",
    "            epsilon = 1e-6, name = \"encoder_{}_ffn_layernormalization\".format( i )\n",
    "    )( attention_output + ffn_output )\n",
    "\n",
    "    #o resultado possui a mesma dimensao da entrada porem com representacoes melhores\n",
    "    #mesmos dados porem enriquecidos de contexto\n",
    "    return sequence_output\n"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "id": "9af62811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:06:00.569280Z",
     "start_time": "2025-04-08T20:06:00.555277Z"
    }
   },
   "source": [
    "#Funcao de perda de entropia cruzada esparsa\n",
    "#usada quando tem duas ou mais classes de label\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy( reduction = None )\n",
    "#media dos valores de perda ao longo do treinamento\n",
    "loss_tracker = keras.metrics.Mean( name = \"loss\" )\n",
    "\n",
    "\n",
    "#esta classe serve para calcular as metricas e a perda de maneira customizada\n",
    "#assim temos um controle maior do treinamento\n",
    "class MaskedLanguageModel( keras.Model ):\n",
    "\n",
    "    def compute_loss( self, x = None, y = None, y_pred = None, sample_weight = None ):\n",
    "        #ja que na funcao de perda nao tem reducao vamos ter um valor de perda pra cada exemplo\n",
    "        #entao o fazemos com que a soma total seja o que sera otimizada\n",
    "        loss = loss_fn( y, y_pred, sample_weight )\n",
    "        loss_tracker.update_state( loss, sample_weight = sample_weight )\n",
    "        return keras.ops.sum( loss )\n",
    "\n",
    "    def compute_metrics( self, x, y, y_pred, sample_weight ):\n",
    "        # Retorna um dicionario com a perda media do loss_tracker\n",
    "        return { \"loss\": loss_tracker.result() }\n",
    "\n",
    "    @property\n",
    "    def metrics( self ):\n",
    "        #este serve para que nao seja necessario o reset manual das metricas\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [ loss_tracker ]"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "id": "83087148",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:06:00.647308Z",
     "start_time": "2025-04-08T20:06:00.603317Z"
    }
   },
   "source": [
    "def create_masked_language_bert_model():\n",
    "    #configura a camada de entrada para que seja uma lista de tokens INT\n",
    "    inputs = layers.Input( (config.MAX_LEN,), dtype = \"int64\" )\n",
    "\n",
    "    #converte os IDs dos tokens em vetores\n",
    "    #cava vetor tera  dimensao de EMBED_DIM\n",
    "    word_embeddings = layers.Embedding(\n",
    "            config.VOCAB_SIZE, config.EMBED_DIM, name = \"word_embedding\"\n",
    "    )( inputs )\n",
    "\n",
    "    #cria a informacao adicional das posicoes dos tokens para o modelo usando a bibliote do keras_hub\n",
    "    position_embeddings = keras_hub.layers.PositionEmbedding(\n",
    "            sequence_length = config.MAX_LEN\n",
    "    )( word_embeddings )\n",
    "\n",
    "    #combina as duas informacoes criadas, a de semantica de a de posicao\n",
    "    embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "    encoder_output = embeddings\n",
    "\n",
    "    #para cada camada se aplica o bert, um bloco transformer contendo Multi-Head Self-Attention e Feed-Forward Network\n",
    "    for i in range( config.NUM_LAYERS ):\n",
    "        encoder_output = bert_module( encoder_output, encoder_output, encoder_output, i )\n",
    "\n",
    "    #camada de classificacao para o mlm, transformando a saída do encoder em predições de tokens\n",
    "    #a função de ativação softmax transforma as saídas brutas da rede neural em um vetor de probabilidades\n",
    "    mlm_output = layers.Dense( config.VOCAB_SIZE, name = \"mlm_cls\", activation = \"softmax\" )(\n",
    "            encoder_output\n",
    "    )\n",
    "\n",
    "    #criar a instancia da classe mlm que ja definimos\n",
    "    mlm_model = MaskedLanguageModel( inputs, mlm_output, name = \"masked_bert_model\" )\n",
    "\n",
    "    #otimizacao usando \"Adam\" e compilacao\n",
    "    optimizer = keras.optimizers.Adam( learning_rate = config.LR )\n",
    "    mlm_model.compile( optimizer = optimizer )\n",
    "    return mlm_model\n",
    "\n",
    "\n",
    "#mapeamento de ID para token e vice versa para facilitar interpretacao dos dados\n",
    "id2token = dict( enumerate( vectorize_layer.get_vocabulary() ) )\n",
    "token2id = { y: x for x, y in id2token.items() }"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "id": "874798b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:06:00.791302Z",
     "start_time": "2025-04-08T20:06:00.681304Z"
    }
   },
   "source": [
    "#por herdar keras.callbacks.Callback pode ser usada durante o treinamento\n",
    "class MaskedTextGenerator( keras.callbacks.Callback ):\n",
    "    def __init__( self, sample_tokens, top_k = 5 ):\n",
    "        self.sample_tokens = sample_tokens  #exemplo de entrada\n",
    "        self.k = top_k  #quantos candidatos serao considerados para cada mascara\n",
    "\n",
    "    #sequencia de tokens para uma string\n",
    "    def decode( self, tokens ):\n",
    "        return \" \".join( [ id2token.get( int( t ), \"[UNK]\" ) for t in tokens if t != 0 ] )\n",
    "\n",
    "    #converte o seu ID para seu token correspondente\n",
    "    def convert_ids_to_tokens( self, id ):\n",
    "        return id2token[ id ]\n",
    "\n",
    "    #executado ao final de cada epoca\n",
    "    def on_epoch_end( self, epoch, logs = None ):\n",
    "        #recebe as previsoes do modelo para o exemplo fornecido\n",
    "        prediction = self.model.predict( self.sample_tokens )\n",
    "\n",
    "        #procura onde esta os tokens de mascara\n",
    "        masked_index = np.where( self.sample_tokens == mask_token_id )\n",
    "        masked_index = masked_index[ 1 ]\n",
    "\n",
    "        #pega as probabilidades de predicao para os tokens mascarados\n",
    "        mask_prediction = prediction[ 0 ][ masked_index ]\n",
    "\n",
    "        #ordena e armazena em values os top_k (5 neste caso) maiores probabilidades\n",
    "        top_indices = mask_prediction[ 0 ].argsort()[ -self.k: ][ ::-1 ]\n",
    "        values = mask_prediction[ 0 ][ top_indices ]\n",
    "\n",
    "        #para cada entre os top candidatos cria uma copia e substitui\n",
    "        #depois printa o texto original e os resultados com as palavras preditas junto com a probabilidade e token\n",
    "        for i in range( len( top_indices ) ):\n",
    "            p = top_indices[ i ]\n",
    "            v = values[ i ]\n",
    "\n",
    "            # Verifica se self.sample_tokens é um tensor do PyTorch\n",
    "            if isinstance( self.sample_tokens, torch.Tensor ):\n",
    "                tokens = np.copy( self.sample_tokens[ 0 ].cpu().numpy() )  # Se for tensor, move para CPU\n",
    "            else:\n",
    "                tokens = np.copy( self.sample_tokens[ 0 ] )  # Se já for numpy.ndarray, apenas faz a cópia\n",
    "\n",
    "            tokens[ masked_index[ 0 ] ] = p\n",
    "            result = {\n",
    "                \"input_text\": self.decode( self.sample_tokens[ 0 ].cpu().numpy() ) if isinstance( self.sample_tokens,\n",
    "                                                                                                  torch.Tensor\n",
    "                                                                                                  ) else self.decode(\n",
    "                        self.sample_tokens[ 0 ]\n",
    "                ),\n",
    "                \"prediction\": self.decode( tokens ),\n",
    "                \"probability\": v,\n",
    "                \"predicted mask token\": self.convert_ids_to_tokens( p ),\n",
    "            }\n",
    "            pprint( result )\n",
    "\n",
    "\n",
    "#converte o exemplo para uma sequencia de IDs e instacia com o callback com o exemplo, este sera usado nas predicoes no final de cada epoca\n",
    "sample_tokens = vectorize_layer( [ \"Quero [mask] com você\" ] )\n",
    "\n",
    "# Caso sample_tokens seja um tensor PyTorch, mova para a CPU antes de passar para o callback\n",
    "if isinstance( sample_tokens, torch.Tensor ):\n",
    "    generator_callback = MaskedTextGenerator( sample_tokens.cpu().numpy()\n",
    "                                              )  # Movendo para a CPU e convertendo para numpy\n",
    "else:\n",
    "    generator_callback = MaskedTextGenerator( sample_tokens )\n",
    "\n",
    "#cria o modelo bert pra o mlm e resume com summary()\n",
    "bert_masked_model = create_masked_language_bert_model()\n",
    "bert_masked_model.summary()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"masked_bert_model\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"masked_bert_model\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape     \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m   Param #\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mConnected to     \u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)       │          \u001B[38;5;34m0\u001B[0m │ -                 │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_embedding      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  │  \u001B[38;5;34m3,840,000\u001B[0m │ input_layer_2[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mEmbedding\u001B[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_embedding… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  │     \u001B[38;5;34m32,768\u001B[0m │ word_embedding[\u001B[38;5;34m0\u001B[0m… │\n",
       "│ (\u001B[38;5;33mPositionEmbedding\u001B[0m) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (\u001B[38;5;33mAdd\u001B[0m)         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  │          \u001B[38;5;34m0\u001B[0m │ word_embedding[\u001B[38;5;34m0\u001B[0m… │\n",
       "│                     │                   │            │ position_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_multihea… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  │     \u001B[38;5;34m66,048\u001B[0m │ add_3[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],      │\n",
       "│ (\u001B[38;5;33mMultiHeadAttentio…\u001B[0m │                   │            │ add_3[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],      │\n",
       "│                     │                   │            │ add_3[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_att_drop… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  │          \u001B[38;5;34m0\u001B[0m │ encoder_0_multih… │\n",
       "│ (\u001B[38;5;33mDropout\u001B[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_4 (\u001B[38;5;33mAdd\u001B[0m)         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  │          \u001B[38;5;34m0\u001B[0m │ add_3[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],      │\n",
       "│                     │                   │            │ encoder_0_att_dr… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_att_laye… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  │        \u001B[38;5;34m256\u001B[0m │ add_4[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]       │\n",
       "│ (\u001B[38;5;33mLayerNormalizatio…\u001B[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_ffn       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  │     \u001B[38;5;34m33,024\u001B[0m │ encoder_0_att_la… │\n",
       "│ (\u001B[38;5;33mSequential\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_ffn_drop… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  │          \u001B[38;5;34m0\u001B[0m │ encoder_0_ffn[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mDropout\u001B[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_5 (\u001B[38;5;33mAdd\u001B[0m)         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  │          \u001B[38;5;34m0\u001B[0m │ encoder_0_att_la… │\n",
       "│                     │                   │            │ encoder_0_ffn_dr… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_ffn_laye… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m, \u001B[38;5;34m128\u001B[0m)  │        \u001B[38;5;34m256\u001B[0m │ add_5[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]       │\n",
       "│ (\u001B[38;5;33mLayerNormalizatio…\u001B[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlm_cls (\u001B[38;5;33mDense\u001B[0m)     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m,       │  \u001B[38;5;34m3,870,000\u001B[0m │ encoder_0_ffn_la… │\n",
       "│                     │ \u001B[38;5;34m30000\u001B[0m)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_embedding      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,000</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_embedding… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span> │ word_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionEmbedding</span>) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ word_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ position_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_multihea… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                     │                   │            │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_att_drop… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_0_multih… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                     │                   │            │ encoder_0_att_dr… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_att_laye… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_ffn       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ encoder_0_att_la… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_ffn_drop… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_0_ffn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_0_att_la… │\n",
       "│                     │                   │            │ encoder_0_ffn_dr… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_ffn_laye… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlm_cls (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,870,000</span> │ encoder_0_ffn_la… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">30000</span>)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m7,842,352\u001B[0m (29.92 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,842,352</span> (29.92 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m7,842,352\u001B[0m (29.92 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,842,352</span> (29.92 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "id": "f99975dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:13:25.338255Z",
     "start_time": "2025-04-08T20:06:00.845445Z"
    }
   },
   "source": [
    "# Calcula os passos por época para treino e validação\n",
    "# Usar len(train_df) e len(test_df) para os cálculos\n",
    "train_steps_per_epoch: int = len( train_df ) // config.BATCH_SIZE\n",
    "val_steps_per_epoch: int = len( test_df ) // config.BATCH_SIZE\n",
    "\n",
    "print( f\"Passos por época (Treino): {train_steps_per_epoch}\" )\n",
    "print( f\"Passos por época (Validação): {val_steps_per_epoch}\" )\n",
    "\n",
    "# Treina o modelo usando os datasets separados e validation_data\n",
    "history = bert_masked_model.fit(\n",
    "        mlm_train_ds,\n",
    "        validation_data = mlm_val_ds,  # Passa o dataset de validação\n",
    "        epochs = 5,  # Ou o número de épocas desejado\n",
    "        steps_per_epoch = train_steps_per_epoch,  # Passos para treino\n",
    "        validation_steps = val_steps_per_epoch,  # Passos para validação\n",
    "        callbacks = [ generator_callback ]  # Mantém o callback para ver exemplos\n",
    ")\n",
    "\n",
    "bert_masked_model.save( \"bert_mlm.keras\" )\n",
    "print( \"\\nTreinamento concluído e modelo salvo!\" )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passos por época (Treino): 90\n",
      "Passos por época (Validação): 22\n",
      "Epoch 1/5\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('o'),\n",
      " 'prediction': 'estou com muita o de você',\n",
      " 'probability': np.float32(0.0002959077)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('de'),\n",
      " 'prediction': 'estou com muita de de você',\n",
      " 'probability': np.float32(0.00028482193)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('você'),\n",
      " 'prediction': 'estou com muita você de você',\n",
      " 'probability': np.float32(0.00027802182)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('não'),\n",
      " 'prediction': 'estou com muita não de você',\n",
      " 'probability': np.float32(0.00027384982)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('eu'),\n",
      " 'prediction': 'estou com muita eu de você',\n",
      " 'probability': np.float32(0.00026912426)}\n",
      "\u001B[1m90/90\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m141s\u001B[0m 2s/step - loss: 10.1283 - val_loss: 8.9360\n",
      "Epoch 2/5\n",
      "\u001B[1m 1/90\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m6s\u001B[0m 76ms/step - loss: 8.9400"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Codes\\transformer_mlm\\.venv\\lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('o'),\n",
      " 'prediction': 'estou com muita o de você',\n",
      " 'probability': np.float32(0.000304245)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('de'),\n",
      " 'prediction': 'estou com muita de de você',\n",
      " 'probability': np.float32(0.00029257123)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('você'),\n",
      " 'prediction': 'estou com muita você de você',\n",
      " 'probability': np.float32(0.0002857859)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('não'),\n",
      " 'prediction': 'estou com muita não de você',\n",
      " 'probability': np.float32(0.0002815147)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('eu'),\n",
      " 'prediction': 'estou com muita eu de você',\n",
      " 'probability': np.float32(0.0002767419)}\n",
      "\u001B[1m90/90\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 174ms/step - loss: 8.9400 - val_loss: 8.9160\n",
      "Epoch 3/5\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('de'),\n",
      " 'prediction': 'estou com muita de de você',\n",
      " 'probability': np.float32(0.0026194078)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('o'),\n",
      " 'prediction': 'estou com muita o de você',\n",
      " 'probability': np.float32(0.0025322803)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('não'),\n",
      " 'prediction': 'estou com muita não de você',\n",
      " 'probability': np.float32(0.002499995)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('eu'),\n",
      " 'prediction': 'estou com muita eu de você',\n",
      " 'probability': np.float32(0.0024675438)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('você'),\n",
      " 'prediction': 'estou com muita você de você',\n",
      " 'probability': np.float32(0.0023485415)}\n",
      "\u001B[1m90/90\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m135s\u001B[0m 2s/step - loss: 8.5650 - val_loss: 7.5195\n",
      "Epoch 4/5\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('de'),\n",
      " 'prediction': 'estou com muita de de você',\n",
      " 'probability': np.float32(0.002672308)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('o'),\n",
      " 'prediction': 'estou com muita o de você',\n",
      " 'probability': np.float32(0.0025739383)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('não'),\n",
      " 'prediction': 'estou com muita não de você',\n",
      " 'probability': np.float32(0.002543028)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('eu'),\n",
      " 'prediction': 'estou com muita eu de você',\n",
      " 'probability': np.float32(0.0025143395)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('você'),\n",
      " 'prediction': 'estou com muita você de você',\n",
      " 'probability': np.float32(0.0023906198)}\n",
      "\u001B[1m90/90\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 173ms/step - loss: 7.8652 - val_loss: 7.5095\n",
      "Epoch 5/5\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('de'),\n",
      " 'prediction': 'estou com muita de de você',\n",
      " 'probability': np.float32(0.010324493)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('eu'),\n",
      " 'prediction': 'estou com muita eu de você',\n",
      " 'probability': np.float32(0.010120981)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('não'),\n",
      " 'prediction': 'estou com muita não de você',\n",
      " 'probability': np.float32(0.0100432625)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('o'),\n",
      " 'prediction': 'estou com muita o de você',\n",
      " 'probability': np.float32(0.009678461)}\n",
      "{'input_text': 'estou com muita [mask] de você',\n",
      " 'predicted mask token': np.str_('e'),\n",
      " 'prediction': 'estou com muita e de você',\n",
      " 'probability': np.float32(0.008902256)}\n",
      "\u001B[1m90/90\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m136s\u001B[0m 2s/step - loss: 7.3733 - val_loss: 6.8605\n",
      "\n",
      "Treinamento concluído e modelo salvo!\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:13:25.651956Z",
     "start_time": "2025-04-08T20:13:25.404231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Plotar histórico de loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot( history.history[ 'loss' ], label = 'Training Loss' )\n",
    "plt.plot( history.history[ 'val_loss' ], label = 'Validation Loss' )  # Keras nomeia a loss de validação como 'val_loss'\n",
    "plt.title( 'Training and Validation Loss' )\n",
    "plt.xlabel( 'Epoch' )\n",
    "plt.ylabel( 'Loss' )\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "80d9bb8dbdfce5b2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:14:17.176329Z",
     "start_time": "2025-04-08T20:14:17.162328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retoma o treinamento\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Carrega o modelo salvo\n",
    "if 0:\n",
    "    bert_mlm_model = load_model( \"bert_mlm.keras\" )\n",
    "    bert_mlm_model.fit( mlm_ds.repeat(), epochs = 5, steps_per_epoch = steps, callbacks = [ generator_callback ] )"
   ],
   "id": "54e2be6bf43e279a",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Predição de Frases\n",
    "\n",
    "Utilização do modelo treinado para realizar a predição de novas frases."
   ],
   "id": "a5c8af1d595f4aa2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:14:19.586576Z",
     "start_time": "2025-04-08T20:14:19.563580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Função para prever o token mascarado e mostrar top K predições\n",
    "def predict_masked_token( text_with_mask: str, model, vectorize_layer, id2token, top_k = 5 ):\n",
    "    \"\"\"\n",
    "    Prevê o token mascarado em uma frase e retorna as top K previsões com probabilidades.\n",
    "\n",
    "    Args:\n",
    "        text_with_mask (str): A frase de entrada contendo exatamente um token '[mask]'.\n",
    "        model (keras.Model): O modelo treinado (MLM).\n",
    "        vectorize_layer (TextVectorization): A camada de vetorização JÁ ADAPTADA.\n",
    "        id2token (dict): Dicionário mapeando ID para token.\n",
    "        top_k (int): Quantidade de melhores previsões a serem mostradas.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame com as top K previsões, tokens e probabilidades.\n",
    "                         Retorna None se '[mask]' não for encontrado ou mais de um for encontrado.\n",
    "    \"\"\"\n",
    "    # 1. Vetorizar a frase de entrada\n",
    "    #    O reshape [1, -1] adiciona a dimensão do batch (lote de tamanho 1)\n",
    "    sample_tokens = vectorize_layer( [ text_with_mask ] )  # Não precisa de .numpy() aqui para predict\n",
    "\n",
    "    # Verifica se a vetorização retornou algo e se tem o formato esperado\n",
    "    if sample_tokens is None or sample_tokens.shape[ 0 ] == 0:\n",
    "        print( \"Erro: A vetorização não produziu uma saída válida.\" )\n",
    "        return None\n",
    "\n",
    "    # 2. Encontrar o índice do token [mask]\n",
    "    #    `.numpy()` converte o tensor para numpy array para usar np.where\n",
    "    #    Pega o id do token de máscara global que definimos antes\n",
    "    masked_index_tuple = np.where( sample_tokens.cpu().numpy()[ 0 ] == mask_token_id )\n",
    "\n",
    "    # Verifica se encontrou o token de máscara e se é apenas um\n",
    "    if len( masked_index_tuple[ 0 ] ) != 1:\n",
    "        print(\n",
    "                f\"Erro: Esperado exatamente um token '[mask]' na frase vetorizada, mas encontrado(s) {len( masked_index_tuple[ 0 ] )}.\"\n",
    "        )\n",
    "        print( f\"Frase vetorizada: {sample_tokens.numpy()[ 0 ]}\" )\n",
    "        print( f\"ID esperado para [mask]: {mask_token_id}\" )\n",
    "        # Tenta decodificar para ajudar a depurar\n",
    "        try:\n",
    "            decoded_tokens = [ id2token.get( int( t ), \"[UNK]\" ) for t in sample_tokens.numpy()[ 0 ] if t != 0 ]\n",
    "            print( f\"Tokens decodificados: {' '.join( decoded_tokens )}\" )\n",
    "        except Exception as e:\n",
    "            print( f\"Não foi possível decodificar os tokens: {e}\" )\n",
    "        return None\n",
    "\n",
    "    masked_index = masked_index_tuple[ 0 ][ 0 ]  # Pega o índice escalar\n",
    "\n",
    "    # 3. Fazer a predição com o modelo\n",
    "    prediction = model.predict( sample_tokens )\n",
    "\n",
    "    # 4. Extrair as probabilidades para a posição mascarada\n",
    "    #    prediction[0] -> batch de tamanho 1\n",
    "    #    prediction[0][masked_index] -> probabilidades para o token na posição mascarada\n",
    "    mask_prediction_probabilities = prediction[ 0 ][ masked_index ]\n",
    "\n",
    "    # 5. Encontrar os top K índices e suas probabilidades\n",
    "    #    argsort() retorna os índices que ordenariam o array (do menor para o maior)\n",
    "    #    [-top_k:] pega os K maiores índices\n",
    "    #    [::-1] inverte para ter do maior para o menor\n",
    "    top_indices = mask_prediction_probabilities.argsort()[ -top_k: ][ ::-1 ]\n",
    "    top_probabilities = mask_prediction_probabilities[ top_indices ]\n",
    "\n",
    "    # 6. Converter os IDs dos tokens previstos para palavras\n",
    "    top_tokens = [ id2token.get( idx, \"[UNK]\" ) for idx in top_indices ]\n",
    "\n",
    "    # 7. Criar e retornar um DataFrame com os resultados\n",
    "    results_df = pd.DataFrame( {\n",
    "        \"Token Previsto\": top_tokens,\n",
    "        \"Probabilidade\": top_probabilities\n",
    "    }\n",
    "    )\n",
    "\n",
    "    print( f\"\\nPredições para: '{text_with_mask}'\" )\n",
    "    return results_df"
   ],
   "id": "b08a6f73cd999b58",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:15:08.101139Z",
     "start_time": "2025-04-08T20:15:08.001622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "frase_teste_1 = \"Quero [mask] com você\"\n",
    "resultados_df_1 = predict_masked_token( frase_teste_1, bert_masked_model, vectorize_layer, id2token, top_k = 10 )\n",
    "\n",
    "if resultados_df_1 is not None:\n",
    "    print( resultados_df_1.to_string( index = False ) )  # .to_string para melhor formatação no print"
   ],
   "id": "c63d4fb3a3660983",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step\n",
      "\n",
      "Predições para: 'Quero [mask] com você'\n",
      "Token Previsto  Probabilidade\n",
      "            de       0.010399\n",
      "            eu       0.010117\n",
      "           não       0.009954\n",
      "             o       0.009710\n",
      "             e       0.008874\n",
      "          você       0.008484\n",
      "             a       0.007839\n",
      "            me       0.007106\n",
      "           pra       0.006496\n",
      "             é       0.005465\n"
     ]
    }
   ],
   "execution_count": 71
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
