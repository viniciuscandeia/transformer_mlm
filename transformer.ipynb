{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e98cd1fa27373e9",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "- Encoder: O Encoder recebe a sequência de entrada (por exemplo, uma frase em português) e a processa para criar uma representação vetorial de alta qualidade dessa sequência. Essa representação captura o significado e o contexto de cada palavra em relação às outras palavras da frase.\n",
    "- Decoder: O Decoder recebe a representação gerada pelo Encoder e a utiliza para gerar uma sequência de saída (por exemplo, a tradução da frase para o inglês).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beae2a5e32306d38",
   "metadata": {},
   "source": [
    "# Masked Language Model\n",
    "\n",
    "Um Masked Language Model (MLM) é um tipo de modelo de linguagem amplamente utilizado em processamento de linguagem natural.\n",
    "\n",
    "Durante o treinamento, uma parte dos tokens (palavras ou subpalavras) no texto de entrada é substituída por um token especial de máscara, como \"[MASK]\". O objetivo do modelo é prever corretamente quais eram os tokens originais que foram mascarados.\n",
    "\n",
    "Essa estratégia obriga o modelo a aprender contextos ricos e relações entre as palavras, o que é fundamental para o desempenho em diversas tarefas, como análise de sentimentos, tradução, e resposta a perguntas. Modelos famosos que utilizam essa técnica incluem o BERT, que demonstrou ganhos significativos em várias benchmarks de NLP .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8259a8ef1b3febb",
   "metadata": {},
   "source": [
    "## IMBD\n",
    "\n",
    "Para este projeto, será utilizado a base do IMBD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bed58b15c8e73d",
   "metadata": {},
   "source": [
    "## Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a89620b2e2393cb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:49:18.484928Z",
     "start_time": "2025-04-04T13:49:18.479383Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[ \"KERAS_BACKEND\" ] = \"torch\"  # or jax, or tensorflow\n",
    "\n",
    "import keras_hub\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cce974cf0c26a19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:49:18.493282Z",
     "start_time": "2025-04-04T13:49:18.489637Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 256\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 0.001\n",
    "    VOCAB_SIZE = 30000\n",
    "    EMBED_DIM = 128\n",
    "    NUM_HEAD = 8  # used in bert model\n",
    "    FF_DIM = 128  # used in bert model\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39422ab42f331838",
   "metadata": {},
   "source": [
    "# Carregando os dados\n",
    "\n",
    "Primeiro, vamos carregar os dados que estão na pasta \"aclImbd\".\n",
    "\n",
    "Duas funções serão utilizadas para isso:\n",
    "\n",
    "- Uma irá criar uma lista contendo o conteúdo dos arquivos.\n",
    "- A outra ficará responsável por criar um dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "822b332050609de1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:49:26.333078Z",
     "start_time": "2025-04-04T13:49:18.505936Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_text_list_from_files( files ) -> list[ str ]:\n",
    "    \"\"\"\n",
    "       Esta função irá retornar uma lista contendo todas as frases dos arquivos.\n",
    "    \"\"\"\n",
    "    text_list: list[ str ] = [ ]\n",
    "    for name in files:\n",
    "        with open( name, \"r\", encoding = \"utf-8\" ) as f:\n",
    "            for line in f:\n",
    "                text_list.append( line )\n",
    "    return text_list\n",
    "\n",
    "\n",
    "def get_data_from_text_files( folder_name ):\n",
    "    # Arquivos de texto com avaliações positivas e negativas\n",
    "    pos_files = glob.glob( f\"aclImdb/{folder_name}/pos/*.txt\" )\n",
    "    neg_files = glob.glob( f\"aclImdb/{folder_name}/neg/*.txt\" )\n",
    "\n",
    "    # Listas com as avaliações\n",
    "    pos_texts: list[ str ] = get_text_list_from_files( pos_files )\n",
    "    neg_texts: list[ str ] = get_text_list_from_files( neg_files )\n",
    "\n",
    "    # Criação de um dataframe, com colunas chamadas \"review\" e \"sentiment\"\n",
    "    df = pd.DataFrame(\n",
    "            {\n",
    "                # Concatenação das listas\n",
    "                \"review\": pos_texts + neg_texts,\n",
    "                # Criação de uma nova lista\n",
    "                \"sentiment\": [ 0 ] * len( pos_texts ) + [ 1 ] * len( neg_texts ),\n",
    "            }\n",
    "    )\n",
    "\n",
    "    # Sample -> pega uma amostra aleatória\n",
    "    # len(df) -> do tamanho do df original\n",
    "    # reset_index -> ao usar sample, o índice original das linhas é mantido\n",
    "    df = df.sample( len( df ) ).reset_index( drop = True )\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = get_data_from_text_files( \"train\" )\n",
    "test_df = get_data_from_text_files( \"test\" )\n",
    "\n",
    "# Concatenação dos dataframes para realizar pré-processamento em toda a base\n",
    "all_data = pd.concat( [ train_df, test_df ], ignore_index = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa3089bbffe5c2b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:49:26.356124Z",
     "start_time": "2025-04-04T13:49:26.349020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is a terrible, terrible film!!!!!!!!!&lt;br ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The wonderfully urbane Ronald Coleman is show-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You spend most of this two-hour film wondering...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The film as entertainment is very good and Jim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In 1988, Paperhouse was hailed as a \"thinking ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  this is a terrible, terrible film!!!!!!!!!<br ...          1\n",
       "1  The wonderfully urbane Ronald Coleman is show-...          0\n",
       "2  You spend most of this two-hour film wondering...          0\n",
       "3  The film as entertainment is very good and Jim...          0\n",
       "4  In 1988, Paperhouse was hailed as a \"thinking ...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e617962188106c4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:49:26.411469Z",
     "start_time": "2025-04-04T13:49:26.407967Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def custom_standardization( input_data ):\n",
    "    \"\"\"Normalização de texto. \"\"\"\n",
    "    # Converter todas as letras para minúsculas\n",
    "    lowercase = tf.strings.lower( input_data )\n",
    "\n",
    "    # Expressão Regular para remover a tag HTML\n",
    "    stripped_html = tf.strings.regex_replace( lowercase, \"<br />\", \" \" )\n",
    "\n",
    "    # Remover caracteres especiais\n",
    "    return tf.strings.regex_replace(\n",
    "            stripped_html,\n",
    "            \"[%s]\" % re.escape( \"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\" ),\n",
    "            \"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1413c85f5980aae3",
   "metadata": {},
   "source": [
    "## Vetorização de Texto\n",
    "\n",
    "Para um transformer, a vetorização de um texto é o processo fundamental de transformar o texto bruto em uma representação numérica que o modelo possa entender e processar. Em essência, é como traduzir a linguagem humana para a linguagem matemática que o transformer consegue trabalhar.\n",
    "\n",
    "Imagine que o transformer é um computador que só entende números. Para que ele consiga ler e compreender um texto, precisamos converter cada palavra (ou parte da palavra) em um conjunto de números. Esse conjunto de números é o que chamamos de vetor.\n",
    "\n",
    "Aqui está um detalhamento do processo de vetorização para um transformer:\n",
    "\n",
    "1. Tokenização: O primeiro passo é dividir o texto em unidades menores, chamadas tokens. Um token pode ser uma palavra inteira, parte de uma palavra (subpalavra), ou até mesmo um caractere. Por exemplo, a frase \"O gato comeu o rato\" poderia ser tokenizada como: [\"O\", \"gato\", \"comeu\", \"o\", \"rato\"].\n",
    "\n",
    "2. Criação do Vocabulário: Em seguida, é criado um vocabulário, que é uma lista de todos os tokens únicos presentes no conjunto de dados de treinamento do modelo. Cada token nesse vocabulário recebe um índice único.\n",
    "\n",
    "3. Indexação: Cada token no texto de entrada é então mapeado para o seu índice correspondente no vocabulário. Usando o exemplo anterior e supondo um vocabulário, os tokens poderiam ser convertidos em índices como: [10, 25, 50, 10, 75].\n",
    "\n",
    "4. Embedding: A etapa crucial para transformers é a criação de embeddings. Em vez de simplesmente usar os índices brutos, cada índice é transformado em um vetor denso de números reais. Esse vetor captura o significado semântico e as relações entre as palavras.\n",
    "\n",
    "    - Word Embeddings: Cada palavra (ou token) é associada a um vetor de baixa dimensionalidade (por exemplo, 512 ou 768 dimensões). Palavras com significados semelhantes tendem a ter vetores próximos no espaço vetorial. Por exemplo, os vetores para \"gato\" e \"felino\" provavelmente estarão mais próximos do que os vetores para \"gato\" e \"carro\".\n",
    "\n",
    "    - Positional Embeddings: Transformers também precisam entender a ordem das palavras em uma frase. Para isso, são adicionados embeddings posicionais aos word embeddings. Esses vetores codificam a posição de cada token na sequência, permitindo que o modelo diferencie entre \"o gato comeu o rato\" e \"o rato comeu o gato\".\n",
    "\n",
    "5. Input para o Transformer: Os vetores resultantes (a soma dos word embeddings e positional embeddings para cada token) são então alimentados como entrada para as diferentes camadas do transformer (como as camadas de atenção)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d8904fb48419e91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:49:26.466780Z",
     "start_time": "2025-04-04T13:49:26.462303Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vectorize_layer( texts: list[ str ], vocab_size: int, max_seq: int, special_tokens: list = [ \"[MASK]\" ] ):\n",
    "    \"\"\"Build Text vectorization layer\n",
    "\n",
    "    Args:\n",
    "      texts (list): List of string i.e input texts\n",
    "      vocab_size (int): vocab size\n",
    "      max_seq (int): Maximum sequence length.\n",
    "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
    "\n",
    "    Returns:\n",
    "        layers.Layer: Return TextVectorization Keras Layer\n",
    "    \"\"\"\n",
    "\n",
    "    # Criação da camada de TextVectorization\n",
    "    vectorize_layer = TextVectorization(\n",
    "            max_tokens = vocab_size,  # Define o tamanho máximo do vocabulário\n",
    "            output_mode = \"int\",  # Define que a saída deve ser uma sequência de números inteiros\n",
    "            standardize = custom_standardization,  # Aplicar função de pré-processamento\n",
    "            output_sequence_length = max_seq,  # Garantir que toda sequência de saída tenha comprimento max_seq\n",
    "    )\n",
    "\n",
    "    # todo Mostrar um exemplo\n",
    "\n",
    "    # Adaptação aos textos de entrada\n",
    "    # A camada \"sabe\" como mapear palavras para números inteiros, com base nas entradas\n",
    "    vectorize_layer.adapt( texts )\n",
    "\n",
    "    # Obtém o vocabulário aprendido\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "    # Por padrão, o TextVectorization coloca \"\" (para padding, índice 0) e \"[UNK]\" (para palavras desconhecidas,\n",
    "    # índice 1) no início do vocabulário.\n",
    "\n",
    "    # Pegando uma porção do vocabulário:\n",
    "    # - Ignorando \"\" e \"[UNK]\"\n",
    "    # - Pega quase tudo, deixando espaço suficiente para special_tokens\n",
    "    vocab = vocab[ 2: vocab_size - len( special_tokens ) ] + special_tokens\n",
    "\n",
    "    # Atualiza o vocabulário da camada\n",
    "    vectorize_layer.set_vocabulary( vocab )\n",
    "\n",
    "    # Retorna a camada\n",
    "    return vectorize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14af57ee8c312389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:49:29.764851Z",
     "start_time": "2025-04-04T13:49:26.504024Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pegando a camada de vetorização\n",
    "vectorize_layer = get_vectorize_layer(\n",
    "        all_data.review.values.tolist(),\n",
    "        config.VOCAB_SIZE,\n",
    "        config.MAX_LEN,\n",
    "        special_tokens = [ \"[mask]\" ],\n",
    ")\n",
    "\n",
    "# Processamento de um novo dado: \"[mask]\"\n",
    "# - Aplica a função de pré-processamento\n",
    "# - Divide em token, pega seu id e cria uma sequência de comprimento config.MAX_LEN\n",
    "# - Converte o resultado para um array NumPy\n",
    "# - Pega o id do \"[mask]\"\n",
    "mask_token_id = vectorize_layer( [ \"[mask]\" ] ).cpu().numpy()[ 0 ][ 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf38423e529f2726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:49:29.780404Z",
     "start_time": "2025-04-04T13:49:29.777192Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode( texts ):\n",
    "    \"\"\" Retorna um array NumPy das sequências numéricas dos textos de entrada.\"\"\"\n",
    "    # Criação das sequências numéricas para os textos de entrada\n",
    "    encoded_texts = vectorize_layer( texts )\n",
    "    # Retorna as sequências como um array NumPy\n",
    "    return encoded_texts.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5c334136918c96",
   "metadata": {},
   "source": [
    "### Contexto para o código: Mascaramento\n",
    "\n",
    "Em essência, o mascaramento envolve ocultar aleatoriamente algumas das palavras (ou tokens) em uma sequência de texto de entrada. O objetivo é fazer com que o modelo aprenda a prever as palavras que foram mascaradas, com base no contexto das palavras vizinhas não mascaradas.\n",
    "\n",
    "Imagine a frase: \"O gato está dormindo no tapete.\"\n",
    "\n",
    "No processo de mascaramento, poderíamos aleatoriamente escolher algumas palavras para ocultar, substituindo-as por um token especial, geralmente chamado [MASK]. Por exemplo, a frase poderia se tornar:\n",
    "\n",
    "\"O [MASK] está dormindo no [MASK].\"\n",
    "\n",
    "O modelo de linguagem, durante o treinamento, receberia essa versão mascarada da frase como entrada e teria como objetivo prever as palavras originais que foram substituídas por [MASK]. Neste caso, o modelo deveria aprender a prever \"gato\" para o primeiro [MASK] e \"tapete\" para o segundo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "977366761074a52b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:49:29.803687Z",
     "start_time": "2025-04-04T13:49:29.796586Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_masked_input_and_labels( encoded_texts ):\n",
    "    # Cria um array NumPy com o mesmo tamanho de encoded_texts preenchido com números aleatórios entre 0 e 1\n",
    "    # Vai comparar cada um dos números com 0.15, se for maior ou igual, será True, do contrário, será False\n",
    "    # inp_mask será um array de booleanos de mesmo tamanho que encoded_texts\n",
    "    inp_mask = np.random.rand( *encoded_texts.shape ) < 0.15\n",
    "\n",
    "    # Não deixa realizar o mascaramento em tokens especiais\n",
    "    inp_mask[ encoded_texts <= 2 ] = False\n",
    "\n",
    "    # Cria um array com o mesmo tamanho de encoded_texts preenchido com o valor -1\n",
    "    # O valor -1 é usado para indicar que esses tokens não são alvos para a previsão durante o treinamento\n",
    "    labels = -1 * np.ones( encoded_texts.shape, dtype = int )\n",
    "\n",
    "    # Para as posições no array qye são True em inp_mask, os valores dos IDs dos tokens são atribuídos a labels\n",
    "    # Assim, labels terá os IDs dos tokens que foram mascarados, e o modelo terá que prever esses IDs\n",
    "    labels[ inp_mask ] = encoded_texts[ inp_mask ]\n",
    "\n",
    "    # Cria uma cópia de encoded_texts\n",
    "    encoded_texts_masked = np.copy( encoded_texts )\n",
    "\n",
    "    # Cria uma nova máscara booleana\n",
    "    # Ela é True apenas nas posições onde inp_mask também é True E um novo número aleatório gerado para\n",
    "    # essa posição é menor que 0.9\n",
    "    # Apenas 90% dos 15% dos tokens selecionados para mascaramente, serão, de fato, mascarados\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand( *encoded_texts.shape ) < 0.90)\n",
    "\n",
    "    # Atualizando posições onde inp_mask_2mask são True para a máscara\n",
    "    encoded_texts_masked[ inp_mask_2mask ] = mask_token_id\n",
    "\n",
    "    # Cria uma nova máscara booleana\n",
    "    # Ela é True apenas nas posições onde inp_mask_2mask também é True E um novo número aleatório gerado para\n",
    "    # essa posição é menor que 1/9\n",
    "    # 1/9 dos 90% serão tokens aleatórios\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand( *encoded_texts.shape ) < 1 / 9)\n",
    "\n",
    "    # Nas posições onde inp_mask_2random é True, o token em encoded_texts_masked será um token aleatório\n",
    "    # Gera um array de números aleatórios partindo de 3 e indo até antes de mask_token_id\n",
    "    # Começou em 3 porque as primeiras posições foram excluídas\n",
    "    encoded_texts_masked[ inp_mask_2random ] = np.random.randint(\n",
    "            3, mask_token_id, inp_mask_2random.sum()\n",
    "    )\n",
    "\n",
    "    # Cria um array de mesmo tamanho que labels preenchido com 1\n",
    "    sample_weights = np.ones( labels.shape )\n",
    "\n",
    "    # Nas posições dos tokens que têm -1, o valor em sample_weights será 0\n",
    "    # Isso significa que a perda durante o treinamento será calculada apenas para os tokens\n",
    "    # que foram realmente mascarados\n",
    "    sample_weights[ labels == -1 ] = 0\n",
    "\n",
    "    # Cria uma cópia de encoded_texts\n",
    "    y_labels = np.copy( encoded_texts )\n",
    "\n",
    "    # A versão da entrada com alguns tokens substituídos por [MASK] ou por tokens aleatórios\n",
    "    # Os tokens originais da entrada, que servem como os rótulos para o treinamento\n",
    "    # Um array de pesos que indica quais posições em y_labels devem ser consideradas no cálculo da perda\n",
    "    return encoded_texts_masked, y_labels, sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0c9d5289de1a13",
   "metadata": {},
   "source": [
    "## Codificação e Datasets\n",
    "\n",
    "1. Classificação de Sentimentos: Cria datasets de treinamento e teste (train_classifier_ds e test_classifier_ds) onde as revisões são codificadas e emparelhadas com seus rótulos de sentimento.\n",
    "\n",
    "2. Modelo de Linguagem Mascarada: Cria um dataset (mlm_ds) para treinar um modelo a prever palavras mascaradas em um conjunto de todas as revisões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55a74b6efbf9b0d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T13:49:37.157938Z",
     "start_time": "2025-04-04T13:49:29.816031Z"
    }
   },
   "outputs": [],
   "source": [
    "# Codificação das reviews do dataframe de treinamento\n",
    "x_train = encode( train_df.review.values )\n",
    "\n",
    "# Pegando os sentimentos de cada review\n",
    "y_train = train_df.sentiment.values\n",
    "\n",
    "train_classifier_ds = (\n",
    "    # Cria um dataset onde cada elemento é uma tupla: a review codificada e o seu sentimento\n",
    "    tf.data.Dataset.from_tensor_slices( (x_train, y_train) )\n",
    "    # Embaralha os dados do dataset\n",
    "    .shuffle( 1000 )\n",
    "    # Agrupa os dados em lotes\n",
    "    .batch( config.BATCH_SIZE )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d546c097a2d0259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificação das reviews do dataframe de teste\n",
    "x_test = encode( test_df.review.values )\n",
    "\n",
    "# Pegando os sentimentos de cada review\n",
    "y_test = test_df.sentiment.values\n",
    "\n",
    "test_classifier_ds = (\n",
    "    # Cria um dataset onde cada elemento é uma tupla: a review codificada e o seu sentimento\n",
    "    tf.data.Dataset.from_tensor_slices( (x_test, y_test) )\n",
    "    # Agrupa os dados em lotes\n",
    "    .batch( config.BATCH_SIZE )\n",
    ")\n",
    "\n",
    "# Armazenando o dataframe de teste para um uso futuro\n",
    "test_raw_classifier_ds = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "776d07c167f0b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica a codificação em toda a base\n",
    "x_all_review = encode( all_data.review.values )\n",
    "\n",
    "# Passa a codificação da base para realizar o mascaramento dos dados\n",
    "\n",
    "# x_masked_train: As revisões com alguns tokens mascarados (substituídos por um token especial ou por tokens aleatórios).\n",
    "#   Esta será a entrada para o modelo de linguagem mascarada.\n",
    "\n",
    "# y_masked_labels: Os rótulos para a tarefa de mascaramento.\n",
    "\n",
    "# sample_weights: Pesos que indicam quais tokens devem ser considerados no cálculo da perda\n",
    "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
    "        x_all_review\n",
    ")\n",
    "\n",
    "# Cria um dataset onde, cada elemento será uma tupla:\n",
    "# - As reviews mascaradas\n",
    "# - Rótulos originais\n",
    "# - Pesos de amostra\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        (x_masked_train, y_masked_labels, sample_weights)\n",
    ")\n",
    "\n",
    "# Embaralha o dataset e agrupa em lotes\n",
    "mlm_ds = mlm_ds.shuffle( 1000 ).batch( config.BATCH_SIZE )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3613e3b",
   "metadata": {},
   "source": [
    "## Criação do modelo BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "1. Bloco transformer com componentes/operacoes Multi-Head Self-Attention, Feed Foward Network e dropout.\n",
    "\n",
    "2. Customização do modelo, definindo como calcular as metricas que serao utilizadas, neste caso calculando a perda com Sparse Categorical Crossentropy e atualizando com a média da perda.\n",
    "\n",
    "3. Construção do modelo BERT para o mlm.\n",
    "\n",
    "4. Callback para a geração de texto para tokens mascarados e apresentação de resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ac4d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_module(query, key, value, i):\n",
    "    \n",
    "    #Fazer uma \"self-attention\" com multiplas cabeças\n",
    "    #self attention e uma maneira do modelo enteder as relacoes de uma sequencia de palavras\n",
    "    #por ter mais de 1 cabeça permite notar essas relacoes em diferentes partes do texto\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=config.NUM_HEAD,\n",
    "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
    "        name=\"encoder_{}_multiheadattention\".format(i),\n",
    "    )(query, key, value)\n",
    "    \n",
    "    #Dropout para ajudar na regularizacao (evitar overfitting)\n",
    "    attention_output = layers.Dropout(0.1, name=\"encoder_{}_att_dropout\".format(i))(\n",
    "        attention_output\n",
    "    )\n",
    "    \n",
    "    #Layer normalization da atencao que foi calculada e da entrada original\n",
    "    #serve para garantir o processo de informacao de maneira consistente, ajudando na performance e eficiencia\n",
    "    attention_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}_att_layernormalization\".format(i)\n",
    "    )(query + attention_output)\n",
    "\n",
    "    # Feed-forward layer, serve para que informacoes em estagios tadios do processamento sejam\n",
    "    #enviados para estagios mais iniciais\n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(config.FF_DIM, activation=\"relu\"),#expande  dimensao para FF_DIMe ativacao ReLU\n",
    "            layers.Dense(config.EMBED_DIM),#volta para a dimensao original\n",
    "        ],\n",
    "        name=\"encoder_{}_ffn\".format(i),\n",
    "    )\n",
    "    \n",
    "    #aplicacao de dropout novamente\n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}_ffn_dropout\".format(i))(\n",
    "        ffn_output\n",
    "    )\n",
    "    \n",
    "    #layer normalization novamente com a saida da camada de atencao\n",
    "    sequence_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}_ffn_layernormalization\".format(i)\n",
    "    )(attention_output + ffn_output)\n",
    "    \n",
    "    #o resultado possui a mesma dimensao da entrada porem com representacoes melhores\n",
    "    #mesmos dados porem enriquecidos de contexto\n",
    "    return sequence_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9af62811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcao de perda de entropia cruzada esparsa\n",
    "#usada quando tem duas ou mais classes de label\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(reduction=None)\n",
    "#media dos valores de perda ao longo do treinamento\n",
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "#esta classe serve para calcular as metricas e a perda de maneira customizada\n",
    "#assim temos um controle maior do treinamento\n",
    "class MaskedLanguageModel(keras.Model):\n",
    "\n",
    "    def compute_loss(self, x=None, y=None, y_pred=None, sample_weight=None):\n",
    "        #ja que na funcao de perda nao tem reducao vamos ter um valor de perda pra cada exemplo\n",
    "        #entao o fazemos com que a soma total seja o que sera otimizada\n",
    "        loss = loss_fn(y, y_pred, sample_weight)\n",
    "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
    "        return keras.ops.sum(loss)\n",
    "\n",
    "    def compute_metrics(self, x, y, y_pred, sample_weight):\n",
    "        # Retorna um dicionario com a perda media do loss_tracker\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        #este serve para que nao seja necessario o reset manual das metricas\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [loss_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83087148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_language_bert_model():\n",
    "    #configura a camada de entrada para que seja uma lista de tokens INT\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=\"int64\")\n",
    "    \n",
    "    #converte os IDs dos tokens em vetores\n",
    "    #cava vetor tera  dimensao de EMBED_DIM\n",
    "    word_embeddings = layers.Embedding(\n",
    "        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n",
    "    )(inputs)\n",
    "    \n",
    "    #cria a informacao adicional das posicoes dos tokens para o modelo usando a bibliote do keras_hub\n",
    "    position_embeddings = keras_hub.layers.PositionEmbedding(\n",
    "        sequence_length=config.MAX_LEN\n",
    "    )(word_embeddings)\n",
    "    \n",
    "    #combina as duas informacoes criadas, a de semantica de a de posicao\n",
    "    embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "    encoder_output = embeddings\n",
    "    \n",
    "    #para cada camada se aplica o bert, um bloco transformer contendo Multi-Head Self-Attention e Feed-Forward Network\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
    "\n",
    "    #camada de classificacao para o mlm, transformando a saída do encoder em predições de tokens\n",
    "    #a função de ativação softmax transforma as saídas brutas da rede neural em um vetor de probabilidades\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
    "        encoder_output\n",
    "    )\n",
    "    \n",
    "    #criar a instancia da classe mlm que ja definimos\n",
    "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
    "\n",
    "    #otimizacao usando \"Adam\" e compilacao\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    mlm_model.compile(optimizer=optimizer)\n",
    "    return mlm_model\n",
    "\n",
    "#mapeamento de ID para token e vice versa para facilitar interpretacao dos dados\n",
    "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
    "token2id = {y: x for x, y in id2token.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874798b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"masked_bert_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"masked_bert_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_embedding      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_embedding  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span> │ word_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionEmbedding</span>) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ word_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ position_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_multihea… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│                     │                   │            │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_att_drop… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_0_multih… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│                     │                   │            │ encoder_0_att_dr… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_att_laye… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_ffn       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ encoder_0_att_la… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_ffn_drop… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_0_ffn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_0_att_la… │\n",
       "│                     │                   │            │ encoder_0_ffn_dr… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_ffn_laye… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlm_cls (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,870,000</span> │ encoder_0_ffn_la… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">30000</span>)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_embedding      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m3,840,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_embedding  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m32,768\u001b[0m │ word_embedding[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mPositionEmbedding\u001b[0m) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ word_embedding[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ position_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_multihea… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m66,048\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│                     │                   │            │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_att_drop… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ encoder_0_multih… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│                     │                   │            │ encoder_0_att_dr… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_att_laye… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_ffn       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m33,024\u001b[0m │ encoder_0_att_la… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_ffn_drop… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ encoder_0_ffn[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ encoder_0_att_la… │\n",
       "│                     │                   │            │ encoder_0_ffn_dr… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_0_ffn_laye… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlm_cls (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m,       │  \u001b[38;5;34m3,870,000\u001b[0m │ encoder_0_ffn_la… │\n",
       "│                     │ \u001b[38;5;34m30000\u001b[0m)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,842,352</span> (29.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,842,352\u001b[0m (29.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,842,352</span> (29.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,842,352\u001b[0m (29.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#por herdar keras.callbacks.Callback pode ser usada durante o treinamento\n",
    "class MaskedTextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, sample_tokens, top_k=5):\n",
    "        self.sample_tokens = sample_tokens #exemplo de entrada\n",
    "        self.k = top_k #quantos candidatos serao considerados para cada mascara\n",
    "\n",
    "    #sequencia de tokens para uma string\n",
    "    def decode(self, tokens):\n",
    "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
    "\n",
    "    #converte o seu ID para seu token correspondente\n",
    "    def convert_ids_to_tokens(self, id):\n",
    "        return id2token[id]\n",
    "\n",
    "    #executado ao final de cada epoca\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        #recebe as previsoes do modelo para o exemplo fornecido\n",
    "        prediction = self.model.predict(self.sample_tokens)\n",
    "\n",
    "        #procura onde esta os tokens de mascara\n",
    "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
    "        masked_index = masked_index[1]\n",
    "        \n",
    "        #pega as probabilidades de predicao para os tokens mascarados\n",
    "        mask_prediction = prediction[0][masked_index]\n",
    "\n",
    "        #ordena e armazena em values os top_k (5 neste caso) maiores probabilidades\n",
    "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
    "        values = mask_prediction[0][top_indices]\n",
    "\n",
    "        #para cada entre os top candidatos cria uma copia e substitui\n",
    "        #depois printa o texto original e os resultados com as palavras preditas junto com a probabilidade e token\n",
    "        for i in range(len(top_indices)):\n",
    "            p = top_indices[i]\n",
    "            v = values[i]\n",
    "            tokens = np.copy(sample_tokens[0])\n",
    "            tokens[masked_index[0]] = p\n",
    "            result = {\n",
    "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
    "                \"prediction\": self.decode(tokens),\n",
    "                \"probability\": v,\n",
    "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
    "            }\n",
    "            pprint(result)\n",
    "\n",
    "#converte o exemplo para uma sequencia de IDs e instacia com o callback com o exemplo, este sera usado nas predicoes no final de cada epoca\n",
    "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
    "generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n",
    "\n",
    "#cria o modelo bert pra o mlm e resume com summary()\n",
    "bert_masked_model = create_masked_language_bert_model()\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99975dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
