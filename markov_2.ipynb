{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Markov\n",
    "\n",
    "**Algoritmo para criação de uma HMM**\n",
    "1. Defina o espaço de estados e o espaço de observações\n",
    "2. Defina a distribuição do estado inicial\n",
    "3. Defina a probabilidade das transições de estado\n",
    "4. Defina a probabilidade de uma observação dada um estado\n",
    "5. Crie o modelo\n",
    "6. Dada uma observação, retorne a sequência mais provável de estados\n",
    "escondidos\n",
    "7. Avalie o modelo"
   ],
   "id": "94b118e4cdca816d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Contexto\n",
    "\n",
    "- Observações: Palavras do vocabulário das letras.\n",
    "- Estados Ocultos: Classes Gramaticais (POS tags).\n",
    "- π (distribuição inicial): Probabilidade de cada classe gramatical iniciar um verso/frase.\n",
    "- A (matriz de transição): Probabilidade de uma classe gramatical seguir outra.\n",
    "- B (matriz de emissão): Probabilidade de uma palavra específica ser gerada por uma determinada classe gramatical."
   ],
   "id": "cb9bde34c72a7456"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Leitura dos Dados\n",
    "\n",
    "A função `get_text_list_from_files` recebe o nome da pasta e retorna os conteúdos dos arquivos como uma lista de strings."
   ],
   "id": "231e8518efc71017"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:53:30.092150Z",
     "start_time": "2025-04-15T12:53:29.631379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def get_text_list_from_files( folder_name: str ) -> list[ str ]:\n",
    "    \"\"\"\n",
    "    Lê o conteúdo de múltiplos arquivos e retorna uma lista de strings,\n",
    "    onde cada string representa o conteúdo de um arquivo.\n",
    "\n",
    "    Args:\n",
    "        folder_name (str): O nome da pasta onde os arquivos de texto estão localizados.\n",
    "                           Espera-se que os arquivos estejam dentro de um subdiretório chamado 'musicas'.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Uma lista onde cada elemento é o conteúdo dos arquivos lidos.\n",
    "    \"\"\"\n",
    "\n",
    "    # Utiliza a biblioteca glob para encontrar todos os arquivos com extensão .txt\n",
    "    # dentro do subdiretório especificado dentro de 'musicas'.\n",
    "    files = glob.glob( f\"musicas/{folder_name}/*.txt\" )\n",
    "\n",
    "    text_list: list[ str ] = [ ]\n",
    "    for file_path in files:\n",
    "        with open( file_path, \"r\", encoding = \"utf-8\" ) as file:\n",
    "            # Lê todo o conteúdo do arquivo de uma vez.\n",
    "            # Se o arquivo contiver várias linhas e você precisar de cada linha como um item separado na lista,\n",
    "            # você pode iterar sobre o objeto 'file' (por exemplo, `for line in file: text_list.append(line.strip())`).\n",
    "            text_list.append( file.read() )\n",
    "    return text_list\n",
    "\n",
    "\n",
    "# Cria o DataSet de treino chamando a função com o nome da pasta 'train'.\n",
    "train_ds = get_text_list_from_files( \"train\" )\n",
    "\n",
    "# Cria o DataFrame de teste/validação chamando a função com o nome da pasta 'test'.\n",
    "test_ds = get_text_list_from_files( \"test\" )\n",
    "\n",
    "# Imprime o tamanho (número de linhas) do DataFrame de treino.\n",
    "print( f\"Tamanho do DataFrame de Treino: {len( train_ds )}\" )\n",
    "\n",
    "# Imprime o tamanho (número de linhas) do DataFrame de teste/validação.\n",
    "print( f\"Tamanho do DataFrame de Teste/Validação: {len( test_ds )}\" )"
   ],
   "id": "66fd2d384c5ced4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do DataFrame de Treino: 1444\n",
      "Tamanho do DataFrame de Teste/Validação: 361\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pré-processamento: Tagging Gramatical (POS Tagging)\n",
    "\n",
    "Nesta seção, preparamos as ferramentas e definimos a função principal para realizar o Part-of-Speech (POS) Tagging nos nossos textos. O POS tagging é uma etapa fundamental em muitos pipelines de Processamento de Linguagem Natural (PLN), onde atribuímos a cada palavra sua classe gramatical (verbo, substantivo, adjetivo, etc.).\n",
    "\n",
    "Utilizaremos a biblioteca `spaCy` com seu modelo pré-treinado para o português (`pt_core_news_lg`), que oferece uma boa performance e detalhes nas tags. O código a seguir carrega o modelo (fazendo o download se necessário) e define a função `tag_sequences` que processará listas de textos."
   ],
   "id": "a0dbded132c1da7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:53:38.214317Z",
     "start_time": "2025-04-15T12:53:30.145309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "\n",
    "# Carregando o modelo português do spaCy\n",
    "try:\n",
    "    nlp = spacy.load( \"pt_core_news_lg\" )\n",
    "except OSError:\n",
    "    print( \"Modelo 'pt_core_news_lg' não encontrado. Baixando...\" )\n",
    "    os.system( \"python -m spacy download pt_core_news_lg\" )\n",
    "    nlp = spacy.load( \"pt_core_news_lg\" )\n",
    "\n",
    "\n",
    "def tag_sequences( text_list: list[ str ] ) -> list[ list[ tuple[ str, str ] ] ]:\n",
    "    \"\"\"\n",
    "    Aplica POS tagging a uma lista de textos (frases/versos).\n",
    "\n",
    "    Args:\n",
    "        text_list (list[str]): Lista de strings (frases ou versos).\n",
    "\n",
    "    Returns:\n",
    "        list[list[tuple[str, str]]]: Uma lista de sequências taggeadas.\n",
    "                                     Cada sequência é uma lista de tuplas (palavra, tag).\n",
    "    \"\"\"\n",
    "    tagged_sequences = [ ]\n",
    "    # Processa os textos em lote para eficiência\n",
    "    # A limpeza agressiva de pontuação deve vir DEPOIS ou ser feita pelo tagger\n",
    "    docs = nlp.pipe( text_list )  # Processa a lista de textos\n",
    "\n",
    "    for doc in docs:\n",
    "        sequence = [ ]\n",
    "        for token in doc:\n",
    "            # token.text: a palavra/token original\n",
    "            # token.pos_: a classe gramatical Universal Dependencies (mais simples, ex: NOUN, VERB, ADJ)\n",
    "            if not token.is_punct and not token.is_space:  # Opcional: ignorar pontuação e espaços aqui\n",
    "                sequence.append( (token.text.lower(), token.pos_) )  # Armazena palavra (minúscula) e tag\n",
    "        if sequence:  # Adiciona apenas se a sequência não estiver vazia\n",
    "            tagged_sequences.append( sequence )\n",
    "    return tagged_sequences\n"
   ],
   "id": "753eb19469998f60",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:54:14.321034Z",
     "start_time": "2025-04-15T12:53:38.592858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tagged_train_sequences = tag_sequences( train_ds )\n",
    "tagged_test_sequences = tag_sequences( test_ds )"
   ],
   "id": "4138ba7f4f012b2c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preparação para Modelagem: Vocabulário e Estados\n",
    "\n",
    "Com as sequências devidamente taggeadas (armazenadas em `tagged_train_sequences`), o próximo passo é preparar esses dados para serem utilizados em modelos que requerem representação numérica. Para isso, precisamos:\n",
    "\n",
    "1.  **Identificar o Vocabulário:** Extrair todas as palavras únicas presentes no corpus de treino.\n",
    "2.  **Identificar os Estados:** Extrair todas as tags gramaticais (POS tags) únicas, que representarão os \"estados\" em modelos como HMMs.\n",
    "3.  **Criar Mapeamentos para Índices:** Construir dicionários que associam cada palavra única e cada tag única a um índice numérico inteiro. Isso é essencial para criar matrizes de probabilidade ou vetores de entrada para redes neurais.\n",
    "\n",
    "O código a seguir realiza essas tarefas: percorre as sequências taggeadas, utiliza `set` para coletar itens únicos, ordena as listas resultantes para garantir consistência (`vocabulary`, `states`), e cria os dicionários de mapeamento (`word_to_index`, `tag_to_index`). Por fim, exibe o tamanho do vocabulário e o número de estados encontrados."
   ],
   "id": "ec2792ce4b3b2675"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:54:14.416034Z",
     "start_time": "2025-04-15T12:54:14.335036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "all_words = set()\n",
    "all_tags = set()\n",
    "\n",
    "for sequence in tagged_train_sequences:\n",
    "    for word, tag in sequence:\n",
    "        all_words.add( word )  # Adiciona palavra ao set\n",
    "        all_tags.add( tag )  # Adiciona tag ao set\n",
    "\n",
    "# Ordenar para ter índices consistentes\n",
    "vocabulary = sorted( list( all_words ) )\n",
    "states = sorted( list( all_tags ) )\n",
    "\n",
    "# Criar mapeamentos para índices (essencial para matrizes numpy)\n",
    "word_to_index = { word: i for i, word in enumerate( vocabulary ) }\n",
    "tag_to_index = { tag: i for i, tag in enumerate( states ) }\n",
    "\n",
    "n_vocab = len( vocabulary )\n",
    "n_states = len( states )\n",
    "\n",
    "print( f\"Tamanho do vocabulário: {n_vocab}\" )\n",
    "print( f\"Número de estados (tags) únicos: {n_states}\" )\n",
    "\n",
    "# print(\"Estados:\", states)\n",
    "# print(\"Vocabulário (primeiros 50):\", vocabulary[:50])"
   ],
   "id": "b835c44796d69e8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário: 11162\n",
      "Número de estados (tags) únicos: 16\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cálculo do Vetor π (Probabilidades Iniciais)\n",
    "\n",
    "Este bloco de código calcula o vetor de probabilidades iniciais para um modelo de sequência, onde cada elemento de `pi` representa a chance de uma tag ser a primeira em uma sequência.\n",
    "\n",
    "- **Contagem:**\n",
    "  Conta quantas vezes cada tag aparece na posição inicial de cada sequência usando `Counter`.\n",
    "\n",
    "- **Inicialização:**\n",
    "  Cria um vetor `pi` com zeros e atribui as contagens correspondentes às tags usando um mapeamento (`tag_to_index`).\n",
    "\n",
    "- **Normalização:**\n",
    "  Converte as contagens em probabilidades dividindo pelo número total de sequências. Caso não haja sequências, utiliza uma distribuição uniforme.\n"
   ],
   "id": "fe5d1fb043547b39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:54:14.478064Z",
     "start_time": "2025-04-15T12:54:14.449034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "initial_tag_counts = Counter()\n",
    "total_sequences = len( tagged_train_sequences )\n",
    "\n",
    "for sequence in tagged_train_sequences:\n",
    "    if sequence:  # Verifica se a sequência não está vazia\n",
    "        first_tag = sequence[ 0 ][ 1 ]  # Pega a tag da primeira tupla (palavra, tag)\n",
    "        initial_tag_counts[ first_tag ] += 1\n",
    "\n",
    "# Inicializa o vetor pi com zeros\n",
    "pi = np.zeros( n_states )\n",
    "for tag, count in initial_tag_counts.items():\n",
    "    if tag in tag_to_index:  # Garante que a tag está no nosso conjunto de estados\n",
    "        pi[ tag_to_index[ tag ] ] = count\n",
    "\n",
    "# Normalizar para obter probabilidades\n",
    "if total_sequences > 0:\n",
    "    pi = pi / total_sequences\n",
    "else:\n",
    "    # Caso não haja sequências, distribui uniformemente (ou define como zero)\n",
    "    pi = np.ones( n_states ) / n_states\n",
    "\n",
    "print( f\"Vetor Pi (Prob. Iniciais) calculado. Shape: {pi.shape}\" )"
   ],
   "id": "339b4152020b11e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vetor Pi (Prob. Iniciais) calculado. Shape: (16,)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cálculo da Matriz A de Transição\n",
    "\n",
    "Este código constrói a matriz de probabilidades de transição \\( A \\), onde cada elemento \\( A[i, j] \\) representa a probabilidade de transitar da tag \\( i \\) para a tag \\( j \\).\n",
    "\n",
    "- **Suavização de Laplace:**\n",
    "  Inicializamos as contagens com um valor pequeno (`alpha_smooth = 0.1`) para evitar probabilidades zero em transições não vistas.\n",
    "\n",
    "- **Contagem de Transições:**\n",
    "  Para cada sequência de treinamento, o código incrementa as contagens de transição entre tags consecutivas e ajusta as contagens de origens.\n",
    "\n",
    "- **Normalização:**\n",
    "  As contagens de transições são normalizadas pela soma total de transições originadas em cada tag, resultando na matriz \\( A \\) com cada linha representando uma distribuição de probabilidade que soma 1.\n"
   ],
   "id": "90cec5f31a72863"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:54:14.822982Z",
     "start_time": "2025-04-15T12:54:14.559034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Matriz A[i, j] = P(tag_j | tag_i)\n",
    "# Usaremos contagens e depois normalizaremos\n",
    "\n",
    "# Adicionar suavização de Laplace (add-alpha smoothing) é recomendado\n",
    "# para evitar probabilidades zero para transições não vistas.\n",
    "alpha_smooth = 0.1  # Um pequeno valor de suavização\n",
    "\n",
    "# Contagem de transições (tag_i -> tag_j)\n",
    "# Inicializa com alpha para suavização\n",
    "transition_counts = np.full( (n_states, n_states), alpha_smooth )\n",
    "\n",
    "# Contagem total de vezes que cada tag_i origina uma transição\n",
    "# Inicializa com alpha * n_states para consistência na normalização com suavização\n",
    "tag_origin_counts = np.full( n_states, alpha_smooth * n_states )\n",
    "\n",
    "for sequence in tagged_train_sequences:\n",
    "    for i in range( len( sequence ) - 1 ):  # Itera até a penúltima tupla\n",
    "        current_tag = sequence[ i ][ 1 ]\n",
    "        next_tag = sequence[ i + 1 ][ 1 ]\n",
    "\n",
    "        if current_tag in tag_to_index and next_tag in tag_to_index:\n",
    "            idx_current = tag_to_index[ current_tag ]\n",
    "            idx_next = tag_to_index[ next_tag ]\n",
    "\n",
    "            transition_counts[ idx_current, idx_next ] += 1\n",
    "            tag_origin_counts[ idx_current ] += 1  # Incrementa a contagem de origem\n",
    "\n",
    "# Normalizar para obter a matriz A de probabilidades\n",
    "# A[i, j] = count(tag_i -> tag_j) / count(tag_i como origem)\n",
    "A = transition_counts / tag_origin_counts[ :, np.newaxis ]  # Divide cada linha pelo total de origem correspondente\n",
    "\n",
    "# Verifica se as linhas somam 1 (ou muito próximo devido a float precision)\n",
    "# print(\"Soma das linhas da Matriz A (deve ser próximo de 1):\", np.sum(A, axis=1))\n",
    "print( f\"Matriz A (Transição) calculada. Shape: {A.shape}\" )"
   ],
   "id": "b50f5937285e7a95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz A (Transição) calculada. Shape: (16, 16)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cálculo da Matriz B de Emissão\n",
    "\n",
    "Este bloco constrói a matriz \\(B\\), onde cada elemento \\(B[j, k]\\) representa a probabilidade de uma tag \\(j\\) emitir uma palavra \\(k\\).\n",
    "\n",
    "- **Suavização de Laplace:**\n",
    "  Inicializa as contagens com um valor pequeno para lidar com emissões não observadas.\n",
    "\n",
    "- **Contagem de Emissões:**\n",
    "  Para cada par (palavra, tag) na sequência de treinamento, incrementa as contagens para a emissão da palavra pela tag correspondente.\n",
    "\n",
    "- **Normalização:**\n",
    "  Cada linha da matriz \\(B\\) é normalizada dividindo pelas contagens totais de cada tag, gerando distribuições de probabilidade para as emissões."
   ],
   "id": "482c6e4cd6e197d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:54:15.123240Z",
     "start_time": "2025-04-15T12:54:14.832979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Matriz B[j, k] = P(palavra_k | tag_j)\n",
    "# Usaremos contagens e depois normalizaremos\n",
    "\n",
    "# Contagem de emissões (tag_j -> palavra_k)\n",
    "# Inicializa com alpha para suavização\n",
    "emission_counts = np.full( (n_states, n_vocab), alpha_smooth )\n",
    "\n",
    "# Contagem total de vezes que cada tag_j aparece\n",
    "# Inicializa com alpha * n_vocab para consistência\n",
    "tag_total_counts = np.full( n_states, alpha_smooth * n_vocab )\n",
    "\n",
    "for sequence in tagged_train_sequences:\n",
    "    for word, tag in sequence:\n",
    "        if tag in tag_to_index and word in word_to_index:\n",
    "            idx_tag = tag_to_index[ tag ]\n",
    "            idx_word = word_to_index[ word ]\n",
    "\n",
    "            emission_counts[ idx_tag, idx_word ] += 1\n",
    "            tag_total_counts[ idx_tag ] += 1  # Incrementa a contagem total da tag\n",
    "\n",
    "# Normalizar para obter a matriz B de probabilidades\n",
    "# B[j, k] = count(tag_j emitindo palavra_k) / count(total de tag_j)\n",
    "B = emission_counts / tag_total_counts[ :, np.newaxis ]  # Divide cada linha pelo total da tag correspondente\n",
    "\n",
    "# Verifica se as linhas somam 1 (ou muito próximo)\n",
    "# print(\"Soma das linhas da Matriz B (deve ser próximo de 1):\", np.sum(B, axis=1))\n",
    "print( f\"Matriz B (Emissão) calculada. Shape: {B.shape}\" )"
   ],
   "id": "6b74151c4a2a9ff3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz B (Emissão) calculada. Shape: (16, 11162)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Mapeamentos Inversos\n",
    "\n",
    "Para podermos interpretar os resultados (converter índices de volta para palavras e tags)."
   ],
   "id": "d1291b1eb1360651"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:54:15.185658Z",
     "start_time": "2025-04-15T12:54:15.173684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assumindo que você já tem word_to_index e tag_to_index\n",
    "\n",
    "index_to_word = { index: word for word, index in word_to_index.items() }\n",
    "index_to_tag = { index: tag for tag, index in tag_to_index.items() }\n",
    "\n",
    "print( \"Mapeamentos inversos criados.\" )"
   ],
   "id": "ae136034b72567e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapeamentos inversos criados.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Algoritmo de Viterbi\n",
    "\n",
    "Esta célula implementa o algoritmo de Viterbi para encontrar a sequência de tags mais provável para uma sequência de palavras, usando os parâmetros do HMM.\n",
    "\n",
    "**Etapas:**\n",
    "- **Inicialização:** Calcula as probabilidades para a primeira palavra, tratando palavras desconhecidas (OOV) com baixa probabilidade.\n",
    "- **Recursão:** Atualiza as probabilidades acumuladas e mantém os \"backpointers\" para cada tag em cada posição.\n",
    "- **Terminação e Backtracking:** Seleciona a última tag com a maior probabilidade e reconstrói a sequência ótima de tags.\n",
    "\n",
    "O resultado é a lista de tags associadas à sequência de observações.\n"
   ],
   "id": "7954759edfa36700"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:54:15.294303Z",
     "start_time": "2025-04-15T12:54:15.280303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def viterbi( obs_sequence: list[ str ],\n",
    "             pi: np.ndarray,\n",
    "             A: np.ndarray,\n",
    "             B: np.ndarray,\n",
    "             word_to_index: dict,\n",
    "             tag_to_index: dict,\n",
    "             index_to_tag: dict\n",
    "             ) -> list[ str ]:\n",
    "    \"\"\"\n",
    "    Implementa o Algoritmo de Viterbi para encontrar a sequência de estados (tags)\n",
    "    mais provável para uma dada sequência de observações (palavras).\n",
    "\n",
    "    Args:\n",
    "        obs_sequence (list[str]): A sequência de palavras observadas.\n",
    "        pi (np.ndarray): Vetor de probabilidades iniciais dos estados (shape: n_states).\n",
    "        A (np.ndarray): Matriz de transição de estados (shape: n_states x n_states).\n",
    "                           A[i, j] = P(estado_j | estado_i).\n",
    "        B (np.ndarray): Matriz de emissão (shape: n_states x n_vocab).\n",
    "                           B[j, k] = P(palavra_k | estado_j).\n",
    "        word_to_index (dict): Mapeamento de palavra para índice no vocabulário.\n",
    "        tag_to_index (dict): Mapeamento de tag (estado) para índice de estado.\n",
    "        index_to_tag (dict): Mapeamento de índice de estado para tag.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A sequência de tags mais provável.\n",
    "                   Retorna lista vazia se a sequência de observação for vazia.\n",
    "    \"\"\"\n",
    "    n_states = A.shape[ 0 ]\n",
    "    n_obs = len( obs_sequence )\n",
    "    n_vocab = B.shape[ 1 ]  # Necessário para lidar com OOV\n",
    "\n",
    "    if n_obs == 0:\n",
    "        return [ ]\n",
    "\n",
    "    # Adicionar um valor pequeno para evitar log(0) -> -inf\n",
    "    epsilon = 1e-10\n",
    "    log_pi = np.log( pi + epsilon )\n",
    "    log_A = np.log( A + epsilon )\n",
    "    log_B = np.log( B + epsilon )\n",
    "\n",
    "    # Matriz T1 (viterbi_prob): Guarda a probabilidade máxima (em log) do caminho\n",
    "    # que termina no estado j no tempo t. Shape: (n_states, n_obs)\n",
    "    viterbi_prob = np.zeros( (n_states, n_obs) )\n",
    "\n",
    "    # Matriz T2 (backpointer): Guarda o índice do estado anterior (no tempo t-1)\n",
    "    # que levou à probabilidade máxima em T1[j, t]. Shape: (n_states, n_obs)\n",
    "    backpointer = np.zeros( (n_states, n_obs), dtype = int )\n",
    "\n",
    "    first_word = obs_sequence[ 0 ]\n",
    "    if first_word in word_to_index:\n",
    "        first_word_idx = word_to_index[ first_word ]\n",
    "        # Probabilidade de emissão da primeira palavra para cada estado\n",
    "        log_emission_prob_t0 = log_B[ :, first_word_idx ]\n",
    "    else:\n",
    "        # Palavra fora do vocabulário (OOV) - Atribui uma baixa prob. uniforme\n",
    "        # print( f\"Aviso: Palavra '{first_word}' não está no vocabulário de treino. Usando baixa probabilidade.\" )\n",
    "        log_emission_prob_t0 = np.full( n_states, np.log( epsilon / n_states ) )  # Distribui epsilon\n",
    "\n",
    "    # Probabilidade inicial = prob inicial do estado + prob de emitir a 1ª palavra\n",
    "    viterbi_prob[ :, 0 ] = log_pi + log_emission_prob_t0\n",
    "    backpointer[ :, 0 ] = 0  # Não há estado anterior\n",
    "\n",
    "    for t in range( 1, n_obs ):\n",
    "        word = obs_sequence[ t ]\n",
    "        if word in word_to_index:\n",
    "            word_idx = word_to_index[ word ]\n",
    "            log_emission_prob_t = log_B[ :, word_idx ]\n",
    "        else:\n",
    "            # Lidar com OOV para palavras subsequentes\n",
    "            # print( f\"Aviso: Palavra '{word}' não está no vocabulário de treino. Usando baixa probabilidade.\" )\n",
    "            log_emission_prob_t = np.full( n_states, np.log( epsilon / n_states ) )\n",
    "\n",
    "        for j in range( n_states ):  # Para cada estado atual j no tempo t\n",
    "            # Calcula a probabilidade de chegar ao estado j vindo de *cada* estado i no tempo t-1\n",
    "            # prob = prob_max(t-1)_i + prob_trans(i -> j) + prob_emiss(palavra_t | estado_j)\n",
    "            # Note que log_emission_prob_t é o mesmo para todos os 'i' ao calcular para o estado 'j'\n",
    "            probs_via_prev_state = viterbi_prob[ :, t - 1 ] + log_A[ :, j ] + log_emission_prob_t[ j ]\n",
    "\n",
    "            # Encontra a probabilidade máxima e o estado anterior que a gerou\n",
    "            viterbi_prob[ j, t ] = np.max( probs_via_prev_state )\n",
    "            backpointer[ j, t ] = np.argmax( probs_via_prev_state )\n",
    "\n",
    "    # Encontra o índice do estado final com a maior probabilidade\n",
    "    last_state_idx = np.argmax( viterbi_prob[ :, n_obs - 1 ] )\n",
    "    # max_log_prob = viterbi_prob[last_state_idx, n_obs - 1] # Se precisar da prob. total\n",
    "\n",
    "    # Reconstrói o caminho ótimo de trás para frente\n",
    "    best_path_indices = [ 0 ] * n_obs\n",
    "    best_path_indices[ n_obs - 1 ] = last_state_idx\n",
    "\n",
    "    for t in range( n_obs - 2, -1, -1 ):  # Itera de n_obs-2 até 0\n",
    "        # O estado no tempo t é dado pelo backpointer do estado escolhido no tempo t+1\n",
    "        best_path_indices[ t ] = backpointer[ best_path_indices[ t + 1 ], t + 1 ]\n",
    "\n",
    "    # Converte os índices do caminho de volta para os nomes das tags\n",
    "    best_path_tags = [ index_to_tag[ idx ] for idx in best_path_indices ]\n",
    "\n",
    "    return best_path_tags"
   ],
   "id": "d3f3ddb114ae9b2f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Avaliação do Modelo HMM no Conjunto de Teste\n",
    "\n",
    "Agora que temos o modelo treinado (π, A, B) e o algoritmo de Viterbi para fazer predições, podemos avaliar o desempenho do nosso HMM de POS tagging.\n",
    "\n",
    "Utilizaremos o conjunto de dados `tagged_test_sequences`, que não foi usado durante o treinamento das matrizes de probabilidade. Para cada sequência no conjunto de teste, faremos o seguinte:\n",
    "1. Extrair a sequência de palavras (observações).\n",
    "2. Extrair a sequência de tags reais (ground truth).\n",
    "3. Usar a função `viterbi` para prever as tags com base nas palavras observadas.\n",
    "4. Comparar as tags preditas com as tags reais e contar o número de acertos.\n",
    "\n",
    "Ao final, calcularemos a acurácia geral do modelo, que é a proporção de tags corretamente identificadas em relação ao número total de tags no conjunto de teste."
   ],
   "id": "b2e8bf214dd2961f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:54:26.561436Z",
     "start_time": "2025-04-15T12:54:15.360302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inicializa contadores para a avaliação\n",
    "total_tags_evaluated = 0\n",
    "correct_tags_predicted = 0\n",
    "\n",
    "print( f\"Iniciando avaliação em {len( tagged_test_sequences )} sequências de teste...\" )\n",
    "\n",
    "# Itera sobre cada sequência no conjunto de teste\n",
    "for i, sequence_tuples in enumerate( tagged_test_sequences ):\n",
    "    if not sequence_tuples:  # Pula sequências vazias, se houver\n",
    "        continue\n",
    "\n",
    "    # Extrai as palavras (observações) e as tags reais\n",
    "    obs_sequence = [ word for word, tag in sequence_tuples ]\n",
    "    actual_tags = [ tag for word, tag in sequence_tuples ]\n",
    "\n",
    "    # Usa o Viterbi para prever as tags\n",
    "    predicted_tags = viterbi(\n",
    "            obs_sequence = obs_sequence,\n",
    "            pi = pi,\n",
    "            A = A,\n",
    "            B = B,\n",
    "            word_to_index = word_to_index,\n",
    "            tag_to_index = tag_to_index,\n",
    "            index_to_tag = index_to_tag\n",
    "    )\n",
    "\n",
    "    # Verifica se o Viterbi retornou uma sequência com o mesmo comprimento\n",
    "    if len( predicted_tags ) == len( actual_tags ):\n",
    "        # Compara as tags preditas com as reais\n",
    "        for predicted, actual in zip( predicted_tags, actual_tags ):\n",
    "            if predicted == actual:\n",
    "                correct_tags_predicted += 1\n",
    "        total_tags_evaluated += len( actual_tags )  # Adiciona o número de tags nesta sequência ao total\n",
    "    else:\n",
    "        # Se os comprimentos diferirem, algo deu errado no Viterbi para esta sequência.\n",
    "        # É bom registrar isso, mas para a acurácia, podemos apenas contar as tags reais no total\n",
    "        # e não adicionar acertos.\n",
    "        print( f\"AVISO: Comprimento da sequência {i} difere! \"\n",
    "               f\"Observações: {len( obs_sequence )}, Reais: {len( actual_tags )}, Preditas: {len( predicted_tags )}\"\n",
    "               )\n",
    "        # Mesmo com erro, contabilizamos as tags que deveriam ter sido avaliadas.\n",
    "        total_tags_evaluated += len( actual_tags )\n",
    "\n",
    "print( \"\\nAvaliação concluída.\" )\n",
    "\n",
    "# Calcula a acurácia final\n",
    "if total_tags_evaluated > 0:\n",
    "    accuracy = correct_tags_predicted / total_tags_evaluated\n",
    "    print( \"\\n--- Resultados da Avaliação do Modelo HMM ---\" )\n",
    "    print( f\"Total de Tags Avaliadas no Conjunto de Teste: {total_tags_evaluated}\" )\n",
    "    print( f\"Total de Tags Preditas Corretamente: {correct_tags_predicted}\" )\n",
    "    print( f\"Acurácia Geral do POS Tagging: {accuracy:.2%}\" )  # Exibe como porcentagem\n",
    "else:\n",
    "    print( \"\\nNão foi possível calcular a acurácia (nenhuma tag avaliada). \"\n",
    "           \"Verifique o conjunto de teste.\"\n",
    "           )"
   ],
   "id": "b8a046c030263c05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando avaliação em 361 sequências de teste...\n",
      "\n",
      "Avaliação concluída.\n",
      "\n",
      "--- Resultados da Avaliação do Modelo HMM ---\n",
      "Total de Tags Avaliadas no Conjunto de Teste: 56441\n",
      "Total de Tags Preditas Corretamente: 49881\n",
      "Acurácia Geral do POS Tagging: 88.38%\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
