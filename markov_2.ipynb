{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Markov\n",
    "\n",
    "**Algoritmo para criação de uma HMM**\n",
    "1. Defina o espaço de estados e o espaço de observações\n",
    "2. Defina a distribuição do estado inicial\n",
    "3. Defina a probabilidade das transições de estado\n",
    "4. Defina a probabilidade de uma observação dada um estado\n",
    "5. Crie o modelo\n",
    "6. Dada uma observação, retorne a sequência mais provável de estados\n",
    "escondidos\n",
    "7. Avalie o modelo"
   ],
   "id": "94b118e4cdca816d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Contexto\n",
    "\n",
    "- Observações: Palavras do vocabulário das letras.\n",
    "- Estados Ocultos: Classes Gramaticais (POS tags).\n",
    "- π (distribuição inicial): Probabilidade de cada classe gramatical iniciar um verso/frase.\n",
    "- A (matriz de transição): Probabilidade de uma classe gramatical seguir outra.\n",
    "- B (matriz de emissão): Probabilidade de uma palavra específica ser gerada por uma determinada classe gramatical."
   ],
   "id": "cb9bde34c72a7456"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Leitura dos Dados\n",
    "\n",
    "A função `get_text_list_from_files` recebe o nome da pasta e retorna os conteúdos dos arquivos como uma lista de strings."
   ],
   "id": "231e8518efc71017"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T19:13:30.353914Z",
     "start_time": "2025-04-14T19:13:29.951645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def get_text_list_from_files( folder_name: str ) -> list[ str ]:\n",
    "    \"\"\"\n",
    "    Lê o conteúdo de múltiplos arquivos e retorna uma lista de strings,\n",
    "    onde cada string representa o conteúdo de um arquivo.\n",
    "\n",
    "    Args:\n",
    "        folder_name (str): O nome da pasta onde os arquivos de texto estão localizados.\n",
    "                           Espera-se que os arquivos estejam dentro de um subdiretório chamado 'musicas'.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Uma lista onde cada elemento é o conteúdo dos arquivos lidos.\n",
    "    \"\"\"\n",
    "\n",
    "    # Utiliza a biblioteca glob para encontrar todos os arquivos com extensão .txt\n",
    "    # dentro do subdiretório especificado dentro de 'musicas'.\n",
    "    files = glob.glob( f\"musicas/{folder_name}/*.txt\" )\n",
    "\n",
    "    text_list: list[ str ] = [ ]\n",
    "    for file_path in files:\n",
    "        with open( file_path, \"r\", encoding = \"utf-8\" ) as file:\n",
    "            # Lê todo o conteúdo do arquivo de uma vez.\n",
    "            # Se o arquivo contiver várias linhas e você precisar de cada linha como um item separado na lista,\n",
    "            # você pode iterar sobre o objeto 'file' (por exemplo, `for line in file: text_list.append(line.strip())`).\n",
    "            text_list.append( file.read() )\n",
    "    return text_list\n",
    "\n",
    "\n",
    "# Cria o DataSet de treino chamando a função com o nome da pasta 'train'.\n",
    "train_ds = get_text_list_from_files( \"train\" )\n",
    "\n",
    "# Cria o DataFrame de teste/validação chamando a função com o nome da pasta 'test'.\n",
    "test_ds = get_text_list_from_files( \"test\" )\n",
    "\n",
    "# Imprime o tamanho (número de linhas) do DataFrame de treino.\n",
    "print( f\"Tamanho do DataFrame de Treino: {len( train_ds )}\" )\n",
    "\n",
    "# Imprime o tamanho (número de linhas) do DataFrame de teste/validação.\n",
    "print( f\"Tamanho do DataFrame de Teste/Validação: {len( test_ds )}\" )"
   ],
   "id": "66fd2d384c5ced4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do DataFrame de Treino: 1444\n",
      "Tamanho do DataFrame de Teste/Validação: 361\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pré-processamento dos Dados",
   "id": "a0dbded132c1da7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T19:13:32.167252Z",
     "start_time": "2025-04-14T19:13:30.361954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "\n",
    "# Carregando o modelo português do spaCy\n",
    "try:\n",
    "    nlp = spacy.load( \"pt_core_news_lg\" )\n",
    "except OSError:\n",
    "    print( \"Modelo 'pt_core_news_lg' não encontrado. Baixando...\" )\n",
    "    os.system( \"python -m spacy download pt_core_news_lg\" )\n",
    "    nlp = spacy.load( \"pt_core_news_lg\" )\n",
    "\n",
    "\n",
    "def tag_sequences( text_list: list[ str ] ) -> list[ list[ tuple[ str, str ] ] ]:\n",
    "    \"\"\"\n",
    "    Aplica POS tagging a uma lista de textos (frases/versos).\n",
    "\n",
    "    Args:\n",
    "        text_list (list[str]): Lista de strings (frases ou versos).\n",
    "\n",
    "    Returns:\n",
    "        list[list[tuple[str, str]]]: Uma lista de sequências taggeadas.\n",
    "                                     Cada sequência é uma lista de tuplas (palavra, tag).\n",
    "    \"\"\"\n",
    "    tagged_sequences = [ ]\n",
    "    # Processa os textos em lote para eficiência\n",
    "    # A limpeza agressiva de pontuação deve vir DEPOIS ou ser feita pelo tagger\n",
    "    docs = nlp.pipe( text_list )  # Processa a lista de textos\n",
    "\n",
    "    for doc in docs:\n",
    "        sequence = [ ]\n",
    "        for token in doc:\n",
    "            # token.text: a palavra/token original\n",
    "            # token.pos_: a classe gramatical Universal Dependencies (mais simples, ex: NOUN, VERB, ADJ)\n",
    "            # token.tag_: a classe gramatical mais detalhada, específica do corpus (ex: N|MAS|SG)\n",
    "            # Vamos usar token.pos_ para simplificar\n",
    "            if not token.is_punct and not token.is_space:  # Opcional: ignorar pontuação e espaços aqui\n",
    "                sequence.append( (token.text.lower(), token.pos_) )  # Armazena palavra (minúscula) e tag\n",
    "        if sequence:  # Adiciona apenas se a sequência não estiver vazia\n",
    "            tagged_sequences.append( sequence )\n",
    "    return tagged_sequences\n"
   ],
   "id": "753eb19469998f60",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T19:14:07.818985Z",
     "start_time": "2025-04-14T19:13:32.208925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_ds = [ text.lower() for text in train_ds ]\n",
    "test_ds = [ text.lower() for text in test_ds ]\n",
    "\n",
    "tagged_train_sequences = tag_sequences( train_ds )\n",
    "tagged_test_sequences = tag_sequences( test_ds )"
   ],
   "id": "4138ba7f4f012b2c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Construção do vocabulários, estados e mapeamento",
   "id": "ec2792ce4b3b2675"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T19:14:07.959376Z",
     "start_time": "2025-04-14T19:14:07.860262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "all_words = set()\n",
    "all_tags = set()\n",
    "\n",
    "for sequence in tagged_train_sequences:\n",
    "    for word, tag in sequence:\n",
    "        all_words.add( word )  # Adiciona palavra ao set\n",
    "        all_tags.add( tag )  # Adiciona tag ao set\n",
    "\n",
    "# Ordenar para ter índices consistentes\n",
    "vocabulary = sorted( list( all_words ) )\n",
    "states = sorted( list( all_tags ) )\n",
    "\n",
    "# Criar mapeamentos para índices (essencial para matrizes numpy)\n",
    "word_to_index = { word: i for i, word in enumerate( vocabulary ) }\n",
    "tag_to_index = { tag: i for i, tag in enumerate( states ) }\n",
    "\n",
    "n_states = len( states )\n",
    "n_vocab = len( vocabulary )\n",
    "\n",
    "print( f\"Número de estados (tags) únicos: {n_states}\" )\n",
    "print( f\"Tamanho do vocabulário: {n_vocab}\" )\n",
    "\n",
    "# print(\"Estados:\", states)\n",
    "# print(\"Vocabulário (primeiros 50):\", vocabulary[:50])"
   ],
   "id": "b835c44796d69e8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de estados (tags) únicos: 16\n",
      "Tamanho do vocabulário: 11162\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T19:14:08.008389Z",
     "start_time": "2025-04-14T19:14:07.991983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 2. Calcular Probabilidades Iniciais (π) ---\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "initial_tag_counts = Counter()\n",
    "total_sequences = len( tagged_train_sequences )\n",
    "\n",
    "for sequence in tagged_train_sequences:\n",
    "    if sequence:  # Verifica se a sequência não está vazia\n",
    "        first_tag = sequence[ 0 ][ 1 ]  # Pega a tag da primeira tupla (palavra, tag)\n",
    "        initial_tag_counts[ first_tag ] += 1\n",
    "\n",
    "# Inicializa o vetor pi com zeros (ou uma pequena probabilidade para suavização - veja nota)\n",
    "pi = np.zeros( n_states )\n",
    "for tag, count in initial_tag_counts.items():\n",
    "    if tag in tag_to_index:  # Garante que a tag está no nosso conjunto de estados\n",
    "        pi[ tag_to_index[ tag ] ] = count\n",
    "\n",
    "# Normalizar para obter probabilidades\n",
    "if total_sequences > 0:\n",
    "    pi = pi / total_sequences\n",
    "else:\n",
    "    # Caso não haja sequências, distribui uniformemente (ou define como zero)\n",
    "    pi = np.ones( n_states ) / n_states\n",
    "\n",
    "print( f\"Vetor Pi (Prob. Iniciais) calculado. Shape: {pi.shape}\" )"
   ],
   "id": "339b4152020b11e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vetor Pi (Prob. Iniciais) calculado. Shape: (16,)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T19:14:08.343552Z",
     "start_time": "2025-04-14T19:14:08.040965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3. Calcular Matriz de Transição (A) ---\n",
    "\n",
    "# Matriz A[i, j] = P(tag_j | tag_i)\n",
    "# Usaremos contagens e depois normalizaremos\n",
    "\n",
    "# Adicionar suavização de Laplace (add-alpha smoothing) é recomendado\n",
    "# para evitar probabilidades zero para transições não vistas.\n",
    "alpha_smooth = 0.1  # Um pequeno valor de suavização\n",
    "\n",
    "# Contagem de transições (tag_i -> tag_j)\n",
    "# Inicializa com alpha para suavização\n",
    "transition_counts = np.full( (n_states, n_states), alpha_smooth )\n",
    "\n",
    "# Contagem total de vezes que cada tag_i origina uma transição\n",
    "# Inicializa com alpha * n_states para consistência na normalização com suavização\n",
    "tag_origin_counts = np.full( n_states, alpha_smooth * n_states )\n",
    "\n",
    "for sequence in tagged_train_sequences:\n",
    "    for i in range( len( sequence ) - 1 ):  # Itera até a penúltima tupla\n",
    "        current_tag = sequence[ i ][ 1 ]\n",
    "        next_tag = sequence[ i + 1 ][ 1 ]\n",
    "\n",
    "        if current_tag in tag_to_index and next_tag in tag_to_index:\n",
    "            idx_current = tag_to_index[ current_tag ]\n",
    "            idx_next = tag_to_index[ next_tag ]\n",
    "\n",
    "            transition_counts[ idx_current, idx_next ] += 1\n",
    "            tag_origin_counts[ idx_current ] += 1  # Incrementa a contagem de origem\n",
    "\n",
    "# Normalizar para obter a matriz A de probabilidades\n",
    "# A[i, j] = count(tag_i -> tag_j) / count(tag_i como origem)\n",
    "A = transition_counts / tag_origin_counts[ :, np.newaxis ]  # Divide cada linha pelo total de origem correspondente\n",
    "\n",
    "# Verifica se as linhas somam 1 (ou muito próximo devido a float precision)\n",
    "# print(\"Soma das linhas da Matriz A (deve ser próximo de 1):\", np.sum(A, axis=1))\n",
    "print( f\"Matriz A (Transição) calculada. Shape: {A.shape}\" )"
   ],
   "id": "b50f5937285e7a95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz A (Transição) calculada. Shape: (16, 16)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T19:14:08.611316Z",
     "start_time": "2025-04-14T19:14:08.376725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 4. Calcular Matriz de Emissão (B) ---\n",
    "\n",
    "# Matriz B[j, k] = P(palavra_k | tag_j)\n",
    "# Usaremos contagens e depois normalizaremos\n",
    "\n",
    "# Contagem de emissões (tag_j -> palavra_k)\n",
    "# Inicializa com alpha para suavização\n",
    "emission_counts = np.full( (n_states, n_vocab), alpha_smooth )\n",
    "\n",
    "# Contagem total de vezes que cada tag_j aparece\n",
    "# Inicializa com alpha * n_vocab para consistência\n",
    "tag_total_counts = np.full( n_states, alpha_smooth * n_vocab )\n",
    "\n",
    "for sequence in tagged_train_sequences:\n",
    "    for word, tag in sequence:\n",
    "        if tag in tag_to_index and word in word_to_index:\n",
    "            idx_tag = tag_to_index[ tag ]\n",
    "            idx_word = word_to_index[ word ]\n",
    "\n",
    "            emission_counts[ idx_tag, idx_word ] += 1\n",
    "            tag_total_counts[ idx_tag ] += 1  # Incrementa a contagem total da tag\n",
    "\n",
    "# Normalizar para obter a matriz B de probabilidades\n",
    "# B[j, k] = count(tag_j emitindo palavra_k) / count(total de tag_j)\n",
    "B = emission_counts / tag_total_counts[ :, np.newaxis ]  # Divide cada linha pelo total da tag correspondente\n",
    "\n",
    "# Verifica se as linhas somam 1 (ou muito próximo)\n",
    "# print(\"Soma das linhas da Matriz B (deve ser próximo de 1):\", np.sum(B, axis=1))\n",
    "print( f\"Matriz B (Emissão) calculada. Shape: {B.shape}\" )\n",
    "\n",
    "# --- Resumo do que foi calculado ---\n",
    "# vocabulary: list[str] - Lista única de palavras\n",
    "# states: list[str] - Lista única de tags POS\n",
    "# word_to_index: dict[str, int] - Mapeamento palavra -> índice\n",
    "# tag_to_index: dict[str, int] - Mapeamento tag -> índice\n",
    "# pi: np.ndarray (shape n_states) - Vetor de probabilidades iniciais\n",
    "# A: np.ndarray (shape n_states x n_states) - Matriz de transição de estados\n",
    "# B: np.ndarray (shape n_states x n_vocab) - Matriz de emissão"
   ],
   "id": "6b74151c4a2a9ff3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz B (Emissão) calculada. Shape: (16, 11162)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Mapeamentos Inversos\n",
    "\n",
    "Para podermos interpretar os resultados (converter índices de volta para palavras e tags)."
   ],
   "id": "d1291b1eb1360651"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T19:14:08.668993Z",
     "start_time": "2025-04-14T19:14:08.644073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assumindo que você já tem word_to_index e tag_to_index\n",
    "\n",
    "index_to_word = { index: word for word, index in word_to_index.items() }\n",
    "index_to_tag = { index: tag for tag, index in tag_to_index.items() }\n",
    "\n",
    "print( \"Mapeamentos inversos criados.\" )"
   ],
   "id": "ae136034b72567e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapeamentos inversos criados.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Lógica de Predição\n",
    "\n",
    "A ideia principal é calcular a probabilidade de cada palavra do vocabulário aparecer na posição mascarada, considerando o contexto (a tag da palavra anterior).\n",
    "\n"
   ],
   "id": "415c606283713215"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T19:15:17.274652Z",
     "start_time": "2025-04-14T19:15:17.250123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Importe o spacy e carregue o modelo nlp novamente se estiver em um novo script/sessão\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"pt_core_news_lg\") # Ou o modelo que você usou\n",
    "\n",
    "def predict_masked_word( sentence_tokens: list[ str ],\n",
    "                         mask_token: str = \"MASK\",\n",
    "                         pi: np.ndarray = pi,\n",
    "                         A: np.ndarray = A,\n",
    "                         B: np.ndarray = B,\n",
    "                         word_to_index: dict = word_to_index,\n",
    "                         tag_to_index: dict = tag_to_index,\n",
    "                         index_to_word: dict = index_to_word,\n",
    "                         index_to_tag: dict = index_to_tag,\n",
    "                         nlp = nlp\n",
    "                         ):  # Passa o objeto nlp do spaCy\n",
    "    \"\"\"\n",
    "    Prevê a palavra mais provável para substituir o token de máscara em uma sentença.\n",
    "\n",
    "    Args:\n",
    "        sentence_tokens (list[str]): Lista de palavras da sentença com um token de máscara.\n",
    "                                     Ex: ['eu', 'amo', 'MASK']\n",
    "        mask_token (str): O token que representa a máscara.\n",
    "        pi (np.ndarray): Vetor de probabilidades iniciais.\n",
    "        A (np.ndarray): Matriz de transição.\n",
    "        B (np.ndarray): Matriz de emissão.\n",
    "        word_to_index (dict): Mapeamento palavra -> índice.\n",
    "        tag_to_index (dict): Mapeamento tag -> índice.\n",
    "        index_to_word (dict): Mapeamento índice -> palavra.\n",
    "        index_to_tag (dict): Mapeamento índice -> tag.\n",
    "        nlp: Objeto spaCy carregado para fazer POS tagging.\n",
    "\n",
    "    Returns:\n",
    "        str: A palavra prevista mais provável.\n",
    "        (Ou None se não puder prever)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mask_index = sentence_tokens.index( mask_token )\n",
    "    except ValueError:\n",
    "        print( f\"Erro: Token de máscara '{mask_token}' não encontrado na sentença.\" )\n",
    "        return None\n",
    "\n",
    "    n_states = A.shape[ 0 ]\n",
    "    n_vocab = B.shape[ 1 ]\n",
    "\n",
    "    # --- Passo 1: Determinar as probabilidades da TAG na posição MASK ---\n",
    "\n",
    "    # Probabilidade de cada estado (tag) na posição da máscara, P(tag_mask)\n",
    "    state_probabilities_at_mask = np.zeros( n_states )\n",
    "\n",
    "    if mask_index == 0:\n",
    "        # Se a máscara é a primeira palavra, usamos as probabilidades iniciais (pi)\n",
    "        state_probabilities_at_mask = pi\n",
    "    else:\n",
    "        # Se não for a primeira, usamos a matriz de transição (A)\n",
    "        prev_word = sentence_tokens[ mask_index - 1 ]\n",
    "\n",
    "        # Precisamos da tag da palavra anterior. Usamos o spaCy.\n",
    "        # É melhor taggear a parte conhecida da sentença para dar contexto ao spaCy\n",
    "        known_text = \" \".join( sentence_tokens[ :mask_index ] )\n",
    "        doc = nlp( known_text )\n",
    "\n",
    "        if not doc or not doc[ -1 ].pos_ in tag_to_index:\n",
    "            print(\n",
    "                    f\"Aviso: Não foi possível obter uma tag válida para a palavra anterior '{prev_word}'. Usando distribuição uniforme para tags.\"\n",
    "            )\n",
    "            # Se não conseguirmos a tag anterior, podemos assumir uma distribuição uniforme\n",
    "            # ou usar alguma outra heurística. Uniforme é mais simples.\n",
    "            state_probabilities_at_mask = np.ones( n_states ) / n_states\n",
    "        else:\n",
    "            prev_tag = doc[ -1 ].pos_\n",
    "            prev_tag_idx = tag_to_index[ prev_tag ]\n",
    "            # As probabilidades de transição da tag anterior para qualquer tag atual\n",
    "            state_probabilities_at_mask = A[ prev_tag_idx, : ]\n",
    "\n",
    "    # --- Passo 2: Calcular a probabilidade de cada PALAVRA na posição MASK ---\n",
    "\n",
    "    # P(palavra | contexto) = Σ [ P(palavra | tag_mask) * P(tag_mask | contexto) ]\n",
    "    # onde P(tag_mask | contexto) são as state_probabilities_at_mask que calculamos\n",
    "    # e P(palavra | tag_mask) vem da matriz B\n",
    "\n",
    "    # Inicializa as probabilidades das palavras\n",
    "    word_probabilities = np.zeros( n_vocab )\n",
    "\n",
    "    for current_tag_idx in range( n_states ):\n",
    "        prob_current_tag = state_probabilities_at_mask[ current_tag_idx ]\n",
    "        if prob_current_tag > 0:  # Otimização: só calcula se a tag tiver chance de ocorrer\n",
    "            # Probabilidades de emissão para a tag atual P(palavra | tag_atual)\n",
    "            emission_probs_for_tag = B[ current_tag_idx, : ]\n",
    "            # Adiciona a contribuição ponderada pela probabilidade da tag\n",
    "            word_probabilities += prob_current_tag * emission_probs_for_tag\n",
    "\n",
    "    # --- Passo 3: Encontrar a palavra mais provável ---\n",
    "    if np.sum( word_probabilities ) == 0:\n",
    "        # Isso pode acontecer se todas as probabilidades intermediárias forem zero\n",
    "        print( \"Aviso: Não foi possível calcular probabilidades para as palavras.\" )\n",
    "        return None  # Ou retornar uma palavra padrão\n",
    "\n",
    "    # Encontra o índice da palavra com maior probabilidade\n",
    "    # Evita prever o próprio token de máscara se ele estiver no vocabulário\n",
    "    if mask_token in word_to_index:\n",
    "        mask_vocab_idx = word_to_index[ mask_token ]\n",
    "        word_probabilities[ mask_vocab_idx ] = 0  # Zera a prob. do token MASK\n",
    "\n",
    "    # Poderia também zerar a probabilidade de tokens desconhecidos ou raros se necessário\n",
    "\n",
    "    predicted_word_idx = np.argmax( word_probabilities )\n",
    "\n",
    "    # Converte o índice de volta para a palavra\n",
    "    predicted_word = index_to_word[ predicted_word_idx ]\n",
    "\n",
    "    return predicted_word\n"
   ],
   "id": "9ce5aaf8f14a80d4",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T19:16:26.048351Z",
     "start_time": "2025-04-14T19:16:26.023812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "frase: str = \"Eu amo que MASK \"\n",
    "frase_exemplo = frase.split()\n",
    "palavra_prevista = predict_masked_word( frase_exemplo )\n",
    "\n",
    "if palavra_prevista:\n",
    "    print( f\"Frase: {' '.join( frase_exemplo )}\" )\n",
    "    print( f\"Palavra Prevista: {palavra_prevista}\" )\n",
    "\n",
    "frase_exemplo_2 = [ \"MASK\", \"é\", \"bom\" ]\n",
    "palavra_prevista_2 = predict_masked_word( frase_exemplo_2 )\n",
    "\n",
    "if palavra_prevista_2:\n",
    "    print( f\"Frase: {' '.join( frase_exemplo_2 )}\" )\n",
    "    print( f\"Palavra Prevista: {palavra_prevista_2}\" )"
   ],
   "id": "e006db2688e89de4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase: Eu amo que MASK\n",
      "Palavra Prevista: eu\n",
      "Frase: MASK é bom\n",
      "Palavra Prevista: não\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T19:18:17.251780Z",
     "start_time": "2025-04-14T19:18:17.190084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Importe o spacy e carregue o modelo nlp novamente se estiver em um novo script/sessão\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"pt_core_news_lg\") # Ou o modelo que você usou\n",
    "\n",
    "def predict_masked_word_with_tag_probs( sentence_tokens: list[ str ],\n",
    "                                        mask_token: str = \"MASK\",\n",
    "                                        top_n_tags: int = 5,  # Quantas tags mostrar\n",
    "                                        pi: np.ndarray = pi,\n",
    "                                        A: np.ndarray = A,\n",
    "                                        B: np.ndarray = B,\n",
    "                                        word_to_index: dict = word_to_index,\n",
    "                                        tag_to_index: dict = tag_to_index,\n",
    "                                        index_to_word: dict = index_to_word,\n",
    "                                        index_to_tag: dict = index_to_tag,\n",
    "                                        nlp = nlp\n",
    "                                        ):\n",
    "    \"\"\"\n",
    "    Prevê a palavra mais provável para substituir o token de máscara e\n",
    "    exibe as tags POS mais prováveis para essa posição.\n",
    "\n",
    "    Args:\n",
    "        sentence_tokens (list[str]): Lista de palavras da sentença com um token de máscara.\n",
    "        mask_token (str): O token que representa a máscara.\n",
    "        top_n_tags (int): Número de tags mais prováveis a serem exibidas.\n",
    "        pi, A, B, word_to_index, tag_to_index, index_to_word, index_to_tag: Parâmetros do HMM.\n",
    "        nlp: Objeto spaCy carregado para fazer POS tagging.\n",
    "\n",
    "    Returns:\n",
    "        str: A palavra prevista mais provável.\n",
    "        (Ou None se não puder prever)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mask_index = sentence_tokens.index( mask_token )\n",
    "    except ValueError:\n",
    "        print( f\"Erro: Token de máscara '{mask_token}' não encontrado na sentença.\" )\n",
    "        return None\n",
    "\n",
    "    n_states = A.shape[ 0 ]\n",
    "    n_vocab = B.shape[ 1 ]\n",
    "\n",
    "    # --- Passo 1: Determinar as probabilidades da TAG na posição MASK ---\n",
    "    state_probabilities_at_mask = np.zeros( n_states )\n",
    "    prev_tag_info = \"N/A (Primeira Palavra)\"  # Info para display\n",
    "\n",
    "    if mask_index == 0:\n",
    "        state_probabilities_at_mask = pi\n",
    "    else:\n",
    "        prev_word = sentence_tokens[ mask_index - 1 ]\n",
    "        known_text = \" \".join( sentence_tokens[ :mask_index ] )\n",
    "        doc = nlp( known_text )\n",
    "\n",
    "        if not doc or not doc[ -1 ].pos_ in tag_to_index:\n",
    "            print(\n",
    "                f\"Aviso: Não foi possível obter uma tag válida para a palavra anterior '{prev_word}'. Usando distribuição uniforme para tags.\"\n",
    "                )\n",
    "            state_probabilities_at_mask = np.ones( n_states ) / n_states\n",
    "            prev_tag_info = f\"{prev_word} (Tag Desconhecida)\"\n",
    "        else:\n",
    "            prev_tag = doc[ -1 ].pos_\n",
    "            prev_tag_idx = tag_to_index[ prev_tag ]\n",
    "            state_probabilities_at_mask = A[ prev_tag_idx, : ]\n",
    "            prev_tag_info = f\"{prev_word} ({prev_tag})\"  # Guarda info da tag anterior\n",
    "\n",
    "    # --- Passo EXTRA: Exibir as Tags Mais Prováveis ---\n",
    "    print( \"-\" * 30 )\n",
    "    print( f\"Análise para MASK na frase: {' '.join( sentence_tokens )}\" )\n",
    "    print( f\"Palavra Anterior (Tag): {prev_tag_info}\" )\n",
    "    print( f\"Top {top_n_tags} Tags Mais Prováveis para MASK:\" )\n",
    "\n",
    "    # Cria lista de (tag, probabilidade)\n",
    "    tag_probs = [ ]\n",
    "    for idx, prob in enumerate( state_probabilities_at_mask ):\n",
    "        if idx in index_to_tag:  # Garante que o índice existe no mapeamento\n",
    "            tag_name = index_to_tag[ idx ]\n",
    "            tag_probs.append( (tag_name, prob) )\n",
    "\n",
    "    # Ordena pela probabilidade (decrescente)\n",
    "    sorted_tags = sorted( tag_probs, key = lambda item: item[ 1 ], reverse = True )\n",
    "\n",
    "    # Exibe as top N\n",
    "    for i in range( min( top_n_tags, len( sorted_tags ) ) ):\n",
    "        tag, probability = sorted_tags[ i ]\n",
    "        print( f\"  {i + 1}. {tag:<10} Prob: {probability:.4f}\" )  # :<10 alinha a tag à esquerda\n",
    "    print( \"-\" * 30 )\n",
    "\n",
    "    # --- Passo 2: Calcular a probabilidade de cada PALAVRA na posição MASK ---\n",
    "    word_probabilities = np.zeros( n_vocab )\n",
    "    for current_tag_idx in range( n_states ):\n",
    "        prob_current_tag = state_probabilities_at_mask[ current_tag_idx ]\n",
    "        if prob_current_tag > 0:\n",
    "            emission_probs_for_tag = B[ current_tag_idx, : ]\n",
    "            word_probabilities += prob_current_tag * emission_probs_for_tag\n",
    "\n",
    "    # --- Passo 3: Encontrar a palavra mais provável ---\n",
    "    if np.sum( word_probabilities ) == 0:\n",
    "        print( \"Aviso: Não foi possível calcular probabilidades para as palavras.\" )\n",
    "        return None\n",
    "\n",
    "    if mask_token in word_to_index:\n",
    "        mask_vocab_idx = word_to_index[ mask_token ]\n",
    "        word_probabilities[ mask_vocab_idx ] = 0\n",
    "\n",
    "    predicted_word_idx = np.argmax( word_probabilities )\n",
    "    predicted_word = index_to_word[ predicted_word_idx ]\n",
    "\n",
    "    return predicted_word\n",
    "\n",
    "\n",
    "# --- Exemplo de Uso ---\n",
    "frase_exemplo = [ \"o\", \"sol\", \"brilha\", \"e\", \"o\", \"céu\", \"está\", \"MASK\" ]\n",
    "palavra_prevista = predict_masked_word_with_tag_probs( frase_exemplo, top_n_tags = 5\n",
    "                                                       )  # Mostra as 5 tags mais prováveis\n",
    "\n",
    "if palavra_prevista:\n",
    "    # A função agora imprime a tabela de tags internamente\n",
    "    print( f\"Palavra Prevista Final: {palavra_prevista}\" )\n",
    "    print( \"\\n\" )\n",
    "\n",
    "frase_exemplo_2 = [ \"MASK\", \"sempre\", \"volta\" ]\n",
    "palavra_prevista_2 = predict_masked_word_with_tag_probs( frase_exemplo_2, top_n_tags = 5 )\n",
    "\n",
    "if palavra_prevista_2:\n",
    "    print( f\"Palavra Prevista Final: {palavra_prevista_2}\" )\n",
    "    print( \"\\n\" )\n",
    "\n",
    "frase_exemplo_3 = [ \"ela\", \"gosta\", \"de\", \"MASK\", \"chocolate\" ]  # Máscara no meio\n",
    "palavra_prevista_3 = predict_masked_word_with_tag_probs( frase_exemplo_3, top_n_tags = 5 )\n",
    "\n",
    "if palavra_prevista_3:\n",
    "    print( f\"Palavra Prevista Final: {palavra_prevista_3}\" )\n",
    "    print( \"\\n\" )"
   ],
   "id": "5254a100bf4bcc71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Análise para MASK na frase: o sol brilha e o céu está MASK\n",
      "Palavra Anterior (Tag): está (AUX)\n",
      "Top 5 Tags Mais Prováveis para MASK:\n",
      "  1. VERB       Prob: 0.2917\n",
      "  2. ADJ        Prob: 0.1330\n",
      "  3. DET        Prob: 0.1153\n",
      "  4. ADV        Prob: 0.1032\n",
      "  5. PRON       Prob: 0.1014\n",
      "------------------------------\n",
      "Palavra Prevista Final: que\n",
      "\n",
      "\n",
      "------------------------------\n",
      "Análise para MASK na frase: MASK sempre volta\n",
      "Palavra Anterior (Tag): N/A (Primeira Palavra)\n",
      "Top 5 Tags Mais Prováveis para MASK:\n",
      "  1. VERB       Prob: 0.1994\n",
      "  2. PRON       Prob: 0.1759\n",
      "  3. ADV        Prob: 0.1572\n",
      "  4. DET        Prob: 0.1018\n",
      "  5. NOUN       Prob: 0.0949\n",
      "------------------------------\n",
      "Palavra Prevista Final: não\n",
      "\n",
      "\n",
      "------------------------------\n",
      "Análise para MASK na frase: ela gosta de MASK chocolate\n",
      "Palavra Anterior (Tag): de (ADP)\n",
      "Top 5 Tags Mais Prováveis para MASK:\n",
      "  1. NOUN       Prob: 0.5432\n",
      "  2. DET        Prob: 0.1926\n",
      "  3. PRON       Prob: 0.1450\n",
      "  4. ADV        Prob: 0.0338\n",
      "  5. PROPN      Prob: 0.0300\n",
      "------------------------------\n",
      "Palavra Prevista Final: o\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
